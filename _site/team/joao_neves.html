<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>João C. Neves - Intelligent Systems</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    colors: {
                        primary: {
                            DEFAULT: '#2563eb',
                            dark: '#1d4ed8',
                            light: '#3b82f6'
                        },
                        secondary: {
                            DEFAULT: '#7c3aed',
                            dark: '#6d28d9',
                            light: '#8b5cf6'
                        },
                        accent: '#0ea5e9',
                        surface: '#f9fafb',
                        ink: {
                            DEFAULT: '#1e293b',
                            light: '#64748b'
                        }
                    },
                    boxShadow: {
                        'intense': '0 10px 25px rgba(0, 0, 0, 0.1)',
                        'hover': '0 15px 30px rgba(0, 0, 0, 0.12)'
                    }
                }
            }
        }
    </script>
    <link rel="stylesheet" href="/assets/css/modern.css">
    <link rel="stylesheet" href="/assets/css/gradient-headings.css">
    <style>
        /* Base styles for the Intelligent Systems lab */
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
        }
        
        /* Gradient Text for Headings */
        .gradient-text {
            background: linear-gradient(to right, #facc15, #f97316, #ef4444); /* warm */
            -webkit-background-clip: text;
            background-clip: text;
            color: transparent;
            font-weight: 700;
            }

        
            h1 {
  background: linear-gradient(to right, #ffffff, #facc15, #fda4af);
  background-clip: text;
  -webkit-background-clip: text;
  color: transparent;
  font-weight: 700;
  display: inline-block; /* ensures background-clip works correctly */
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.15); /* subtle depth */

}

        
        /* Modern card styling with subtle hover effects */
        .card {
            transition: all 0.3s ease;
        }
        
        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 15px 30px rgba(0, 0, 0, 0.12);
        }
        
        /* Modern gradient decorations */
        .gradient-line {
            height: 3px;
            width: 50px;
            background: linear-gradient(90deg, #2563eb 0%, #7c3aed 100%);
            border-radius: 3px;
            margin: 0.5rem 0 1.5rem 0;
        }
        
        /* Logo animation */
        .site-logo {
            transition: transform 0.3s ease;
        }
        
        .site-logo:hover {
            transform: scale(1.05);
        }
        
        /* Cool gradient borders */
        .gradient-border {
            position: relative;
        }
        
        .gradient-border::after {
            content: '';
            position: absolute;
            left: 0;
            bottom: -3px;
            width: 100%;
            height: 3px;
            background: linear-gradient(90deg, #2563eb, #7c3aed, #0ea5e9);
            border-radius: 3px;
            transform: scaleX(0);
            transform-origin: left;
            transition: transform 0.3s ease;
        }
        
        .gradient-border:hover::after {
            transform: scaleX(1);
        }
        
        /* Fix for member profile spacing issues */
        .relative.z-10 svg {
            height: 40px !important;
            max-height: 40px !important;
        }
        .container.mx-auto.px-4.py-4 {
            margin-top: 0rem !important;
        }
        
        /* Modern soft styling for the header strip */
        .relative.overflow-hidden {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.05);
        }
        
        .relative.overflow-hidden::before {
            content: '';
            position: absolute;
            inset: 0;
            background: rgba(255, 255, 255, 0.05);
            z-index: 1;
        }
        
        /* Soften text shadows for better readability */
        .drop-shadow-sm {
            text-shadow: 0 1px 2px rgba(0, 0, 0, 0.1);
        }
    </style>
    
</head>
<body class="min-h-screen bg-surface flex flex-col">
    <!-- Header -->
    <header class="bg-white shadow-md fixed top-0 left-0 right-0 z-50 transition-all duration-300">
  <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
    <div class="flex justify-between items-center py-4">
      <a class="flex items-center gap-3 group transition-all duration-300" rel="author" href="/">
        
          <img src="/assets/images/lab_logo.png" alt="Intelligent Systems logo" class="h-12 w-auto transform transition-transform duration-300 group-hover:scale-110">
        
        <span class="text-xl gradient-text transition-all duration-300">Intelligent Systems</span>
      </a>

      <!-- Desktop Nav -->
      <nav class="hidden md:flex items-center space-x-1">
        
        
        
        
        
        <a href="/news" 
           class="px-3 py-2 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-all duration-200 font-medium gradient-border ">
          News
        </a>
        
        
        <a href="/team" 
           class="px-3 py-2 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-all duration-200 font-medium gradient-border ">
          Team
        </a>
        
        
        <a href="/publications" 
           class="px-3 py-2 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-all duration-200 font-medium gradient-border ">
          Publications
        </a>
        
        
        <a href="/projects" 
           class="px-3 py-2 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-all duration-200 font-medium gradient-border ">
          Projects
        </a>
        
        
        <a href="/datasets" 
           class="px-3 py-2 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-all duration-200 font-medium gradient-border ">
          Datasets
        </a>
        
        
        <a href="/funding" 
           class="px-3 py-2 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-all duration-200 font-medium gradient-border ">
          Funding
        </a>
        
        
        <a href="/alumni" 
           class="px-3 py-2 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-all duration-200 font-medium gradient-border ">
          Alumni
        </a>
        
        
        <!-- Contact Button with special styling -->
        <a href="/contact" class="ml-2 px-4 py-2 gradient-button rounded-md transition-all duration-200 shadow-sm hover:shadow-md font-medium">
          Contact
        </a>
      </nav>

      <!-- Mobile Nav Button -->
      <div class="md:hidden">
        <button type="button" class="mobile-menu-button text-ink hover:text-primary transition-colors" aria-label="Toggle menu">
          <svg class="w-7 h-7" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path>
          </svg>
        </button>
      </div>
    </div>

    <!-- Mobile Nav Menu -->
    <div class="mobile-menu hidden md:hidden pb-4 space-y-1 bg-white">
      
      
      <a href="/news" 
         class="block py-2 px-3 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-colors ">
        News
      </a>
      
      
      <a href="/team" 
         class="block py-2 px-3 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-colors ">
        Team
      </a>
      
      
      <a href="/publications" 
         class="block py-2 px-3 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-colors ">
        Publications
      </a>
      
      
      <a href="/projects" 
         class="block py-2 px-3 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-colors ">
        Projects
      </a>
      
      
      <a href="/datasets" 
         class="block py-2 px-3 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-colors ">
        Datasets
      </a>
      
      
      <a href="/funding" 
         class="block py-2 px-3 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-colors ">
        Funding
      </a>
      
      
      <a href="/alumni" 
         class="block py-2 px-3 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-colors ">
        Alumni
      </a>
      
      <!-- Mobile Contact Button -->
      <a href="/contact" class="block py-2 px-3 mt-2 text-primary font-medium">
        Contact
      </a>
    </div>
  </div>

  <!-- Mobile menu JavaScript -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      const btn = document.querySelector('.mobile-menu-button');
      const menu = document.querySelector('.mobile-menu');
      const header = document.querySelector('header');
      
      btn.addEventListener('click', function() {
        menu.classList.toggle('hidden');
        // Add a slight animation
        if (!menu.classList.contains('hidden')) {
          // Menu is now visible - animate items
          const menuItems = menu.querySelectorAll('a');
          menuItems.forEach((item, index) => {
            item.style.opacity = '0';
            item.style.transform = 'translateY(-10px)';
            setTimeout(() => {
              item.style.transition = 'opacity 0.3s ease, transform 0.3s ease';
              item.style.opacity = '1';
              item.style.transform = 'translateY(0)';
            }, index * 50);
          });
        }
      });
      
      // Close mobile menu when clicking outside
      document.addEventListener('click', function(event) {
        if (!menu.classList.contains('hidden') && 
            !menu.contains(event.target) && 
            !btn.contains(event.target)) {
          menu.classList.add('hidden');
        }
      });
      
      // Header scroll effect with enhanced shadow
      window.addEventListener('scroll', function() {
        if (window.scrollY > 10) {
          header.classList.add('shadow-lg');
          header.classList.add('bg-white/95');
          header.classList.add('backdrop-blur-sm');
        } else {
          header.classList.remove('shadow-lg');
          header.classList.remove('bg-white/95');
          header.classList.remove('backdrop-blur-sm');
        }
      });
    });
  </script>
</header>

<style>
  /* Header animations */
  header {
    transition: box-shadow 0.3s ease;
  }
  
  header.scrolled {
    box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
  }
</style>


    <!-- Main Content -->
    <main class="flex-grow pt-16">
        <div class="min-h-screen bg-gray-50">
  <!-- Hero section with gradient background and profile image -->
  <div class="relative overflow-hidden">
    <div class="absolute inset-0 bg-gradient-to-r from-blue-400 to-indigo-500 opacity-90 z-0"></div>
    <div class="absolute inset-0 bg-pattern opacity-10 z-0"></div>
    <div class="absolute bottom-0 left-0 w-full h-72 bg-gradient-to-t from-white to-transparent z-0"></div>
    
    <div class="container mx-auto px-4 py-12 relative z-10">
      <div class="flex flex-col md:flex-row items-center md:items-start">
        <!-- Profile image -->
        <div class="w-28 h-28 md:w-40 md:h-40 rounded-full overflow-hidden shadow-xl border-4 border-white mb-6 md:mb-0 md:mr-8 flex-shrink-0 bg-white">
          
          <img src="/assets/team/joaocneves.jpg" alt="João C. Neves" class="w-full h-full object-cover ">
          
        </div>
        
        <!-- Name and basic info -->
        <div class="text-center md:text-left">
          <h1 class="text-3xl md:text-4xl font-bold text-white mb-2 drop-shadow-sm">João C. Neves</h1>
          <p class="text-xl text-white mb-4 opacity-90 drop-shadow-sm">Associte Professor</p>
          
          
          <div class="mb-4 inline-block bg-green-500 text-white px-3 py-1 rounded-full text-sm font-semibold shadow-sm">
            Active Member • Since 2012
          </div>
          
          
          <!-- Social/contact links -->
          <div class="flex flex-wrap gap-3 mt-4 justify-center md:justify-start">
            
            <a href="mailto:joao@di.ubi.pt" class="bg-white/20 hover:bg-white/30 text-white font-medium py-2 px-4 rounded-lg inline-flex items-center text-sm transition-all duration-300 shadow-sm hover:shadow">
              <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 8l7.89 5.26a2 2 0 002.22 0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z" />
              </svg>
              Email
            </a>
            
            
            
            <a href="https://www.di.ubi.pt/~jcneves/" target="_blank" rel="noopener" class="bg-white hover:bg-gray-100 text-blue-700 font-medium py-2 px-4 rounded-lg inline-flex items-center text-sm transition-all duration-300 shadow-sm hover:shadow">
              <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 12a9 9 0 01-9 9m9-9a9 9 0 00-9-9m9 9H3m9 9a9 9 0 01-9-9m9 9c1.657 0 3-4.03 3-9s-1.343-9-3-9m0 18c-1.657 0-3-4.03-3-9s1.343-9 3-9m-9 9a9 9 0 019-9" />
              </svg>
              Website
            </a>
            
            
            
            
            
            <a href="https://scholar.google.com/citations?user=U1cK384AAAAJ" target="_blank" rel="noopener" class="bg-blue-700 hover:bg-blue-800 text-white font-medium py-2 px-4 rounded-lg inline-flex items-center text-sm transition-all duration-300 shadow-sm hover:shadow">
              <svg class="h-4 w-4 mr-2" viewBox="0 0 24 24" fill="currentColor">
                <path d="M5.242 13.769L0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977 0-5.548 1.748-6.758 4.269zM12 10a7 7 0 1 0 0 14 7 7 0 0 0 0-14z" />
              </svg>
              Google Scholar
            </a>
            

            

            

            

            
          </div>
        </div>
      </div>
    </div>
    
    <!-- Wave separator -->
    <div class="relative z-10">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1440 150" class="w-full h-auto fill-gray-50">
        <path d="M0,64L48,69.3C96,75,192,85,288,90.7C384,96,480,96,576,80C672,64,768,32,864,32C960,32,1056,64,1152,69.3C1248,75,1344,53,1392,42.7L1440,32L1440,150L1392,150C1344,150,1248,150,1152,150C1056,150,960,150,864,150C768,150,672,150,576,150C480,150,384,150,288,150C192,150,96,150,48,150L0,150Z"></path>
      </svg>
    </div>
  </div>
  
  <!-- Content section -->

  <div class="container mx-auto px-4 py-4">
    <div class="grid grid-cols-1 md:grid-cols-3 gap-8">
      <!-- Left sidebar with details -->
      <div class="md:col-span-1">
        <!-- Lab Info Card -->
        <div class="bg-white rounded-lg shadow-md overflow-hidden mb-8 transform transition hover:shadow-lg">
          <div class="border-b border-gray-100">
            <h2 class="text-xl font-bold text-gray-800 p-6">Lab Information</h2>
          </div>
          <div class="p-6">
            <!-- Role -->
            <div class="mb-4">
              <h3 class="text-sm font-semibold text-gray-500 uppercase tracking-wider mb-2">Role</h3>
              
              
              <p class="text-gray-700">Faculty</p>
            </div>
            
            <!-- Timeline -->
            
            <div class="mb-4">
              <h3 class="text-sm font-semibold text-gray-500 uppercase tracking-wider mb-2">Timeline</h3>
              <div class="relative border-l-2 border-blue-200 pl-4 py-2">
                <div class="absolute w-3 h-3 bg-blue-500 rounded-full -left-[7px] top-0"></div>
                <p class="text-gray-700">
                  <span class="font-medium">Joined:</span> 2012
                </p>
                
              </div>
            </div>
            
            
            <!-- Supervision -->
            
            
            <!-- Current affiliation for alumni -->
            
          </div>
        </div>
        
        <!-- Research Interests Card -->
        
        <div class="bg-white rounded-lg shadow-md overflow-hidden transform transition hover:shadow-lg">
          <div class="border-b border-gray-100">
            <h2 class="text-xl font-bold text-gray-800 p-6">Research Interests</h2>
          </div>
          <div class="p-6">
            <div class="flex flex-wrap gap-2">
              
              <span class="inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-blue-100 text-blue-800">
                Artificial Intelligence
              </span>
              
              <span class="inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-blue-100 text-blue-800">
                Computer Vision
              </span>
              
              <span class="inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-blue-100 text-blue-800">
                Biometrics
              </span>
              
              <span class="inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-blue-100 text-blue-800">
                Deep Learning
              </span>
              
              <span class="inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-blue-100 text-blue-800">
                Pattern Recognition
              </span>
              
              <span class="inline-flex items-center px-3 py-1 rounded-full text-sm font-medium bg-blue-100 text-blue-800">
                Visual Surveillance
              </span>
              
            </div>
          </div>
        </div>
        

        <!-- Research Profiles Card -->
     
        
        <div class="bg-white rounded-lg shadow-md overflow-hidden transform transition hover:shadow-lg mt-8">
          <div class="border-b border-gray-100">
            <h2 class="text-xl font-bold text-gray-800 p-6">Research Profiles</h2>
          </div>
          <div class="p-6">
            <div class="flex flex-col space-y-3">
              
              <a href="https://scholar.google.com/citations?user=U1cK384AAAAJ" target="_blank" rel="noopener" class="flex items-center text-gray-700 hover:text-blue-600 transition-colors">
                <div class="bg-blue-100 p-2 rounded-lg mr-3">
                  <svg class="h-5 w-5 text-blue-700" viewBox="0 0 24 24" fill="currentColor">
                    <path d="M5.242 13.769L0 9.5 12 0l12 9.5-5.242 4.269C17.548 11.249 14.978 9.5 12 9.5c-2.977 0-5.548 1.748-6.758 4.269zM12 10a7 7 0 1 0 0 14 7 7 0 0 0 0-14z" />
                  </svg>
                </div>
                <span>Google Scholar</span>
              </a>
              

              

              

              

              

              
            </div>
          </div>
        </div>
        
      </div>
      
      <!-- Main content area -->        

      <div class="md:col-span-2">
        <!-- Biography -->
        <div class="bg-white rounded-lg shadow-md overflow-hidden mb-8 transform transition hover:shadow-lg">
          <div class="border-b border-gray-100">
            <h2 class="text-xl font-bold text-gray-800 p-6">Biography</h2>
          </div>
          <div class="p-6">
            <div class="prose prose-lg max-w-none text-gray-700">
              <p>João Neves received a B.Sc., M.Sc. in Computer Science from University of Beira Interior in 2011 and 2013, respectively. From 2014 to 2017, he was awarded with a Ph.D. research grant from the Portuguese national funding agency for science (FCT). He obtained the Ph.D. in Computer Science from the University of Beira Interior in 2018. In 2021, he joined the TOMIWORLD company as a Computer Vision Engineer in the context of the research project BIODI. Simultaneously, he joined the University of Beira Interior in 2018 as an Assistant Professor of the Computer Science Department. His research interests are mainly focused in the fields of Computer Vision and Machine Learning, including face recognition, biometrics and surveillance. He authored more than 30 papers, and received the ‘best paper runner up’ award in a CVPR workshop.</p>


            </div>
          </div>
        </div>
   
        <!-- Supervised Students (For Faculty) -->
        
        
        
        
        
        <!-- <p>Kailahs</p> -->
        
        <div class="bg-white rounded-lg shadow-md overflow-hidden mb-8 transform transition hover:shadow-lg">
          <div class="border-b border-gray-100">
            <h2 class="text-xl font-bold text-gray-800 p-6">Students</h2>
          </div>
          <div class="p-6">
            
            
            <div class="mb-6">
              <h3 class="text-lg font-semibold text-gray-700 mb-4">PhD Students (Main Supervisor)</h3>
              <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
                
                <a href="/team/cristiano_patr%C3%ADcio" class="flex items-center p-3 bg-purple-50 rounded-lg hover:bg-purple-100 transition-colors">
                  
                  <img src="/assets/team/cristianopatricio.jpg" alt="Cristiano Patrício" class="h-10 w-10 rounded-full object-cover border border-purple-200">
                  
                  <div class="ml-3">
                    <h4 class="text-sm font-medium text-gray-900">Cristiano Patrício</h4>
                    <p class="text-xs text-gray-500">Since 2021</p>
                  </div>
                </a>
                
              </div>
            </div>
            
         
            
      
            
            
            
          </div>
        </div>
        
        
        <!-- Teaching/Courses -->
        
        <!-- Awards and Achievements -->
        
      
        <!-- Publications -->
        
        
        
        
                  
        
        
        <div class="bg-white rounded-lg shadow-md overflow-hidden mb-8 transform transition hover:shadow-lg">
          <div class="border-b border-gray-100">
            <h2 class="text-xl font-bold text-gray-800 p-6">
              Publications
              <span class="ml-2 text-sm font-medium px-2 py-1 bg-blue-100 text-blue-800 rounded-full">50</span>
            </h2>
          </div>
          <div class="p-6">
            <ul class="divide-y divide-gray-200" id="publications-list">

              
              <li class="py-6 first:pt-0 ">
                <div class="flex flex-col md:flex-row">
                  <div class="flex-grow">
                    <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Automatic annotation of Leishmania infections in fluorescence microscopy images</h3>
                    <p class="text-gray-600 italic mt-1">João C. Neves, Helena Castro, Hugo Proença, Miguel Coimbra</p>
                    <p class="text-gray-700 mt-1">Proceedings of the International Conference on Image Analysis and Recognition - ICIAR 2013 (2013)</p>
                    
                    
                    <details class="mt-3 group">
                      <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                        </svg>
                        Show Abstract
                      </summary>
                      <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                        Leishmania is a unicellular parasite that infects mammals and biologists are interested in determining the effect of drugs in Leishmania infections. This requires the manual annotation of the number of macrophages and parasites in images, in order to obtain the percentage of infection (IP), the average number of parasites per infected cell (NPI) and the infection index (IX). Considering that manual annotation is tedious, time-consuming and often erroneous, in this paper we propose an automatic method for automatic annotation of Leishmania infections us- ing fluorescent microscopy. Moreover, when compared to related works, the proposed method is able to get superior performance under most perspectives.
                      </div>
                    </details>
                    

                    
                    <div class="mt-3">
                      
                      <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                        Fluorescence Microscopy
                      </span>
                      
                      <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                        Leishmania
                      </span>
                      
                      <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                        Image Analysis
                      </span>
                      
                      <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                        Automatic Annotation
                      </span>
                      
                    </div>
                    
                  </div>
                  
                  <!-- Publication links -->
                  <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                    
                    <a href="https://doi.org/10.1007/978-3-642-39094-4_70" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                      <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                      </svg>
                      DOI
                    </a>
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/ICIAR13.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                      <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                      </svg>
                      PDF
                    </a>
                    
                    
                  </div>
                </div>
              </li>
              
              <li class="py-6 first:pt-0 ">
                <div class="flex flex-col md:flex-row">
                  <div class="flex-grow">
                    <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Creating Synthetic IrisCodes to Feed Biometrics Experiments</h3>
                    <p class="text-gray-600 italic mt-1">Hugo Proença, João C. Neves</p>
                    <p class="text-gray-700 mt-1">Proceedings of the 2013 IEEE Workshop on Biometric Measurements and Systems for Security and Medical Applications - BioMS 2013 (2013)</p>
                    
                    
                    <details class="mt-3 group">
                      <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                        </svg>
                        Show Abstract
                      </summary>
                      <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                        The collection of iris data suitable to be used in experiments is difficult, mainly due to two factors: 1) the time spent by volunteers in the acquisition process; and 2) security / privacy concerns of volunteers. Even though there are methods to create images of artificial irises, there is no method exclusively focused in the synthesis of the iris biometric signatures (IrisCodes). In experiments related with some phases of the biometric recognition process (e.g., indexing / retrieval), a large number of signatures is required for proper evaluation, which, in case of real data, is extremely hard to obtain. Hence, this paper describes a stochastic method to synthesize IrisCodes, based on the notion of data correlation. These artificial signatures can be used to feed experiments on iris recognition, namely on the iris matching, indexing and retrieval phases. We experimentally confirmed that both the genuine and impostor distributions obtained on the artificial data closely resemble the values obtained in data sets of real irises. Finally, another interesting feature is that the method is easily parametrized to mimic IrisCodes extracted from data of varying levels of quality, i.e., ranging from data acquired in high controlled to unconstrained environments.
                      </div>
                    </details>
                    

                    
                    <div class="mt-3">
                      
                      <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                        Iris Recognition
                      </span>
                      
                      <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                        Synthetic Data
                      </span>
                      
                      <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                        IrisCodes
                      </span>
                      
                      <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                        Biometric Experiments
                      </span>
                      
                    </div>
                    
                  </div>
                  
                  <!-- Publication links -->
                  <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                    
                    <a href="https://doi.org/10.1109/BIOMS.2013.6656141" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                      <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                      </svg>
                      DOI
                    </a>
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/BioMS13.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                      <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                      </svg>
                      PDF
                    </a>
                    
                    
                  </div>
                </div>
              </li>
              
              <li class="py-6 first:pt-0 ">
                <div class="flex flex-col md:flex-row">
                  <div class="flex-grow">
                    <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Using Ocular Data for Unconstrained Biometric Recognition</h3>
                    <p class="text-gray-600 italic mt-1">Hugo Proença, Gil Santos, João C. Neves</p>
                    <p class="text-gray-700 mt-1">Face Recognition in Adverse Conditions, Maria De Marsico, Michele Nappi, Massimo Tistarelli (Eds.), Advances in Computational Intelligence and Robotics Book Series (2014)</p>
                    
                    
                    <details class="mt-3 group">
                      <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                        </svg>
                        Show Abstract
                      </summary>
                      <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                        There are several scenarios where a full facial picture cannot be obtained nor the iris properly imaged. For such cases, a good possibility might be to use the ocular region for recognition, which is a relatively new idea and is regarded as a good trade-off between using the whole face or the iris alone. The area in the vicinity of the eyes is designated as periocular and is particularly useful on less constrained conditions, when image acquisition is unreliable, or to avoid iris pattern spoofing. This chapter provides a comprehensive summary of the most relevant research conducted in the scope of ocular (periocular) recognition methods. We compare the main features of the publicly available data sets and summarize the techniques most frequently used in the recognition algorithms. Also, we present the state-of-the-art results in terms of recognition accuracy and discuss the current issues on this topic, together with some directions for further work.
                      </div>
                    </details>
                    

                    
                    <div class="mt-3">
                      
                      <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                        Ocular Biometrics
                      </span>
                      
                      <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                        Periocular Recognition
                      </span>
                      
                      <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                        Unconstrained Biometrics
                      </span>
                      
                    </div>
                    
                  </div>
                  
                  <!-- Publication links -->
                  <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/BC_UOD.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                      <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                      </svg>
                      PDF
                    </a>
                    
                    
                  </div>
                </div>
              </li>
              
              
              
              <div id="hidden-publications" class="hidden">
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Biometric Identification from Facial Sketches of Poor Fidelity: Comparison of Human and Machine Performance</h3>
                      <p class="text-gray-600 italic mt-1">Hugo Proença, João C. Neves, João Sequeiros, Nuno Carapito, Nuno C. Garcia</p>
                      <p class="text-gray-700 mt-1">Signal and Image Processing for Biometrics : State of the Art and Recent Advances, Jacob Scharcanski, Hugo Proença, Eliza Yingzi Du (Eds.), Springer Verlag book series (2014)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Facial sketch recognition refers to the establishment of a link between a drew representation of a human face and an identity, based on information given by a eyewitness of some illegal act. It is a topic of growing interest, and various software frameworks to synthesize sketches are available nowadays. When com- pared to the traditional hand-made sketches, such sketches resemble more closely the appearance of real mugshots, and led to the possibility of using automated face recognition methods in the identification task. However, there are often deficiencies of witnesses in describing the subjects' appearance, which might bias the main features of sketches with respect to the corresponding identity. This chapter com- pares the human and machine performance in the task of sketch identification (rank- 1 identification). One hundred subjects were considered as gallery data, and five images from each stored in a database. Also, one hundred sketches were drew by non-professionals and used as probe data, each of these resembling an identity in the gallery set. Next, a set of volunteers was asked to identify each sketch, and their answers compared to the rank-1 identification responses given by automated face recognition techniques. Three appearance-based face recognition algorithms were used: 1) Gabor-based description, with l2 norm distance ; 2) sparse representation for classification; and 3) eigenfaces. The sparse representation for classification algorithm yielded the best results, whereas the responses given by the Gabor-based description algorithm were the most correlated to human responses.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Facial Sketches
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Biometric Identification
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Human vs Machine Performance
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/BC_Sketches.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Face Recognition: Handling Data Misalignments Implicitly by Fusion of Sparse Representations</h3>
                      <p class="text-gray-600 italic mt-1">Hugo Proença, João C. Neves, Juan C. Moreno</p>
                      <p class="text-gray-700 mt-1">IET Computer Vision (2014)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Sparse representations for classification (SRC) are considered a relevant advance to the biometrics field, but are particularly sensitive to data misalignments. In previous studies, such misalignments were compensated for by finding appropriate geometric transforms between the elements in the dictionary and the query image, which is costly in terms of computational burden. This study describes an algorithm that compensates for data misalignments in SRC in an implicit way, that is, without finding/applying any geometric transform at every recognition attempt. The authors' study is based on three concepts: (i) sparse representations; (ii) projections on orthogonal subspaces; and (iii) discriminant locality preserving with maximum margin projections. When compared with the classical SRC algorithm, apart from providing slightly better performance, the proposed method is much more robust against global/local data misalignments. In addition, it attains performance close to the state-of-the-art algorithms at a much lower computational cost, offering a potential solution for real- time scenarios and large-scale applications.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Face Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Sparse Representations
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Data Misalignments
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Fusion
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1049/iet-cvi.2014.0039" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/FaceSparse_IETCV.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">A Master-slave Calibration Algorithm with Fish-eye Correction</h3>
                      <p class="text-gray-600 italic mt-1">João C. Neves, Juan C. Moreno, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">Hindawi Mathematical Problems in Engineering (2015)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Surveillance systems capable of autonomously monitoring vast areas are an emerging trend, particularly when wide-angle cameras are combined with pan-tilt-zoom (PTZ) cameras in a master-slave configuration. The use of fish-eye lenses allows the master camera to maximize the coverage area while the PTZ acts as a foveal sensor, providing high-resolution images of regions of interest. Despite the advantages of this architecture, the mapping between image coordinates and pan-tilt values is the major bottleneck in such systems, since it depends on depth information and fish-eye effect correction. In this paper, we address these problems by exploiting geometric cues to perform height estimation. This information is used both for inferring 3D information from a single static camera deployed on an arbitrary position and for determining lens parameters to remove fish-eye distortion. When compared with the previous approaches, our method has the following advantages: (1) fish-eye distortion is corrected without relying on calibration patterns; (2) 3D information is inferred from a single static camera disposed on an arbitrary location of the scene.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Master-slave System
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Calibration
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Fish-eye Correction
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Surveillance
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1155/2015/427270" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/MasterSlaveCalibration_MPE.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Segmenting the Periocular Region using a Hierarchical Graphical Model Fed by Texture / Shape Information and Geometrical Constraints</h3>
                      <p class="text-gray-600 italic mt-1">Hugo Proença, João C. Neves, Gil Santos</p>
                      <p class="text-gray-700 mt-1">Proceedings of the International Joint Conference on Biometrics - IJCB 2014 (2014)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Using the periocular region for biometric recognition is an interesting possibility: this area of the human body is highly discriminative among subjects and relatively stable in appearance. In this paper, the main idea is that improved solutions for defining the periocular region-of-interest and better pose / gaze estimates can be obtained by segment- ing (labelling) all the components in the periocular vicinity. Accordingly, we describe an integrated algorithm for labelling the periocular region, that uses a unique model to discriminate between seven components in a single-shot: iris, sclera, eyelashes, eyebrows, hair, skin and glasses. Our solution fuses texture / shape descriptors and geometrical constraints to feed a two-layered graphical model (Markov Random Field), which energy minimization provides a robust solution against uncontrolled lighting conditions and variations in subjects pose and gaze.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Periocular Region
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Segmentation
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Graphical Model
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Markov Random Field
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/BTAS.2014.6996228" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://di.ubi.pt/~hugomcp/doc/IJCB14.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Detection and Separation of Overlapping Cells Based on Contour Concavity for Leishmania images</h3>
                      <p class="text-gray-600 italic mt-1">João C. Neves, Helena Castro, Ana Tomás, Miguel Coimbra, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">Wiley Cytometry: Part A (2014)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Life scientists often must count cells in microscopy images, which is a tedious and time-consuming task. Automatic approaches present a solution to this problem. Several procedures have been devised for this task, but the majority suffer from performance degradation in the case of cell overlap. In this article, we propose a method to deter- mine the positions of macrophages and parasites in fluorescence images of Leishmania- infected macrophages. The proposed strategy is primarily based on blob detection, clustering, and separation using concave regions of the cells' contours. In comparison with the approaches of Nogueira (Master's thesis, Department of University of Porto Computer Science, 2011) and Leal et al. (Proceedings of the 9th international conference on Image Analysis and Recognition, Vol. II, ICIAR'12. Berlin, Heidelberg: Springer-Verlag; 2012. pp. 432–439), which also addressed this type of image, we conclude that the proposed methodology achieves better performance in the automatic annotation of Leishmania infections.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Cell Detection
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Image Processing
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Contour Concavity
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Leishmania
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1002/cyto.a.22465" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/Leishmania_CytometryA.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Dynamic Camera Scheduling for Visual Surveillance in Crowded Scenes using Markov Random Fields</h3>
                      <p class="text-gray-600 italic mt-1">João C. Neves, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">Proceedings of the 12th IEEE International Conference on Advanced Video and Signal based Surveillance - AVSS 2015 (2015)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          The use of pan-tilt-zoom (PTZ) cameras for capturing high-resolution data of human-beings is an emerging trend in surveillance systems. However, this new paradigm en- tails additional challenges, such as camera scheduling, that can dramatically affect the performance of the system. In this paper, we present a camera scheduling approach capable of determining - in real-time - the sequence of acquisitions that maximizes the number of different targets obtained, while minimizing the cumulative transition time. Our approach models the problem as an undirected graphical model (Markov random field, MRF), which energy minimization can approximate the shortest tour to visit the maximum number of targets. A comparative analysis with the state-of-the-art camera scheduling methods evidences that our approach is able to improve the observation rate while maintaining a competitive tour time.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Camera Scheduling
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          PTZ Camera
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Visual Surveillance
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Markov Random Fields
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/AVSS.2015.7301790" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">A Calibration Algorithm for Multi-camera Visual Surveillance Systems Based on Single-View Metrology</h3>
                      <p class="text-gray-600 italic mt-1">João C. Neves, Juan C. Moreno, Silvio Barra, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">Proceedings of the 7th Iberian Conference on Pattern Recognition and Image Analysis - IbPRIA 2015 (2015)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          The growing concerns about persons security and the increasing popularity of pan-tilt-zoom (PTZ) cameras, have been raising the interest on automated master-slave surveillance systems. Such systems are typically composed by (1) a fixed wide-angle camera that covers a large area, detects and tracks moving objects in the scene; and (2) a PTZ camera, that provides a close-up view of an object of interest. Previously published approaches attempted to establish 2D correspondences between the video streams of both cameras, which is a ill-posed formulation due to the absence of depth information. On the other side, 3D-based approaches are more accurate but require more than one fixed camera to estimate depth information. In this paper, we describe a novel method for easy and precise calibration of a master-slave surveillance sys- tem, composed by a single fixed wide-angle camera. Our method exploits single view metrology to infer 3D data of the tracked humans and to self- perform the transformation between camera views. Experimental results in both simulated and realistic scenes point for the effectiveness of the proposed model in comparison with the state-of-the-art.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Camera Calibration
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Visual Surveillance
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Single-View Metrology
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Master-slave System
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1007/978-3-319-19390-8_62" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://di.ubi.pt/~hugomcp/doc/ibpria15.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Evaluation of Background Subtraction Algorithms for Human Visual Surveillance</h3>
                      <p class="text-gray-600 italic mt-1">João C. Neves, Kamila Wysoczanska, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">Proceedings of the International Conference on Signal and Image Processing Applications – ICSIPA 2015 (2015)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          The fully automated surveillance of human beings remains an open problem, particularly for in-the-wild scenarios, i.e., for complex backgrounds and under uncontrolled lighting conditions. Background Subtraction (BGS) is typically the first phase of the processing chain of such type of systems and holds the feasibility of all the subsequent phases. Hence, it is particularly important to perceive the relative effectiveness of BGS, with respect to the kind of environment. This paper gives an objective evaluation of the state-of-the-art BGS algorithms on unconstrained outdoor environments. When compared to similar published works, the major novelties are two-fold: 1) the focus is put on scenes populated by human beings; and 2) an objective measure of the wildness of environments is proposed, that strongly correlates to BGS performance, and enables to perceive the algorithms' robustness with respect to the environment complexity. As main conclusions, we observed that the SOBS algorithm outperforms the remaining methods. Nevertheless, its performance leads to conclude that BGS in unconstrained environments is still an open problem.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Background Subtraction
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Visual Surveillance
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Human Detection
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/ICSIPA15.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Acquiring High-resolution Face Images in Outdoor Environments: A master-slave Calibration Algorithm</h3>
                      <p class="text-gray-600 italic mt-1">João C. Neves, Juan C. Moreno, Silvio Barra, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">Proceedings of the IEEE Seventh International Conference on Biometrics: Theory, Applications and Systems – BTAS 2015 (2015)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Facial recognition at-a-distance in surveillance scenarios remains an open problem, particularly due to the small number of pixels representing the facial region. The use of pan-tilt-zoom (PTZ) cameras has been advocated to solve this problem, however, the existing approaches either rely on rough approximations or additional constraints to estimate the mapping between image coordinates and pan-tilt parameters. In this paper, we aim at extending PTZ-assisted facial recognition to surveillance scenarios by proposing a master-slave calibration algorithm capable of accurately estimating pan-tilt parameters without depending on additional constraints. Our approach exploits geometric cues to automatically estimate subjects height and thus determine their 3D position. Experimental results show that the presented algorithm is able to acquire high-resolution face im- ages at a distance ranging from 5 to 40 meters with high success rate. Additionally, we certify the applicability of the aforementioned algorithm to biometric recognition through a face recognition test, comprising 20 probe subjects and 13,020 gallery subjects.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Face Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Surveillance
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          PTZ Camera
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Master-slave Calibration
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/BTAS.2015.7358744" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Quis-Campi: Extending In The Wild Biometric Recognition to Surveillance Environments</h3>
                      <p class="text-gray-600 italic mt-1">Gil Santos, João C. Neves, Sílvio Filipe, Emanuel Grancho, Silvio Barra, Fabio Narducci, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">Proceedings of the 18th International Conference on Image Analysis and Processing - ICIAP 2015 (2015)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Efforts in biometrics are being held into extending robust recognition techniques to in the wild scenarios. Nonetheless, and despite being a very attractive goal, human identification in the surveillance con- text remains an open problem. In this paper, we introduce a novel bio- metric system – Quis-Campi – that effectively bridges the gap between surveillance and biometric recognition while having a minimum amount of operational restrictions. We propose a fully automated surveillance sys- tem for human recognition purposes, attained by combining human detection and tracking, further enhanced by a PTZ camera that delivers data with enough quality to perform biometric recognition. Along with the system concept, implementation details for both hardware and software modules are provided, as well as preliminary results over a real scenario.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Biometric Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Surveillance
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          In-the-Wild
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Human Detection
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Tracking
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1007/978-3-319-23222-5_8" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://di.ubi.pt/~hugomcp/doc/ICIAP15.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Biometric Recognition in Surveillance Scenarios: A Survey</h3>
                      <p class="text-gray-600 italic mt-1">João C. Neves, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">Springer Artificial Intelligence Review (2016)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Interest in the security of individuals has increased in recent years. This increase has in turn led to much wider deployment of surveillance cameras worldwide, and consequently, automated surveillance systems research has received more attention from the scientific community than before. Concurrently, biometrics research has become more popular as well, and it is supported by the increasing number of approaches devised to address specific degradation factors of unconstrained environments. Despite these recent efforts, no automated surveillance system that performs reliable biometric recognition in such an environment has become available. Nevertheless, recent developments in human motion analysis and biometric recognition suggest that both can be combined to develop a fully automated system. As such, this paper reviews recent advances in both areas, with a special focus on surveillance scenarios. When compared to previous studies, we highlight two distinct features, i.e., (1) our emphasis is on approaches that are devised to work in unconstrained environments and surveillance scenarios; and (2) biometric recognition is the final goal of the surveillance system, as opposed to behavior analysis, anomaly detection or action recognition.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Biometric Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Surveillance
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Survey
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Unconstrained Environments
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1007/s10462-016-9474-x" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/BiometricsSurveillance_Survey.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Iris Biometric Indexing</h3>
                      <p class="text-gray-600 italic mt-1">Hugo Proença, João C. Neves</p>
                      <p class="text-gray-700 mt-1">Iris and Periocular Biometric Recognition (2016)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Indexing / retrieving sets of iris biometric signatures has been a topic of increasing popularity, mostly due to the deployment of iris recognition systems in nationwide scale scenarios. In these conditions, for each identification attempt, there might exist hundreds of millions of enrolled identities and is unrealistic to match the probe against all gallery elements in a reasonable amount of time. Hence, the idea of indexing / retrieval is - upon receiving one sample - to find in a quick way a sub-set of elements in the database that most probably contains the identity of interest, i.e., the one corresponding to the probe. Most of the state-of-the-art strategies to index iris biometric signatures were devised to decision environments with a clear separation between genuine and impostor matching scores. However, if iris recognition systems work in low quality data, the resulting decision environments are poorly separable, with a significant overlap between the distributions of both matching scores. This chapter summarises the state-of-the-art in terms of iris bio- metric indexing / retrieval and focuses in an indexing / retrieval method for such low quality data and operates at the code level, i.e., after the signature encoding process. Gallery codes are decomposed at multiple scales, and using the most reliable components of each scale, their position in a n-ary tree is determined. During retrieval, the probe is decomposed similarly, and the distances to multi-scale centroids are used to penalize paths in the tree. At the end, only a subset of branches is traversed up to the last level.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Iris Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Biometric Indexing
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Retrieval
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/BC_II.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Visible-wavelength Iris/Periocular Imaging and Recognition in Surveillance Environments</h3>
                      <p class="text-gray-600 italic mt-1">Hugo Proença, João C. Neves</p>
                      <p class="text-gray-700 mt-1">Elsevier Image and Vision Computing (2016)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Visual surveillance cameras have been massively deployed in public urban environments over the recent years, as a crime prevention and law enforcement solution. This fact raised the interest in developing automata to infer useful information from such crowded scenes (from abnormal behavior detection to human identification). In order to cover wide outdoor areas, one interesting possibility is to combine wide- angle and pan–tilt–zoom (PTZ) cameras in a master–slave configuration. The use of fish-eye lenses allows the master camera to maximize the coverage area while the PTZ acts as a foveal sensor, providing high- resolution images of the interest regions. This paper addresses the feasibility of using this type of data acquisition paradigm for imaging iris/periocular data with enough discriminating power to be used for biometric recognition purposes.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Visual Surveillance
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Iris Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Periocular Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Visible Wavelength
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1016/j.imavis.2016.03.015" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/PeriocularImagingSurveillance.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Fusing Vantage Point Trees and Linear Discriminants for Fast Feature Classification</h3>
                      <p class="text-gray-600 italic mt-1">Hugo Proença, João C. Neves</p>
                      <p class="text-gray-700 mt-1">Springer Journal of Classification (2017)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          This paper describes a classification strategy that can be regarded as a more general form of nearest-neighbor classification. It fuses the concepts of nearest neighbor, linear discriminant and Vantage-Point trees, yielding an efficient indexing data structure and classification algorithm. In the learning phase, we define a set of disjoint subspaces of reduced complexity that can be separated by linear discriminants, ending up with an ensemble of simple (weak) classifiers that work locally. In classification, the closest centroids to the query determine the set of classifiers considered, which responses are weighted. The algorithm was experimentally validated in datasets widely used in the field, attaining error rates that are favourably compara- ble to the state-of-the-art classification techniques. Lastly, the proposed solution has a set of interesting properties for a broad range of applications: 1) it is deterministic; 2) it classifies in time approximately logarithmic with respect to the size of the learning set, being far more efficient than nearest neighbor classification in terms of computational cost; and 3) it keeps the generalization ability of simple models.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Classification
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Nearest Neighbor
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Linear Discriminant
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Vantage-Point trees
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1007/s00357-017-9223-0" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/VPC.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Joint Head Pose/Soft Label Estimation for Human Recognition In-The-Wild</h3>
                      <p class="text-gray-600 italic mt-1">Hugo Proença, João C. Neves, Silvio Barra, Tiago Marques, Juan C. Moreno</p>
                      <p class="text-gray-700 mt-1">IEEE Transactions on Pattern Analysis and Machine Intelligence (2016)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Soft biometrics have been emerging to complement other traits and are particularly useful for poor quality data. In this paper, we propose an efficient algorithm to estimate human head poses and to infer soft biometric labels based on the 3D morphology of the human head. Starting by considering a set of pose hypotheses, we use a learning set of head shapes synthesized from anthropometric surveys to derive a set of 3D head centroids that constitutes a metric space. Next, representing queries by sets of 2D head landmarks, we use projective geometry techniques to rank efficiently the joint 3D head centroids / pose hypotheses according to their likelihood of matching each query. The rationale is that the most likely hypotheses are sufficiently close to the query, so a good solution can be found by convex energy minimization techniques. Once a solution has been found, the 3D head centroid and the query are assumed to have similar morphology, yielding the soft label. Our experiments point toward the usefulness of the proposed solution, which can improve the effectiveness of face recognizers and can also be used as a privacy-preserving solution for biometric recognition in public environments.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Soft Biometrics
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Head Pose Estimation
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Human Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          In-the-Wild
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/TPAMI.2016.2522441" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/SoftBiometrics.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">ICB-RW 2016: International Challenge on Biometric Recognition in the Wild</h3>
                      <p class="text-gray-600 italic mt-1">João C. Neves, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">Proceedings of the 9th IAPR International Conference on Biometrics - ICB 2016 (2016)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Biometric recognition in totally wild conditions, such as the observed in visual surveillance scenarios has not been achieved yet. The ICB-RW competition was promoted to support this endeavor, being the first biometric challenge carried out in data that realistically result from surveillance scenarios. The competition relied on an innovative master- slave surveillance system for the acquisition of face imagery at-a-distance and on-the-move. This paper describes the competition details and reports the performance achieved by the participants algorithms.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Biometric Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Wild Conditions
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Surveillance
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/ICB.2016.7550066" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/ICB16.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Soft Biometrics: Globally Coherent Solutions for Hair Segmentation and Style Recognition based on Hierarchical MRFs</h3>
                      <p class="text-gray-600 italic mt-1">Hugo Proença, João C. Neves</p>
                      <p class="text-gray-700 mt-1">IEEE Transactions on Information Forensics and Security (2017)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Markov Random Fields (MRFs) are a popular tool in many computer vision problems and faithfully model a broad range of local dependencies. However, rooted in the Hammersley-Clifford theorem, they face serious difficulties in enforcing the global coherence of the solutions without using too high order cliques that reduce the computational effectiveness of the inference phase. Having this problem in mind, we describe a multi-layered (hierarchical) architecture for MRFs that is based exclusively in pairwise connections and typically produces globally coherent solutions, with 1) one layer working at the local (pixel) level, modelling the interactions between adjacent image patches; and 2) a complementary layer working at the object (hypothesis) level pushing toward globally consistent solutions. During optimization, both layers interact into an equilibrium state, that not only segments the data, but also classifies it. The proposed MRF architecture is particularly suitable for problems that deal with biological data (e.g., biometrics), where the reasonability of the solutions can be objectively measured. As test case, we considered the problem of hair / facial hair segmentation and labelling, which are soft biometric labels useful for human recognition in-the-wild. We observed performance levels close to the state-of-the-art at a much lower computational cost, both in the segmentation and classification (labelling) tasks.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Soft Biometrics
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Hair Segmentation
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Hair Style Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Markov Random Fields
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/TIFS.2017.2680246" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/HairAnalysis_TIFS.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">IRINA: Iris Recognition (even) in Innacurately Segmented Data</h3>
                      <p class="text-gray-600 italic mt-1">Hugo Proença, João C. Neves</p>
                      <p class="text-gray-700 mt-1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition - CVPR 2017 (2017)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          The effectiveness of current iris recognition systems depends on the accurate segmentation and parameterisation of the iris boundaries, as failures at this point misalign the coefficients of the biometric signatures. This paper describes IRINA, an algorithm for Iris Recognition that is robust against INAccurately segmented samples, which makes it a good candidate to work in poor-quality data. The process is based in the concept of 'corresponding' patch between pairs of images, that is used to estimate the posterior probabilities that patches regard the same biological region, even in case of segmentation errors and non-linear texture deformations. Such information enables to infer a free-form deformation field (2D registration vectors) between images, whose first and second-order statistics provide effective biometric discriminating power. Extensive experiments were carried out in four datasets (CASIA-IrisV3-Lamp, CASIA-IrisV4-Lamp, CASIA-IrisV4-Thousand and WVU) and show that IRINA not only achieves state-of-the-art performance in good quality data, but also handles effectively severe segmentation errors and large differences in pupillary dilation / constriction.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Iris Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Segmentation Robustness
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Biometrics
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Image Registration
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/CVPR.2017.714" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/CVPR2017.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">QUIS-CAMPI: An Annotated Multi-biometrics Data Feed From Surveillance Scenarios</h3>
                      <p class="text-gray-600 italic mt-1">João C. Neves, Juan C. Moreno, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">IET Biometrics (2017)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          The accuracy of biometric recognition in unconstrained scenarios has been a major concern for a large number of researchers. Despite such efforts, no system can recognize in a fully automated manner human beings in totally wild conditions, such as in surveillance environments. In this context, several sets of degraded data have been made available to the research community, where the reported performance by state-of-the-art algorithms is already saturated, suggesting that these sets do not reflect faithfully the conditions in such hard settings. To this end, we introduce the QUIS-CAMPI data feed, comprising samples automatically acquired by an outdoor visual surveillance system, with subjects on-the-move and at-a-distance (up to 50 m). Also, we supply a high-quality set of enrollment data. When compared to similar data sources, the major novelties of QUIS-CAMPI are: 1) biometric samples are acquired in a fully automatic way; 2) it is an open dataset, i.e., the number of probe images and enrolled subjects grow on a daily basis; and 3) it contains multi-biometric traits. The ensemble properties of QUIS-CAMPI ensure that the data span a representative set of covariate factors of real-world scenarios, making it a valuable tool for developing and benchmarking biometric recognition algorithms capable of working in unconstrained scenarios.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Multi-biometrics
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Surveillance
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Data Feed
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Outdoor Recognition
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1049/iet-bmt.2016.0178" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/QUISCAMPI_IET.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Exploiting Data Redundancy for Error Detection in Degraded Biometric Signatures Resulting From in the Wild Environments</h3>
                      <p class="text-gray-600 italic mt-1">João C. Neves, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">Proceedings of the 2nd International Workshop on Biometrics in the Wild, 12th IEEE Conference on Automatic Face and Gesture Recognition – FG 2017 (2017)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          An error-correcting code (ECC) is a process of adding redundant data to a message, such that it can be recovered by a receiver even if a number of errors are introduced in transmission. Inspired by the principles of ECC, we introduce a method capable of detecting degraded features in biometric signatures by exploiting feature correlation. The main novelty is that, unlike existing biometric cryptosystems, the proposed method works directly on the biometric signature. Our approach performs a redundancy analysis of non-degraded data to build an undirected graphical model (Markov Random Field), whose energy minimization determines the sequence of degraded components of the biometric sample. Experiments carried out in different biometric traits ascertain the improvements attained when disregarding degraded features during the matching phase. Also, we stress that the proposed method is general enough to work in different classification methods, such as CNNs.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Error Detection
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Biometric Signatures
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Markov Random Field
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Feature Correlation
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/FG.2017.122" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/fg2017.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Deep-PRWIS: Periocular Recognition Without the Iris and Sclera Using Deep Learning Frameworks</h3>
                      <p class="text-gray-600 italic mt-1">Hugo Proença, João C. Neves</p>
                      <p class="text-gray-700 mt-1">IEEE Transactions on Information Forensics and Security (2018)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          This work is based on a disruptive hypothesis for periocular biometrics: in visible-light data, the recognition performance is optimized when the components inside the ocular globe (the iris and the sclera) are simply discarded, and the recogniser's response is exclusively based in information from the surroundings of the eye. As major novelty, we describe a processing chain based on convolution neural networks (CNNs) that defines the regions-of-interest in the input data that should be privileged in an implicit way, i.e., without masking out any areas in the learning/test samples. By using an ocular segmentation algorithm exclusively in the learning data, we separate the ocular from the periocular parts. Then, we produce a large set of 'multi-class' artificial samples, by interchanging the periocular and ocular parts from different subjects. These samples are used for data augmentation purposes and feed the learning phase of the CNN, always considering as label the ID of the periocular part. This way, for every periocular region, the CNN receives multiple samples of different ocular classes, forcing it to conclude that such regions should not be considered in its response. During the test phase, samples are provided without any segmentation mask and the network naturally disregards the ocular components, which contributes for improvements in performance. Our experiments were carried out in full versions of two widely known data sets (UBIRIS.v2 and FRGC) and show that the proposed method consistently advances the state-of-the-art performance in the closed-world setting, reducing the EERs in about 82% (UBIRIS.v2) and 85% (FRGC) and improving the Rank-1 over 41% (UBIRIS.v2) and 12% (FRGC).
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Periocular Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Deep Learning
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Data Augmentation
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Biometrics
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/TIFS.2017.2771230" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/Deep-PRWIS.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">"A Leopard Cannot Change Its Spots": Improving Face Recognition Using 3D-based Caricatures</h3>
                      <p class="text-gray-600 italic mt-1">João C. Neves, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">IEEE Transactions on Information Forensics and Security (2018)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Caricatures refer to a representation of a person in which the distinctive features are deliberately exaggerated, with several studies showing that humans perform better at recognizing people from caricatures than using original images. Inspired by this observation, this paper introduces the first fully automated caricature-based face recognition approach capable of working with data acquired in the wild. Our approach leverages the 3D face structure from a single 2D image and compares it to a reference model for obtaining a compact representation of face features deviations. This descriptor is subsequently deformed using a 'measure locally, weight globally' strategy to resemble the caricature drawing process. The deformed deviations are incorporated in the 3D model using the Laplacian mesh deformation algorithm, and the 2D face caricature image is obtained by projecting the deformed model in the original camera-view. To demonstrate the advantages of caricature-based face recognition, we train the VGG-Face network from scratch using either original face images (baseline) or caricatured images, and use these models for extracting face descriptors from the LFW, IJB-A and MegaFace datasets. The experiments show an increase in the recognition accuracy when using caricatures rather than original images. Moreover, our approach achieves competitive results with state-of-the-art face recognition methods, even without explicitly tuning the network for any of the evaluation sets.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Face Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          3D Caricatures
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Deep Learning
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Biometrics
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/TIFS.2018.2846617" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/Leopard_TIFS.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">A Reminiscence of "Mastermind": Iris/Periocular Biometrics by "In-Set" CNN Iterative Analysis</h3>
                      <p class="text-gray-600 italic mt-1">Hugo Proença, João C. Neves</p>
                      <p class="text-gray-700 mt-1">IEEE Transactions on Information Forensics and Security (2019)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Convolutional neural networks (CNNs) have emerged as the most popular classification models in biometrics research. Under the discriminative paradigm of pattern recognition, CNNs are used typically in one of two ways: 1) verification mode ('are samples from the same person?'), where pairs of images are provided to the network to distinguish between genuine and impostor instances; and 2) identification mode ('whom is this sample from?'), where appropriate feature representations that map images to identities are found. This paper postulates a novel mode for using CNNs in biometric identification, by learning models that answer to the question 'is the query's identity among this set?'. The insight is a reminiscence of the classical Mastermind game: by iteratively analysing the network responses when multiple random samples of k gallery elements are compared to the query, we obtain weakly correlated matching scores that - altogether - provide solid cues to infer the most likely identity. In this setting, identification is regarded as a variable selection and regularization problem, with sparse linear regression techniques being used to infer the matching probability with respect to each gallery identity. As main strength, this strategy is highly robust to outlier matching scores, which are known to be a primary error source in biometric recognition.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Iris Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Periocular Biometrics
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Convolutional Neural Networks
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Iterative Analysis
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/TIFS.2018.2883853" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/Mastermind_TIFS.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Biometric Recognition in Surveillance Environments Using Master-Slave Architectures</h3>
                      <p class="text-gray-600 italic mt-1">Hugo Proença, João C. Neves</p>
                      <p class="text-gray-700 mt-1">Proceedings of the 31st Conference on Graphics, Patterns and Images- SIBGRAPI 2018 (2018)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          The number of visual surveillance systems deployed worldwide has been growing astoundingly. As a result, attempts have been made to increase the levels of automated analysis of such systems, towards the reliable recognition of human beings in fully covert conditions. Among other possibilities, master-slave architectures can be used to acquire high resolution data of subjects heads from large distances, with enough resolution to perform face recognition. This paper/tutorial provides a comprehensive overview of the major phases behind the development of a recognition system working in outdoor surveillance scenarios, describing frameworks and methods to: 1) use coupled wide view and Pan-Tilt-Zoom (PTZ) imaging devices in surveillance settings, with a wide-view camera covering the whole scene, while a synchronized PTZ device collects high-resolution data from the head region; 2) use soft biometric information (e.g., body metrology and gait) for pruning the set of potential identities for each query; and 3) faithfully balance ethics/privacy and safety/security issues in this kind of systems.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Biometric Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Surveillance
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Master-Slave Architecture
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          PTZ Camera
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/SIBGRAPI.2018.00068" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/SIBGRAPI2018.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Region-Based CNNs for Pedestrian Gender Recognition in Visual Surveillance Environments</h3>
                      <p class="text-gray-600 italic mt-1">Ehsan Yaghoubi, Pendar Alirezazadeh, Eduardo Assunção, João C. Neves, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">Proceedings of the 18th International Conference of the Biometrics Special Interest Group – BIOSIG 2019 (2019)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Inferring soft biometric labels in totally uncontrolled outdoor environments, such as surveillance scenarios, remains a challenge due to the low resolution of data and its covariates that might seriously compromise performance (e.g., occlusions and subjects pose). In this kind of data, even state-of-the-art deep-learning frameworks (such as ResNet) working in a holistic way, attain relatively poor performance, which was the main motivation for the work described in this paper. In particular, having noticed the main effect of the subjects' 'pose' factor, in this paper we describe a method that uses the body keypoints to estimate the subjects pose and define a set of regions of interest (e.g., head, torso, and legs). This information is used to learn appropriate classification models, specialized in different poses/body parts, which contributes to solid improvements in performance. This conclusion is supported by the experiments we conducted in multiple real-world outdoor scenarios, using the data acquired from advertising panels placed in crowded urban environments.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Gender Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Visual Surveillance
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Region-Based CNN
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Soft Biometrics
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/biosig2019_ehsan.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Segmentation-less and Non-holistic Deep-Learning Framework for Iris Recognition</h3>
                      <p class="text-gray-600 italic mt-1">Hugo Proença, João C. Neves</p>
                      <p class="text-gray-700 mt-1">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 'Bias Estimation in Face Analytics' Workshop, – CVPRW 2019 (2019)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Driven by the pioneer iris biometrics approach, the most relevant recognition methods published over the years are 'phase-based', and segment/normalize the iris to obtain dimensionless representations of the data that attenuate the differences in scale, translation, rotation and pupillary dilation. In this paper we present a recognition method that dispenses the iris segmentation, noise detection and normalization phases, and is agnostic to the levels of pupillary dilation, while maintaining state-of-the-art performance. Based on deep-learning classification models, we analyze the displacements between biologically corresponding patches in pairs of iris images, to discriminate between genuine and impostor comparisons. Such corresponding patches are firstly learned in the normalized representations of the irises - the domain where they are optimally distinguishable - but are remapped into a segmentation-less polar coordinate system that uniquely requires iris detection. In recognition time, samples are only converted into this segmentation-less coordinate system, where matching is performed. In the experiments, we considered the challenging open-world setting, and used three well known data sets (CASIA-4-Lamp, CASIA-4-Thousand and WVU), concluding positively about the effectiveness of the proposed algorithm, particularly in cases where accurately segmenting the iris is a challenge.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Iris Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Segmentation-less
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Deep Learning
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Biometrics
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/CVPRW.2019.00283" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/CVPRW19.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">FaceGenderID: Exploiting Gender Information in DCNNs Face Recognition Systems</h3>
                      <p class="text-gray-600 italic mt-1">Marta Blásquez, Aythami Morales, Ester Gonzalez, João C. Neves, Hugo Proença, Rúben Vera-Rodriguez</p>
                      <p class="text-gray-700 mt-1">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 'Biometrics' Workshop – CVPRW 2019 (2019)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          This paper addresses the effect of gender as a covariate in face verification systems. Even though pre-trained models based on Deep Convolutional Neural Networks (DCNNs), such as VGG-Face or ResNet-50, achieve very high performance, they are trained on very large datasets comprising millions of images, which have biases regarding demographic aspects like the gender and the ethnicity among others. In this work, we first analyse the separate performance of these state-of-the-art models for males and females. We observe a gap between face verification performances obtained by both gender classes. These results suggest that features obtained by biased models are affected by the gender covariate. We propose a gender-dependent training approach to improve the feature representation for both genders, and develop both: i) gender specific DCNNs models, and ii) a gender balanced DCNNs model. Our results show significant and consistent improvements in face verification performance for both genders, individually and in general with our proposed approach. Finally, we announce the availability (at GitHub) of the FaceGenderID DCNNs models proposed in this work, which can support further experiments on this topic.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Face Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Gender Bias
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          DCNNs
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Biometrics
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/CVPRW.2019.00278" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/cvprw_2019.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Human Attribute Recognition: A Comprehensive Survey</h3>
                      <p class="text-gray-600 italic mt-1">Ehsan Yaghoubi, Farhad Khezeli, Diana Borza, SV Aruna Kumar, João C. Neves, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">MDPI Applied Sciences (2020)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Human Attribute Recognition (HAR) is a highly active research field in computer vision and pattern recognition domains with various applications such as surveillance or fashion. Several approaches have been proposed to tackle the particular challenges in HAR. However, these approaches have dramatically changed over the last decade, mainly due to the improvements brought by deep learning solutions. To provide insights for future algorithm design and dataset collections, in this survey, (1) we provide an in-depth analysis of existing HAR techniques, concerning the advances proposed to address the HAR's main challenges; (2) we provide a comprehensive discussion over the publicly available datasets for the development and evaluation of novel HAR approaches; (3) we outline the applications and typical evaluation metrics used in the HAR context.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Human Attribute Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Survey
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Computer Vision
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Pattern Recognition
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.3390/app10165608" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/Ehsan_MDPI_AS.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">An Attention-Based Deep Learning Model for Multiple Pedestrian Attributes Recognition</h3>
                      <p class="text-gray-600 italic mt-1">Ehsan Yaghoubi, Diana Borza, João C. Neves, SV Aruna Kumar, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">Elsevier Image and Vision Computing (2020)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          The automatic characterization of pedestrians in surveillance footage is a tough challenge, particularly when the data is extremely diverse with cluttered backgrounds, and subjects are captured from varying distances, under multiple poses, with partial occlusion. Having observed that the state-of-the-art performance is still unsatisfactory, this paper provides a novel solution to the problem, with two-fold contributions: 1) considering the strong semantic correlation between the different full-body attributes, we propose a multi-task deep model that uses an element-wise multiplication layer to extract more comprehensive feature representations. In practice, this layer serves as a filter to remove irrelevant background features, and is particularly important to handle complex, cluttered data; and 2) we introduce a weighted-sum term to the loss function that not only relativizes the contribution of each task but also is crucial for performance improvement in multiple-attribute inference settings. Our experiments were performed on two well-known datasets (RAP and PETA) and point for the superiority of the proposed method with respect to the state-of-the-art.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Pedestrian Attributes Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Attention Mechanism
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Deep Learning
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Multi-task Learning
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1016/j.imavis.2020.103981" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">GANprintR: Improved Fakes and Evaluation of the State of the Art in Face Manipulation Detection</h3>
                      <p class="text-gray-600 italic mt-1">João C. Neves, Ruben Tolosana, Ruben Vera-Rodriguez, Vasco Lopes, Hugo Proença, Julian Fierrez</p>
                      <p class="text-gray-700 mt-1">IEEE Journal of Selected Topics in Signal Processing (2020)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          The availability of large-scale facial databases, together with the remarkable progresses of deep learning technologies, in particular Generative Adversarial Networks (GANs), have led to the generation of extremely realistic fake facial content, raising obvious concerns about the potential for misuse. Such concerns have fostered the research on manipulation detection methods that, contrary to humans, have already achieved astonishing results in various scenarios. This chapter is focused on the analysis of GAN fingerprints in face image synthesis. In particular, it covers an in-depth literature analysis of state-of-the-art detection approaches for the entire face synthesis manipulation. It also describes a recent approach to spoof fake detectors based on a GAN-fingerprint Removal autoencoder (GANprintR). A thorough experimental framework is included in the chapter, highlighting (i) the potential of GANprintR to spoof fake detectors, and (ii) the poor generalisation capability of current fake detectors.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Face Manipulation Detection
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          GAN Fingerprints
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Deep Learning
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Digital Forensics
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/JSTSP.2020.3007250" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Pose Switch-Based Convolutional Neural Network for Clothing Analysis in Visual Surveillance Environments</h3>
                      <p class="text-gray-600 italic mt-1">Pendar Alirezazadeh, Ehsan Yaghoubi, Eduardo Assunção, João C. Neves, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">Proceedings of the 18th International Conference of the Biometrics Special Interest Group – BIOSIG 2019 (2019)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Recognizing pedestrian clothing types and styles in outdoor scenes and totally uncontrolled conditions is appealing to emerging applications such as security, intelligent customer profile analysis and computer-aided fashion design. Recognition of clothing categories from videos remains a challenge, mainly due to the poor data resolution and the data covariates that compromise the effectiveness of automated image analysis techniques (e.g., poses, shadows and partial occlusions). While state-of-the-art methods typically analyze clothing attributes without paying attention to variation of human poses, here we claim for the importance of a feature representation derived from human poses to improve classification rate. Estimating the pose of pedestrians is important to fed guided features into recognizing system. In this paper, we introduce pose switch-based convolutional neural network for recognizing the types of clothes of pedestrians, using data acquired in crowded urban environments. In particular, we compare the effectiveness attained when using CNNs without respect to human poses variant, and assess the improvements in performance attained by pose feature extraction. The observed results enable us to conclude that pose information can improve the performance of clothing recognition system. We focus on the key role of pose information in pedestrian clothing analysis, which can be employed as an interesting topic for further works.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Clothing Analysis
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Visual Surveillance
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Pose Estimation
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          CNN
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/biosig2019_pendar.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">New Trends on Pattern Recognition, Applications and Systems</h3>
                      <p class="text-gray-600 italic mt-1">Hugo Proença, João C. Neves</p>
                      <p class="text-gray-700 mt-1">Applied Sciences (2021)</p>
                      
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Pattern Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Special Issue
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Applications
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Systems
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">All-in-one "HairNet": A Deep Neural Model for Joint Hair Segmentation and Characterization</h3>
                      <p class="text-gray-600 italic mt-1">Diana Borza, Ehsan Yaghoubi, João C. Neves, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">Proceedings of the International Joint Conference on Biometrics – IJCB 2020 (2020)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          The hair appearance is among the most valuable soft biometric traits when performing human recognition at-a-distance. Even in degraded data, the hair's appearance is instinctively used by humans to distinguish between individuals. In this paper we propose a multi-task deep neural model capable of segmenting the hair region, while also inferring the hair color, shape and style, all from in-the-wild images. Our main contributions are two-fold: 1) the design of an all-in-one neural network, based on depth-wise separable convolutions to extract the features; and 2) the use convolutional feature masking layer as an attention mechanism that enforces the analysis only within the 'hair' regions. In a conceptual perspective, the strength of our model is that the segmentation mask is used by the other tasks to perceive - at feature-map level - only the regions relevant to the attribute characterization task. This paradigm allows the network to analyze features from non-rectangular areas of the input data, which is particularly important, considering the irregularity of hair regions. Our experiments showed that the proposed approach reaches a hair segmentation performance comparable to the state-of-the-art, having as main advantage the fact of performing multiple levels of analysis in a single-shot paradigm.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Hair Segmentation
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Hair Characterization
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Deep Learning
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Soft Biometrics
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/IJCB48548.2020.9304904" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">GAN Fingerprints in Face Image Synthesis</h3>
                      <p class="text-gray-600 italic mt-1">João C. Neves, Ruben Tolosana, Ruben Vera-Rodriguez, Vasco Lopes, Hugo Proença, Julian Fierrez</p>
                      <p class="text-gray-700 mt-1">Springer-Verlag book series, Lecture Notes on Electrical Engineering (2022)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          The availability of large-scale facial databases, together with the remarkable progresses of deep learning technologies, in particular Generative Adversarial Networks (GANs), have led to the generation of extremely realistic fake facial content, raising obvious concerns about the potential for misuse. Such concerns have fostered the research on manipulation detection methods that, contrary to humans, have already achieved astonishing results in various scenarios. This chapter is focused on the analysis of GAN fingerprints in face image synthesis. In particular, it covers an in-depth literature analysis of state-of-the-art detection approaches for the entire face synthesis manipulation. It also describes a recent approach to spoof fake detectors based on a GAN-fingerprint Removal autoencoder (GANprintR). A thorough experimental framework is included in the chapter, highlighting (i) the potential of GANprintR to spoof fake detectors, and (ii) the poor generalisation capability of current fake detectors.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          GAN Fingerprints
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Face Image Synthesis
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Fake Detection
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/JSTSP.2020.3007250" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/paper-130.png" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Super-resolution Using Stochastic Differential Equations and Potential Applications on Face Recognition</h3>
                      <p class="text-gray-600 italic mt-1">Marcelo Santos, Rayson Laroca, Rafael O. Ribeiro, João C. Neves, Hugo Proença, David Menotti</p>
                      <p class="text-gray-700 mt-1">Proceedings of the 35th Conference on Graphics, Patterns and Images - SIBGRAPI 2022 (2022)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Diffusion models have proven effective for various applications such as images, audio and graphs generation. Other important applications are image super-resolution and the solution of inverse problems. More recently, some works have used stochastic differential equations (SDEs) to generalize diffusion models to continuous time. In this work, we introduce SDE to generate super-resolution face images. To the best of our knowledge, this is the first time SDEs have been used for such an application. The proposed method provides promising peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM) and consistency than existing super-resolution methods based on diffusion models. We also demonstrated the potential applications of this method for the face recognition task. For this purpose, a generic facial feature extractor is used to compare the super-resolution images with the ground truth and superior results were obtained compared with other methods.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Super-resolution
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Stochastic Differential Equations
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Diffusion Models
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Face Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Image Enhancement
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/SIBGRAPI.2022.000xx" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/sibgrapi_22.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Defying Limits: Super-Resolution Refinement with Diffusion Guidance</h3>
                      <p class="text-gray-600 italic mt-1">Marcelo Santos, João C. Neves, Hugo Proença, David Menotti</p>
                      <p class="text-gray-700 mt-1">Proceedings of the 19th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISAPP 2024) (2024)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Facial recognition has become widely applied across diverse environments. However, in unconstrained settings, face images often suffer from undesired effects such as low resolution, leading to a notable decline in recognition performance. The use of super-resolution (SR) algorithms is effective in supporting facial recognition in these cases. In the context of SR and image synthesis, diffusion models have consistently exhibited superior results. Moreover, diffusion models, as a whole, have garnered significant attention in recent years, surpassing the performance of traditional Generative Adversarial Networks (GANs) and achieving remarkable outcomes across various tasks. Additionally, by combining diffusion models with the gradient of a classifier, it becomes possible to generate data from a specific class. In this paper, we employ a diffusion model based on a Stochastic Differential Equation (SDE) to generate refined SR face images with an upsampling factor of 8× and 16× and address the challenges posed by unconstrained environments in facial recognition. The main contribution of our work lies in utilizing the gradient from a classifier to refine the SR results. This is performed by using soft biometrics such as gender and facial features to guide the SR process. To the best of our knowledge, this is the first time classifier guidance has been used to refine SR results of images from surveillance cameras. We conducted experiments on the CelebA and Quis-Campi datasets to evaluate our approach. The refined SR images exhibit enhanced details and improved visual quality. The quantitative performance is assessed using commonly used SR metrics as well as metrics from face recognition.The experimental results demonstrate the superior performance of our SR algorithm, surpassing other existing methods when applied to images from unconstrained scenarios.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Super-Resolution
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Diffusion Models
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Facial Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Stochastic Differential Equation
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Image Enhancement
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/Marcelo_VISAPP_24.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Advancing Manufacturing Energy Efficiency: The Role of AI and Web-Based Tools</h3>
                      <p class="text-gray-600 italic mt-1">Asmae Lamsaf, Pranita Samale, Hugo Proença, João C. Neves, Kailash Hambarde</p>
                      <p class="text-gray-700 mt-1">Proceedings of the 2024 International Conference on Emerging Smart Computing & Informatics (ESCI 2024) (2024)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          This paper introduces a web-based application that simplifies the data analysis processing chain by automating the analysis of arbitrary variables. In particular, our application allows users to easily upload and process data for the analysis of a target variable by exploiting machine learning and evolutionary algorithms for precise forecasting and optimization. We demonstrate the system's efficacy using a dataset from a textile company, where our application successfully predicted the target variables with a high level of R-squared of 0.78, using the best regression model. These results not only highlight its real-world applicability but also played an important role in enhancing sustainable manufacturing practices. This innovative application offers a significant step towards sustainable and efficient manufacturing, addressing the challenges of high energy consumption and environmental impact in the industry.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Manufacturing
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Energy Efficiency
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Artificial Intelligence
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Web-Based Tools
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Sustainability
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/Asmae_ESCI_24.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">CFC-ATE: Causal Feature Construction via Average Treatment Effect</h3>
                      <p class="text-gray-600 italic mt-1">Asmae Lamsaf, João C. Neves, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">IEEE International Conference on Machine Learning and Applications (ICMLA'24) (2024)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Dimensionality reduction is a crucial step in data preprocessing, particularly for high-dimensional datasets, where the excessive number of features increases the risk of overfitting in machine learning models. Traditional dimensionality reduction methods rely on statistical associations or the relative position of the feature embeddings in the hyper-space to map original features to a compact subspace that preserves the most relevant information of the data. However, these methods fail to capture the causal relationships among variables during the transformation process, leading to a loss of structural coherence of the data in low-dimensional spaces. By employing causal discovery and causal inference, it is possible to simplify these problems, effectively merging critical features while reducing both complexity and dimensionality. Our paper introduces a novel approach, Causal Feature Construction via Average Treatment Effect (CFC-ATE), which leverages causal discovery and inference to create more interpretable and reliable features for predictive modeling. Our methodology consists of the following phases: i) leveraging the causal structure of data through the inference of the causal graph. ii) transforming features through the use of the average treatment effect conditioned on the causal structure of the data. The experiments on diverse real-world datasets and synthetic datasets demonstrate the effectiveness of CFC-ATE in improving model performance by comparing it with three methods of feature selection and three benchmark dimensionality reduction techniques.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Causal Feature Construction
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Average Treatment Effect
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Dimensionality Reduction
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Causal Discovery
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Causal Inference
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/asmae_icmla_2024.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Synthesizing Multilevel Abstraction Ear Sketches for Enhanced Biometric Recognition</h3>
                      <p class="text-gray-600 italic mt-1">David Freire-Obregón, Ziga Emersic, Blaz Meden, João C. Neves, Modesto Castrillon-Santana, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">Elsevier Image and Vision Computing (2025)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Sketch understanding is a significant challenge for general-purpose vision algorithms due to the sparse nature of this kind of drawing when compared to natural visual inputs and their semantic ambiguity, as sketches can evoke multiple interpretations simultaneously. Traditionally, research in sketch-based recognition has been predominantly focused on facial data, with particular emphasis on forensics/law enforcement applications. Our work takes a step forward by shifting the focus to ear images and considering the ''sketch-2-image'' matching problem with respect to the level of sketch abstraction. We introduce a novel adaptation of the well-known triplet loss, designed to fuse multiple abstraction levels of sketches during training. Here, the level of abstraction is inversely related to the number of strokes used to illustrate the ear, whereas the number of strokes used in the sketch is inversely correspondent to its abstraction level. Upon the experiments conducted in four well-known ear datasets, we observed a consistently higher performance of our proposal compared to the state-of-the-art. Finally, such results might easily be extended to other biometric traits, which is also positively regarded and raises an interesting research topic.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Biometric Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Ear Sketches
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Sketch Understanding
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Triplet Loss
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Abstraction Levels
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1016/j.imavis.2025.105424" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/Obregon_IVC.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Causality, Machine Learning, and Feature Selection: A survey</h3>
                      <p class="text-gray-600 italic mt-1">Asmae Lamsaf, Rui Carrilho, João C. Neves, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">Sensors (2025)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Causality, which involves distinguishing between cause and effect, is essential for understanding complex relationships in data. This paper provides a review of causality in two key areas: causal discovery and causal inference. Causal discovery transforms data into graphical structures that illustrate how variables influence one another, while causal inference quantifies the impact of these variables on a target outcome. The models are more robust and accurate with the integration of causal reasoning into machine learning, improving applications like prediction and classification. We present various methods used in detecting causal relationships and how these can be applied in selecting or extracting relevant features, particularly from sensor datasets. When causality is used in feature selection, it supports applications like fault detection, anomaly detection, and predictive maintenance applications critical to the maintenance of complex systems. Traditional correlation-based methods of feature selection often overlook significant causal links, leading to incomplete insights. Our research highlights how integrating causality can be integrated and lead to stronger, deeper feature selection and ultimately enable better decision-making in machine learning tasks.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Causality
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Machine Learning
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Feature Selection
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Causal Discovery
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Causal Inference
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.3390/s25082373" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/Asmae_sensors_2025.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">A Laplacian-based Quantum Graph Neural Network for Semi-Supervised Learning</h3>
                      <p class="text-gray-600 italic mt-1">Hamed Gholipour, Farid Bozorgnia, Kailash Hambarde, Hamzeh Mohammadigheymasi, Javier Mancilla, Andre Sequeira, João C. Neves, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">Springer Quantum Information Processing (2025)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Laplacian learning method is a well-established technique in classical graph-based semi-supervised learning, but its potential in the quantum domain remains largely unexplored. This study investigates the performance of the Laplacian-based Quantum Semi-Supervised Learning (QSSL) method across four benchmark datasets—Iris, Wine, Breast Cancer Wisconsin, and Heart Disease. Further analysis explores the impact of increasing qubit counts, revealing that adding more qubits to a quantum system doesn't always improve performance. The effectiveness of additional qubits depends on the quantum algorithm and how well it matches the dataset. Additionally, we examine the effects of varying entangling layers on entanglement entropy and test accuracy. The performance of Laplacian learning is highly dependent on the number of entangling layers, with optimal configurations varying across different datasets. Typically, moderate levels of entanglement offer the best balance between model complexity and generalization capabilities. These observations highlight the crucial need for precise hyperparameter tuning tailored to each dataset to achieve optimal performance in Laplacian learning methods.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Quantum Computing
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Graph Neural Networks
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Semi-Supervised Learning
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Laplacian Methods
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Entanglement Entropy
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1007/s11128-025-04725-6" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/Hamed_qip.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">A Leopard Cannot Change Its Spots: Improving Face Recognition Using 3D-based Caricatures</h3>
                      <p class="text-gray-600 italic mt-1">João C. Neves, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">IEEE Transactions on Information Forensics and Security (2018)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Caricatures refer to a representation of a person in which the distinctive features are deliberately exaggerated, with several studies showing that humans perform better at recognizing people from caricatures than using original images. Inspired by this observation, this paper introduces the first fully automated caricature-based face recognition approach capable of working with data acquired in the wild. Our approach leverages the 3D face structure from a single 2D image and compares it to a reference model for obtaining a compact representation of face features deviations. This descriptor is subsequently deformed using a 'measure locally, weight globally' strategy to resemble the caricature drawing process. The deformed deviations are incorporated in the 3D model using the Laplacian mesh deformation algorithm, and the 2D face caricature image is obtained by projecting the deformed model in the original camera-view. To demonstrate the advantages of caricature-based face recognition, we train the VGG-Face network from scratch using either original face images (baseline) or caricatured images, and use these models for extracting face descriptors from the LFW, IJB-A and MegaFace datasets. The experiments show an increase in the recognition accuracy when using caricatures rather than original images. Moreover, our approach achieves competitive results with state-of-the-art face recognition methods, even without explicitly tuning the network for any of the evaluation sets.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Face Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Caricatures
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          3D Modeling
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/TIFS.2018.2846617" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/Leopard_TIFS.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Soft Biometrics: Globally Coherent Solutions for Hair Segmentation and Style Recognition based on Hierarchical MRFs</h3>
                      <p class="text-gray-600 italic mt-1">Hugo Proença, João C. Neves</p>
                      <p class="text-gray-700 mt-1">IEEE Transactions on Information Forensics and Security (2017)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Markov Random Fields (MRFs) are a popular tool in many computer vision problems and faithfully model a broad range of local dependencies. However, rooted in the Hammersley-Clifford theorem, they face serious difficulties in enforcing the global coherence of the solutions without using too high order cliques that reduce the computational effectiveness of the inference phase. Having this problem in mind, we describe a multi-layered (hierarchical) architecture for MRFs that is based exclusively in pairwise connections and typically produces globally coherent solutions, with 1) one layer working at the local (pixel) level, modelling the interactions between adjacent image patches; and 2) a complementary layer working at the object (hypothesis) level pushing toward globally consistent solutions. During optimization, both layers interact into an equilibrium state, that not only segments the data, but also classifies it. The proposed MRF architecture is particularly suitable for problems that deal with biological data (e.g., biometrics), where the reasonability of the solutions can be objectively measured. As test case, we considered the problem of hair / facial hair segmentation and labelling, which are soft biometric labels useful for human recognition in-the-wild. We observed performance levels close to the state-of-the-art at a much lower computational cost, both in the segmentation and classification (labelling) tasks.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Soft Biometrics
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Hair Analysis
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Markov Random Fields
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Segmentation
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/TIFS.2017.2680246" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/HairAnalysis_TIFS.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">IRINA: Iris Recognition (even) in Innacurately Segmented Data</h3>
                      <p class="text-gray-600 italic mt-1">Hugo Proença, João C. Neves</p>
                      <p class="text-gray-700 mt-1">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          The effectiveness of current iris recognition systems depends on the accurate segmentation and parameterisation of the iris boundaries, as failures at this point misalign the coefficients of the biometric signatures. This paper describes IRINA, an algorithm for Iris Recognition that is robust against INAccurately segmented samples, which makes it a good candidate to work in poor-quality data. The process is based in the concept of 'corresponding' patch between pairs of images, that is used to estimate the posterior probabilities that patches regard the same biological region, even in case of segmentation errors and non-linear texture deformations. Such information enables to infer a free-form deformation field (2D registration vectors) between images, whose first and second-order statistics provide effective biometric discriminating power. Extensive experiments were carried out in four datasets (CASIA-IrisV3-Lamp, CASIA-IrisV4-Lamp, CASIA-IrisV4-Thousand and WVU) and show that IRINA not only achieves state-of-the-art performance in good quality data, but also handles effectively severe segmentation errors and large differences in pupillary dilation / constriction.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Iris Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Biometrics
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Segmentation
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/CVPR.2017.714" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/CVPR2017.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Joint Head Pose/Soft Label Estimation for Human Recognition In-The-Wild</h3>
                      <p class="text-gray-600 italic mt-1">Hugo Proença, João C. Neves, Silvio Barra, Tiago Marques, Juan C. Moreno</p>
                      <p class="text-gray-700 mt-1">IEEE Transactions on Pattern Analysis and Machine Intelligence (2016)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Soft biometrics have been emerging to complement other traits and are particularly useful for poor quality data. In this paper, we propose an efficient algorithm to estimate human head poses and to infer soft biometric labels based on the 3D morphology of the human head. Starting by considering a set of pose hypotheses, we use a learning set of head shapes synthesized from anthropometric surveys to derive a set of 3D head centroids that constitutes a metric space. Next, representing queries by sets of 2D head landmarks, we use projective geometry techniques to rank efficiently the joint 3D head centroids / pose hypotheses according to their likelihood of matching each query. The rationale is that the most likely hypotheses are sufficiently close to the query, so a good solution can be found by convex energy minimization techniques. Once a solution has been found, the 3D head centroid and the query are assumed to have similar morphology, yielding the soft label. Our experiments point toward the usefulness of the proposed solution, which can improve the effectiveness of face recognizers and can also be used as a privacy-preserving solution for biometric recognition in public environments.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Soft Biometrics
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Head Pose Estimation
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Human Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          3D Morphology
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/TPAMI.2016.2522441" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/SoftBiometrics.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Deep-PRWIS: Periocular Recognition Without the Iris and Sclera Using Deep Learning Frameworks</h3>
                      <p class="text-gray-600 italic mt-1">Hugo Proença, João C. Neves</p>
                      <p class="text-gray-700 mt-1">IEEE Transactions on Information Forensics and Security (2018)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          This work is based on a disruptive hypothesis for periocular biometrics: in visible-light data, the recognition performance is optimized when the components inside the ocular globe (the iris and the sclera) are simply discarded, and the recogniser's response is exclusively based in information from the surroundings of the eye. As major novelty, we describe a processing chain based on convolution neural networks (CNNs) that defines the regions-of-interest in the input data that should be privileged in an implicit way, i.e., without masking out any areas in the learning/test samples. By using an ocular segmentation algorithm exclusively in the learning data, we separate the ocular from the periocular parts. Then, we produce a large set of 'multi-class' artificial samples, by interchanging the periocular and ocular parts from different subjects. These samples are used for data augmentation purposes and feed the learning phase of the CNN, always considering as label the ID of the periocular part. This way, for every periocular region, the CNN receives multiple samples of different ocular classes, forcing it to conclude that such regions should not be considered in its response. During the test phase, samples are provided without any segmentation mask and the network naturally disregards the ocular components, which contributes for improvements in performance. Our experiments were carried out in full versions of two widely known data sets (UBIRIS.v2 and FRGC) and show that the proposed method consistently advances the state-of-the-art performance in the closed-world setting, reducing the EERs in about 82% (UBIRIS.v2) and 85% (FRGC) and improving the Rank-1 over 41% (UBIRIS.v2) and 12% (FRGC).
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Periocular Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Deep Learning
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Biometrics
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Ocular Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Data Augmentation
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/TIFS.2017.2771230" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/Deep-PRWIS.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                    </div>
                  </div>
                </li>
                
                <li class="py-6 first:pt-0 last:pb-0">
                  <div class="flex flex-col md:flex-row">
                    <div class="flex-grow">
                      <h3 class="text-lg font-semibold text-gray-800 hover:text-blue-600 transition">Generative Adversarial Graph Convolutional Networks for Human Action Synthesis</h3>
                      <p class="text-gray-600 italic mt-1">Bruno Degardin, João C. Neves, Vasco Lopes, João Brito, Ehsan Yaghoubi, Hugo Proença</p>
                      <p class="text-gray-700 mt-1">Proceedings of the 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) (2022)</p>
                      
                      
                      <details class="mt-3 group">
                        <summary class="cursor-pointer flex items-center text-sm font-medium text-blue-600 hover:text-blue-800">
                          <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-2 transition-transform group-open:rotate-90" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7" />
                          </svg>
                          Show Abstract
                        </summary>
                        <div class="mt-3 pl-6 text-gray-700 border-l-2 border-blue-200">
                          Synthesising the spatial and temporal dynamics of the human body skeleton remains a challenging task, not only in terms of the quality of the generated shapes, but also of their diversity, particularly to synthesise realistic body movements of a specific action (action conditioning). In this paper, we propose Kinetic-GAN, a novel architecture that leverages the benefits of Generative Adversarial Networks and Graph Convolutional Networks to synthesise the kinetics of the human body. The proposed adversarial architecture can condition up to 120 different actions over local and global body movements while improving sample quality and diversity through latent space disentanglement and stochastic variations. Our experiments were carried out in three well-known datasets, where Kinetic-GAN notably surpasses the state-of-the-art methods in terms of distribution quality metrics while having the ability to synthesise more than one order of magnitude regarding the number of different actions. Our code and models are publicly available at https://github.com/DegardinBruno/Kinetic-GAN.
                        </div>
                      </details>
                      

                      
                      <div class="mt-3">
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Human Action Synthesis
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Graph Convolutional Networks
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Generative Adversarial Networks
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Action Recognition
                        </span>
                        
                        <span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-blue-100 text-blue-800 mr-2 mb-2">
                          Skeleton-based Animation
                        </span>
                        
                      </div>
                      
                    </div>
                    
                    <!-- Publication links -->
                    <div class="mt-4 md:mt-0 md:ml-4 flex md:flex-col gap-3 md:self-end">
                      
                      <a href="https://doi.org/10.1109/WACV51458.2022.00281" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-gray-100 text-gray-800 hover:bg-gray-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13.828 10.172a4 4 0 00-5.656 0l-4 4a4 4 0 105.656 5.656l1.102-1.101m-.758-4.899a4 4 0 005.656 0l4-4a4 4 0 00-5.656-5.656l-1.1 1.1" />
                        </svg>
                        DOI
                      </a>
                      
                      
                      <a href="http://www.di.ubi.pt/~hugomcp/doc/wacv21.pdf" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-blue-100 text-blue-800 hover:bg-blue-200 transition-colors">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                          <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        PDF
                      </a>
                      
                      
                      <a href="https://github.com/DegardinBruno/Kinetic-GAN" target="_blank" rel="noopener" class="inline-flex items-center px-3 py-1 rounded-lg text-sm font-medium bg-green-100 text-green-800 hover:bg-green-200 transition-colors">
                        <svg class="h-4 w-4 mr-1" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true">
                          <path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z" clip-rule="evenodd" />
                        </svg>
                        Code
                      </a>
                      
                    </div>
                  </div>
                </li>
                
              </div>
              
              <div class="text-center mt-6">
                <button id="view-more-btn" class="inline-flex items-center px-4 py-2 border border-transparent shadow-sm text-sm font-medium rounded-md text-white bg-blue-600 hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-offset-2 focus:ring-blue-500 transition-colors">
                  <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-2" viewBox="0 0 20 20" fill="currentColor">
                    <path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd" />
                  </svg>
                  View More Publications (47)
                </button>
              </div>
              
              <script>
                document.addEventListener('DOMContentLoaded', function() {
                  const viewMoreBtn = document.getElementById('view-more-btn');
                  const hiddenPublications = document.getElementById('hidden-publications');
                  
                  if (viewMoreBtn && hiddenPublications) {
                    viewMoreBtn.addEventListener('click', function() {
                      // Show hidden publications
                      hiddenPublications.classList.toggle('hidden');
                      
                      // Change button text based on current state
                      if (hiddenPublications.classList.contains('hidden')) {
                        viewMoreBtn.innerHTML = '<svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-2" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414 0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414 0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd" /></svg> View More Publications (47)';
                      } else {
                        viewMoreBtn.innerHTML = '<svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-2" viewBox="0 0 20 20" fill="currentColor"><path fill-rule="evenodd" d="M14.707 12.707a1 1 0 01-1.414 0L10 9.414l-3.293 3.293a1 1 0 01-1.414-1.414l4-4a1 1 0 011.414 0l4 4a1 1 0 010 1.414z" clip-rule="evenodd" /></svg> Show Less';
                      }
                    });
                  }
                });
              </script>
              
            </ul>
          </div>
        </div>
        

        <!-- Projects -->
        
        
      </div>
    </div>
  </div>
</div>

<style>
/* Grayscale effect for alumni */
.grayscale {
  filter: grayscale(90%);
  transition: filter 0.3s ease;
}

.grayscale:hover {
  filter: grayscale(40%);
}

/* Background pattern */
.bg-pattern {
  background-image: url("data:image/svg+xml,%3Csvg width='100' height='100' viewBox='0 0 100 100' xmlns='http://www.w3.org/2000/svg'%3E%3Cpath d='M11 18c3.866 0 7-3.134 7-7s-3.134-7-7-7-7 3.134-7 7 3.134 7 7 7zm48 25c3.866 0 7-3.134 7-7s-3.134-7-7-7-7 3.134-7 7 3.134 7 7 7zm-43-7c1.657 0 3-1.343 3-3s-1.343-3-3-3-3 1.343-3 3 1.343 3 3 3zm63 31c1.657 0 3-1.343 3-3s-1.343-3-3-3-3 1.343-3 3 1.343 3 3 3zM34 90c1.657 0 3-1.343 3-3s-1.343-3-3-3-3 1.343-3 3 1.343 3 3 3zm56-76c1.657 0 3-1.343 3-3s-1.343-3-3-3-3 1.343-3 3 1.343 3 3 3zM12 86c2.21 0 4-1.79 4-4s-1.79-4-4-4-4 1.79-4 4 1.79 4 4 4zm28-65c2.21 0 4-1.79 4-4s-1.79-4-4-4-4 1.79-4 4 1.79 4 4 4zm23-11c2.76 0 5-2.24 5-5s-2.24-5-5-5-5 2.24-5 5 2.24 5 5 5zm-6 60c2.21 0 4-1.79 4-4s-1.79-4-4-4-4 1.79-4 4 1.79 4 4 4zm29 22c2.76 0 5-2.24 5-5s-2.24-5-5-5-5 2.24-5 5 2.24 5 5 5zM32 63c2.76 0 5-2.24 5-5s-2.24-5-5-5-5 2.24-5 5 2.24 5 5 5zm57-13c2.76 0 5-2.24 5-5s-2.24-5-5-5-5 2.24-5 5 2.24 5 5 5zm-9-21c1.105 0 2-.895 2-2s-.895-2-2-2-2 .895-2 2 .895 2 2 2zM60 91c1.105 0 2-.895 2-2s-.895-2-2-2-2 .895-2 2 .895 2 2 2zM35 41c1.105 0 2-.895 2-2s-.895-2-2-2-2 .895-2 2 .895 2 2 2zM12 60c1.105 0 2-.895 2-2s-.895-2-2-2-2 .895-2 2 .895 2 2 2z' fill='%23ffffff' fill-opacity='0.1' fill-rule='evenodd'/%3E%3C/svg%3E");
}

/* Make details summary marker work consistently across browsers */
details > summary {
  list-style: none;
}
</style>

<script>
  // This script ensures the page loads correctly, even if the user navigated here using JavaScript
  if (document.referrer && document.referrer.includes('/team')) {
    console.log('Navigation from team page detected');
  }
  
  // Detect if this page was loaded in an iframe
  if (window !== window.top) {
    console.log('Page loaded in iframe');
    // Optionally break out of the iframe if needed
    // window.top.location.href = window.location.href;
  }
  
  // Add debugging info to console
  console.log('Profile page loaded successfully:', window.location.pathname);
</script>

    </main>

    <!-- Footer -->
    <footer class="bg-gradient-to-br from-gray-900 to-gray-800 text-white py-12 mt-16">
  <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
    <div class="grid grid-cols-1 md:grid-cols-3 gap-10">
      <!-- About Column -->
      <div>
        <h3 class="text-xl font-semibold mb-4 relative inline-block">
          Intelligent Systems
          <span class="absolute bottom-0 left-0 w-full h-0.5 bg-blue-500 transform scale-x-0 transition-transform duration-300 group-hover:scale-x-100"></span>
        </h3>
        <p class="text-gray-300 mb-4">The Intelligent Systems Laboratory at the University of Beira Interior focuses on computer vision, pattern recognition, and biometrics research.</p>
        <div class="flex space-x-4 mt-6">
          <a href="https://github.com/socia-lab" aria-label="GitHub" class="text-gray-400 hover:text-white transition-colors transform hover:scale-110 duration-300">
            <svg class="h-6 w-6" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true">
              <path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z" clip-rule="evenodd" />
            </svg>
          </a>
          <a href="https://twitter.com/socia_lab" aria-label="Twitter" class="text-gray-400 hover:text-white transition-colors transform hover:scale-110 duration-300">
            <svg class="h-6 w-6" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true">
              <path d="M8.29 20.251c7.547 0 11.675-6.253 11.675-11.675 0-.178 0-.355-.012-.53A8.348 8.348 0 0022 5.92a8.19 8.19 0 01-2.357.646 4.118 4.118 0 001.804-2.27 8.224 8.224 0 01-2.605.996 4.107 4.107 0 00-6.993 3.743 11.65 11.65 0 01-8.457-4.287 4.106 4.106 0 001.27 5.477A4.072 4.072 0 012.8 9.713v.052a4.105 4.105 0 003.292 4.022 4.095 4.095 0 01-1.853.07 4.108 4.108 0 003.834 2.85A8.233 8.233 0 012 18.407a11.616 11.616 0 006.29 1.84" />
            </svg>
          </a>
          <a href="mailto:is-ubi@di.ubi.pt" aria-label="Email" class="text-gray-400 hover:text-white transition-colors transform hover:scale-110 duration-300">
            <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 8l7.89 5.26a2 2 0 002.22 0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z" />
            </svg>
          </a>
          <a href="https://www.linkedin.com/company/socia-lab" aria-label="LinkedIn" class="text-gray-400 hover:text-white transition-colors transform hover:scale-110 duration-300">
            <svg class="h-6 w-6" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true">
              <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h-.003z"/>
            </svg>
          </a>
          <a href="https://www.youtube.com/channel/UCsocialab" aria-label="YouTube" class="text-gray-400 hover:text-white transition-colors transform hover:scale-110 duration-300">
            <svg class="h-6 w-6" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true">
              <path d="M23.498 6.186a3.016 3.016 0 0 0-2.122-2.136C19.505 3.545 12 3.545 12 3.545s-7.505 0-9.377.505A3.017 3.017 0 0 0 .502 6.186C0 8.07 0 12 0 12s0 3.93.502 5.814a3.016 3.016 0 0 0 2.122 2.136c1.871.505 9.376.505 9.376.505s7.505 0 9.377-.505a3.015 3.015 0 0 0 2.122-2.136C24 15.93 24 12 24 12s0-3.93-.502-5.814zM9.545 15.568V8.432L15.818 12l-6.273 3.568z"/>
            </svg>
          </a>
        </div>
      </div>
      
      <!-- Quick Links -->
      <div>
        <h3 class="text-xl font-semibold mb-4 relative inline-block">Quick Links</h3>
        <ul class="space-y-3">
          
          
          
          <li>
            <a href="/news" class="text-gray-300 hover:text-white transition-colors flex items-center group">
              <span class="w-1.5 h-1.5 bg-blue-500 rounded-full mr-2 transform scale-0 group-hover:scale-100 transition-transform duration-300"></span>
              News
            </a>
          </li>
          
          
          <li>
            <a href="/team" class="text-gray-300 hover:text-white transition-colors flex items-center group">
              <span class="w-1.5 h-1.5 bg-blue-500 rounded-full mr-2 transform scale-0 group-hover:scale-100 transition-transform duration-300"></span>
              Team
            </a>
          </li>
          
          
          <li>
            <a href="/publications" class="text-gray-300 hover:text-white transition-colors flex items-center group">
              <span class="w-1.5 h-1.5 bg-blue-500 rounded-full mr-2 transform scale-0 group-hover:scale-100 transition-transform duration-300"></span>
              Publications
            </a>
          </li>
          
          
          <li>
            <a href="/projects" class="text-gray-300 hover:text-white transition-colors flex items-center group">
              <span class="w-1.5 h-1.5 bg-blue-500 rounded-full mr-2 transform scale-0 group-hover:scale-100 transition-transform duration-300"></span>
              Projects
            </a>
          </li>
          
          
          <li>
            <a href="/funding" class="text-gray-300 hover:text-white transition-colors flex items-center group">
              <span class="w-1.5 h-1.5 bg-blue-500 rounded-full mr-2 transform scale-0 group-hover:scale-100 transition-transform duration-300"></span>
              Funding
            </a>
          </li>
          
          
          <li>
            <a href="/alumni" class="text-gray-300 hover:text-white transition-colors flex items-center group">
              <span class="w-1.5 h-1.5 bg-blue-500 rounded-full mr-2 transform scale-0 group-hover:scale-100 transition-transform duration-300"></span>
              Alumni
            </a>
          </li>
          
        </ul>
      </div>
      
      <!-- Contact -->
      <div>
        <h3 class="text-xl font-semibold mb-4 relative inline-block">Contact</h3>
        <address class="not-italic text-gray-300 space-y-2">
          <p class="flex items-center">
            <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-2 text-blue-400" fill="none" viewBox="0 0 24 24" stroke="currentColor">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 21V5a2 2 0 00-2-2H7a2 2 0 00-2 2v16m14 0h2m-2 0h-5m-9 0H3m2 0h5M9 7h1m-1 4h1m4-4h1m-1 4h1m-5 10v-5a1 1 0 011-1h2a1 1 0 011 1v5m-4 0h4" />
            </svg>
            University of Beira Interior
          </p>
          <p class="flex items-center">
            <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-2 text-blue-400" fill="none" viewBox="0 0 24 24" stroke="currentColor">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z" />
            </svg>
            Department of Computer Science
          </p>
          <p class="flex items-center">
            <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-2 text-blue-400" fill="none" viewBox="0 0 24 24" stroke="currentColor">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M17.657 16.657L13.414 20.9a1.998 1.998 0 01-2.827 0l-4.244-4.243a8 8 0 1111.314 0z" />
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 11a3 3 0 11-6 0 3 3 0 016 0z" />
            </svg>
            Rua Marquês d'Ávila e Bolama
          </p>
          <p class="flex items-center ml-7">6201-001 Covilhã, Portugal</p>
          <p class="mt-4 flex items-center">
            <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-2 text-blue-400" fill="none" viewBox="0 0 24 24" stroke="currentColor">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 8l7.89 5.26a2 2 0 002.22 0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z" />
            </svg>
            <a href="mailto:is-ubi@di.ubi.pt" class="text-blue-400 hover:text-blue-300 transition-colors">is-ubi@di.ubi.pt</a>
          </p>
        </address>
      </div>
    </div>
    
    <div class="border-t border-gray-800 mt-10 pt-6 text-center text-gray-400 text-sm">
      <p>© 2025 Intelligent Systems. All rights reserved.</p>
    </div>
  </div>
</footer>

</body>
</html>
