<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Publications - Intelligent Systems</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    colors: {
                        primary: {
                            DEFAULT: '#2563eb',
                            dark: '#1d4ed8',
                            light: '#3b82f6'
                        },
                        secondary: {
                            DEFAULT: '#7c3aed',
                            dark: '#6d28d9',
                            light: '#8b5cf6'
                        },
                        accent: '#0ea5e9',
                        surface: '#f9fafb',
                        ink: {
                            DEFAULT: '#1e293b',
                            light: '#64748b'
                        }
                    },
                    boxShadow: {
                        'intense': '0 10px 25px rgba(0, 0, 0, 0.1)',
                        'hover': '0 15px 30px rgba(0, 0, 0, 0.12)'
                    }
                }
            }
        }
    </script>
    <link rel="stylesheet" href="/is-ubi/assets/css/modern.css">
    <link rel="stylesheet" href="/is-ubi/assets/css/gradient-headings.css">
    <style>
        /* Base styles for the Intelligent Systems lab */
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
        }
        
        /* Gradient Text for Headings */
        .gradient-text {
            background: linear-gradient(to right, #facc15, #f97316, #ef4444); /* warm */
            -webkit-background-clip: text;
            background-clip: text;
            color: transparent;
            font-weight: 700;
            }

        
            h1 {
  background: linear-gradient(to right, #ffffff, #facc15, #fda4af);
  background-clip: text;
  -webkit-background-clip: text;
  color: transparent;
  font-weight: 700;
  display: inline-block; /* ensures background-clip works correctly */
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.15); /* subtle depth */

}

        
        /* Modern card styling with subtle hover effects */
        .card {
            transition: all 0.3s ease;
        }
        
        .card:hover {
            transform: translateY(-5px);
            box-shadow: 0 15px 30px rgba(0, 0, 0, 0.12);
        }
        
        /* Modern gradient decorations */
        .gradient-line {
            height: 3px;
            width: 50px;
            background: linear-gradient(90deg, #2563eb 0%, #7c3aed 100%);
            border-radius: 3px;
            margin: 0.5rem 0 1.5rem 0;
        }
        
        /* Logo animation */
        .site-logo {
            transition: transform 0.3s ease;
        }
        
        .site-logo:hover {
            transform: scale(1.05);
        }
        
        /* Cool gradient borders */
        .gradient-border {
            position: relative;
        }
        
        .gradient-border::after {
            content: '';
            position: absolute;
            left: 0;
            bottom: -3px;
            width: 100%;
            height: 3px;
            background: linear-gradient(90deg, #2563eb, #7c3aed, #0ea5e9);
            border-radius: 3px;
            transform: scaleX(0);
            transform-origin: left;
            transition: transform 0.3s ease;
        }
        
        .gradient-border:hover::after {
            transform: scaleX(1);
        }
        
        /* Fix for member profile spacing issues */
        .relative.z-10 svg {
            height: 40px !important;
            max-height: 40px !important;
        }
        .container.mx-auto.px-4.py-4 {
            margin-top: 0rem !important;
        }
        
        /* Modern soft styling for the header strip */
        .relative.overflow-hidden {
            box-shadow: 0 4px 20px rgba(0, 0, 0, 0.05);
        }
        
        .relative.overflow-hidden::before {
            content: '';
            position: absolute;
            inset: 0;
            background: rgba(255, 255, 255, 0.05);
            z-index: 1;
        }
        
        /* Soften text shadows for better readability */
        .drop-shadow-sm {
            text-shadow: 0 1px 2px rgba(0, 0, 0, 0.1);
        }
    </style>
    
</head>
<body class="min-h-screen bg-surface flex flex-col">
    <!-- Header -->
    <header class="bg-white shadow-md fixed top-0 left-0 right-0 z-50 transition-all duration-300">
  <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
    <div class="flex justify-between items-center py-4">
      <a class="flex items-center gap-3 group transition-all duration-300" rel="author" href="/is-ubi/">
        
          <img src="/is-ubi/assets/images/lab_logo.png" alt="Intelligent Systems logo" class="h-12 w-auto transform transition-transform duration-300 group-hover:scale-110">
        
        <span class="text-xl gradient-text transition-all duration-300">Intelligent Systems</span>
      </a>

      <!-- Desktop Nav -->
      <nav class="hidden md:flex items-center space-x-1">
        
        
        
        
        
        <a href="/is-ubi/news/" 
           class="px-3 py-2 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-all duration-200 font-medium gradient-border ">
          News
        </a>
        
        
        <a href="/is-ubi/team/" 
           class="px-3 py-2 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-all duration-200 font-medium gradient-border ">
          Team
        </a>
        
        
        <a href="/is-ubi/publications/" 
           class="px-3 py-2 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-all duration-200 font-medium gradient-border ">
          Publications
        </a>
        
        
        <a href="/is-ubi/projects/" 
           class="px-3 py-2 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-all duration-200 font-medium gradient-border ">
          Projects
        </a>
        
        
        <a href="/is-ubi/datasets/" 
           class="px-3 py-2 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-all duration-200 font-medium gradient-border ">
          Datasets
        </a>
        
        
        <a href="/is-ubi/funding/" 
           class="px-3 py-2 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-all duration-200 font-medium gradient-border ">
          Funding
        </a>
        
        
        <a href="/is-ubi/alumni/" 
           class="px-3 py-2 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-all duration-200 font-medium gradient-border ">
          Alumni
        </a>
        
        
        <!-- Contact Button with special styling -->
        <a href="/contact/" class="ml-2 px-4 py-2 gradient-button rounded-md transition-all duration-200 shadow-sm hover:shadow-md font-medium">
          Contact
        </a>
      </nav>

      <!-- Mobile Nav Button -->
      <div class="md:hidden">
        <button type="button" class="mobile-menu-button text-ink hover:text-primary transition-colors" aria-label="Toggle menu">
          <svg class="w-7 h-7" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16M4 18h16"></path>
          </svg>
        </button>
      </div>
    </div>

    <!-- Mobile Nav Menu -->
    <div class="mobile-menu hidden md:hidden pb-4 space-y-1 bg-white">
      
      
      <a href="/is-ubi/news/" 
         class="block py-2 px-3 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-colors ">
        News
      </a>
      
      
      <a href="/is-ubi/team/" 
         class="block py-2 px-3 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-colors ">
        Team
      </a>
      
      
      <a href="/is-ubi/publications/" 
         class="block py-2 px-3 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-colors ">
        Publications
      </a>
      
      
      <a href="/is-ubi/projects/" 
         class="block py-2 px-3 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-colors ">
        Projects
      </a>
      
      
      <a href="/is-ubi/datasets/" 
         class="block py-2 px-3 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-colors ">
        Datasets
      </a>
      
      
      <a href="/is-ubi/funding/" 
         class="block py-2 px-3 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-colors ">
        Funding
      </a>
      
      
      <a href="/is-ubi/alumni/" 
         class="block py-2 px-3 rounded-md text-ink hover:text-primary hover:bg-primary/5 transition-colors ">
        Alumni
      </a>
      
      <!-- Mobile Contact Button -->
      <a href="/contact/" class="block py-2 px-3 mt-2 text-primary font-medium">
        Contact
      </a>
    </div>
  </div>

  <!-- Mobile menu JavaScript -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      const btn = document.querySelector('.mobile-menu-button');
      const menu = document.querySelector('.mobile-menu');
      const header = document.querySelector('header');
      
      btn.addEventListener('click', function() {
        menu.classList.toggle('hidden');
        // Add a slight animation
        if (!menu.classList.contains('hidden')) {
          // Menu is now visible - animate items
          const menuItems = menu.querySelectorAll('a');
          menuItems.forEach((item, index) => {
            item.style.opacity = '0';
            item.style.transform = 'translateY(-10px)';
            setTimeout(() => {
              item.style.transition = 'opacity 0.3s ease, transform 0.3s ease';
              item.style.opacity = '1';
              item.style.transform = 'translateY(0)';
            }, index * 50);
          });
        }
      });
      
      // Close mobile menu when clicking outside
      document.addEventListener('click', function(event) {
        if (!menu.classList.contains('hidden') && 
            !menu.contains(event.target) && 
            !btn.contains(event.target)) {
          menu.classList.add('hidden');
        }
      });
      
      // Header scroll effect with enhanced shadow
      window.addEventListener('scroll', function() {
        if (window.scrollY > 10) {
          header.classList.add('shadow-lg');
          header.classList.add('bg-white/95');
          header.classList.add('backdrop-blur-sm');
        } else {
          header.classList.remove('shadow-lg');
          header.classList.remove('bg-white/95');
          header.classList.remove('backdrop-blur-sm');
        }
      });
    });
  </script>
</header>

<style>
  /* Header animations */
  header {
    transition: box-shadow 0.3s ease;
  }
  
  header.scrolled {
    box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
  }
</style>


    <!-- Main Content -->
    <main class="flex-grow pt-16">
        <!-- Page Header with Gradient Background -->
<div class="relative bg-gradient-to-r from-blue-600 to-indigo-800 text-white py-16 mb-12">
  <div class="absolute inset-0 bg-cover bg-center opacity-20" style="background-image: url('/is-ubi/assets/slider/slide_1.png');"></div>
  <div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 text-center relative z-10">
    <h1 class="text-4xl md:text-5xl font-bold mb-4">Publications</h1>
    
    <p class="text-xl md:text-2xl max-w-3xl mx-auto font-light">
      Our research contributions to the field of computer vision and machine learning.
    </p>
    
  </div>
</div>

<!-- Main Content -->
<div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 pb-16">
  <!-- Filter and Sort Controls -->
<div class="bg-white rounded-xl shadow-md p-6 mb-8">
    <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-4">
        <!-- Year Filter -->
        <div>
            <label for="year-filter" class="block text-sm font-medium text-gray-700 mb-2">Year</label>
            <select id="year-filter" class="w-full rounded-lg border-gray-300 shadow-sm focus:border-blue-600 focus:ring focus:ring-blue-600 focus:ring-opacity-50">
                <option value="all">All Years</option>
                
                
                <option value="2025">2025</option>
                
                <option value="2024">2024</option>
                
                <option value="2023">2023</option>
                
                <option value="2022">2022</option>
                
                <option value="2021">2021</option>
                
                <option value="2020">2020</option>
                
                <option value="2019">2019</option>
                
                <option value="2018">2018</option>
                
                <option value="2017">2017</option>
                
                <option value="2016">2016</option>
                
                <option value="2015">2015</option>
                
                <option value="2014">2014</option>
                
                <option value="2013">2013</option>
                
                <option value="2012">2012</option>
                
                <option value="2011">2011</option>
                
                <option value="2010">2010</option>
                
                <option value="2009">2009</option>
                
                <option value="2008">2008</option>
                
                <option value="2007">2007</option>
                
                <option value="2006">2006</option>
                
                <option value="2005">2005</option>
                
                <option value="2004">2004</option>
                
            </select>
        </div>

        <!-- Type Filter -->
        <div>
            <label for="type-filter" class="block text-sm font-medium text-gray-700 mb-2">Publication Type</label>
            <select id="type-filter" class="w-full rounded-lg border-gray-300 shadow-sm focus:border-blue-600 focus:ring focus:ring-blue-600 focus:ring-opacity-50">
                <option value="all">All Types</option>
                
                
                <option value="book">Book</option>
                
                <option value="book chapter">Book chapter</option>
                
                <option value="book_chapter">Book chapter</option>
                
                <option value="conference">Conference</option>
                
                <option value="journal">Journal</option>
                
                <option value="special issue">Special issue</option>
                
                <option value="special_issue">Special issue</option>
                
            </select>
        </div>

        <!-- Tag Filter -->
        <div>
            <label for="tag-filter" class="block text-sm font-medium text-gray-700 mb-2">Tags</label>
            <select id="tag-filter" class="w-full rounded-lg border-gray-300 shadow-sm focus:border-blue-600 focus:ring focus:ring-blue-600 focus:ring-opacity-50">
                <option value="all">All Tags</option>
                
                
                
                
                <option value="2D-3D Registration">2D-3D Registration</option>
                
                <option value="3D Caricatures">3D Caricatures</option>
                
                <option value="3D Modeling">3D Modeling</option>
                
                <option value="3D Morphology">3D Morphology</option>
                
                <option value="Abnormal Events Detection">Abnormal Events Detection</option>
                
                <option value="Abstraction Levels">Abstraction Levels</option>
                
                <option value="Action Recognition">Action Recognition</option>
                
                <option value="Active Contours">Active Contours</option>
                
                <option value="Active Speaker Detection">Active Speaker Detection</option>
                
                <option value="Adversarial Attacks">Adversarial Attacks</option>
                
                <option value="Adversarial Defenses">Adversarial Defenses</option>
                
                <option value="Adversarial Generative Techniques">Adversarial Generative Techniques</option>
                
                <option value="Aliasing">Aliasing</option>
                
                <option value="Anomaly Detection">Anomaly Detection</option>
                
                <option value="Anomaly Localization">Anomaly Localization</option>
                
                <option value="Applications">Applications</option>
                
                <option value="Artificial Intelligence">Artificial Intelligence</option>
                
                <option value="Attention Heatmaps">Attention Heatmaps</option>
                
                <option value="Attention Mechanism">Attention Mechanism</option>
                
                <option value="Attribute Normalization">Attribute Normalization</option>
                
                <option value="Audio Anomaly">Audio Anomaly</option>
                
                <option value="Audio-Visual Processing">Audio-Visual Processing</option>
                
                <option value="Automatic Annotation">Automatic Annotation</option>
                
                <option value="Average Treatment Effect">Average Treatment Effect</option>
                
                <option value="Background Subtraction">Background Subtraction</option>
                
                <option value="Bias Analysis">Bias Analysis</option>
                
                <option value="Biologically Plausible Distortions">Biologically Plausible Distortions</option>
                
                <option value="Biometric Experiments">Biometric Experiments</option>
                
                <option value="Biometric Identification">Biometric Identification</option>
                
                <option value="Biometric Indexing">Biometric Indexing</option>
                
                <option value="Biometric Menagerie">Biometric Menagerie</option>
                
                <option value="Biometric Performance">Biometric Performance</option>
                
                <option value="Biometric Recognition">Biometric Recognition</option>
                
                <option value="Biometric Signatures">Biometric Signatures</option>
                
                <option value="Biometric Verification">Biometric Verification</option>
                
                <option value="Biometrics">Biometrics</option>
                
                <option value="Bit Discriminability">Bit Discriminability</option>
                
                <option value="Bit Fragility">Bit Fragility</option>
                
                <option value="Body Features">Body Features</option>
                
                <option value="Brain MRI Segmentation">Brain MRI Segmentation</option>
                
                <option value="Brain Tissue Segmentation">Brain Tissue Segmentation</option>
                
                <option value="CNN">CNN</option>
                
                <option value="COVID-19">COVID-19</option>
                
                <option value="Calibration">Calibration</option>
                
                <option value="Camera Calibration">Camera Calibration</option>
                
                <option value="Camera Scheduling">Camera Scheduling</option>
                
                <option value="Caricatures">Caricatures</option>
                
                <option value="Caries Detection">Caries Detection</option>
                
                <option value="Causal Discovery">Causal Discovery</option>
                
                <option value="Causal Feature Construction">Causal Feature Construction</option>
                
                <option value="Causal Inference">Causal Inference</option>
                
                <option value="Causality">Causality</option>
                
                <option value="Cell Detection">Cell Detection</option>
                
                <option value="Challenge Evaluation">Challenge Evaluation</option>
                
                <option value="Challenges">Challenges</option>
                
                <option value="Classification">Classification</option>
                
                <option value="Clothing Analysis">Clothing Analysis</option>
                
                <option value="Clothing-invariant Features">Clothing-invariant Features</option>
                
                <option value="Cloud Security">Cloud Security</option>
                
                <option value="Color">Color</option>
                
                <option value="Color Features">Color Features</option>
                
                <option value="Computer Vision">Computer Vision</option>
                
                <option value="Conference Papers">Conference Papers</option>
                
                <option value="Contour Concavity">Contour Concavity</option>
                
                <option value="Contrastive Learning">Contrastive Learning</option>
                
                <option value="Convolutional Neural Networks">Convolutional Neural Networks</option>
                
                <option value="Covariates">Covariates</option>
                
                <option value="Cross-spectral Biometrics">Cross-spectral Biometrics</option>
                
                <option value="Cross-view Scenarios">Cross-view Scenarios</option>
                
                <option value="DCNNs">DCNNs</option>
                
                <option value="Data Augmentation">Data Augmentation</option>
                
                <option value="Data Feed">Data Feed</option>
                
                <option value="Data Fusion">Data Fusion</option>
                
                <option value="Data Misalignments">Data Misalignments</option>
                
                <option value="Database">Database</option>
                
                <option value="Databases">Databases</option>
                
                <option value="Dataset">Dataset</option>
                
                <option value="Decision Support Systems">Decision Support Systems</option>
                
                <option value="Deep Learning">Deep Learning</option>
                
                <option value="Deep Neural Networks">Deep Neural Networks</option>
                
                <option value="Degraded Data">Degraded Data</option>
                
                <option value="Degraded Images">Degraded Images</option>
                
                <option value="Dental Radiographs">Dental Radiographs</option>
                
                <option value="Dental X-Ray">Dental X-Ray</option>
                
                <option value="Diagnosis">Diagnosis</option>
                
                <option value="Diffusion Models">Diffusion Models</option>
                
                <option value="Digital Forensics">Digital Forensics</option>
                
                <option value="Dimensionality Reduction">Dimensionality Reduction</option>
                
                <option value="Discriminability">Discriminability</option>
                
                <option value="Ear Biometrics">Ear Biometrics</option>
                
                <option value="Ear Sketches">Ear Sketches</option>
                
                <option value="Edge Computing">Edge Computing</option>
                
                <option value="Editorial">Editorial</option>
                
                <option value="Elastic Graph Matching">Elastic Graph Matching</option>
                
                <option value="Email Classification">Email Classification</option>
                
                <option value="Emotion Detection">Emotion Detection</option>
                
                <option value="Energy Efficiency">Energy Efficiency</option>
                
                <option value="Entanglement Entropy">Entanglement Entropy</option>
                
                <option value="Entropy-Based Coding">Entropy-Based Coding</option>
                
                <option value="Error Analysis">Error Analysis</option>
                
                <option value="Error Detection">Error Detection</option>
                
                <option value="Evidence Fusion">Evidence Fusion</option>
                
                <option value="Experimental Protocols">Experimental Protocols</option>
                
                <option value="Explainable AI">Explainable AI</option>
                
                <option value="Eye Corner Detection">Eye Corner Detection</option>
                
                <option value="Eye Region Acquisition">Eye Region Acquisition</option>
                
                <option value="Fabric Defect Detection">Fabric Defect Detection</option>
                
                <option value="Face De-Identification">Face De-Identification</option>
                
                <option value="Face Detection">Face Detection</option>
                
                <option value="Face Image Synthesis">Face Image Synthesis</option>
                
                <option value="Face Manipulation Detection">Face Manipulation Detection</option>
                
                <option value="Face Recognition">Face Recognition</option>
                
                <option value="Face and Body Information">Face and Body Information</option>
                
                <option value="Facial Attributes">Facial Attributes</option>
                
                <option value="Facial Expressions">Facial Expressions</option>
                
                <option value="Facial Recognition">Facial Recognition</option>
                
                <option value="Facial Regions">Facial Regions</option>
                
                <option value="Facial Sketches">Facial Sketches</option>
                
                <option value="Fake Detection">Fake Detection</option>
                
                <option value="Feature Analysis">Feature Analysis</option>
                
                <option value="Feature Correlation">Feature Correlation</option>
                
                <option value="Feature Embedding">Feature Embedding</option>
                
                <option value="Feature Embeddings">Feature Embeddings</option>
                
                <option value="Feature Engineering">Feature Engineering</option>
                
                <option value="Feature Representation">Feature Representation</option>
                
                <option value="Feature Selection">Feature Selection</option>
                
                <option value="Fish-eye Correction">Fish-eye Correction</option>
                
                <option value="Fluorescence Microscopy">Fluorescence Microscopy</option>
                
                <option value="Fruit Detection">Fruit Detection</option>
                
                <option value="Fruit Disease Classification">Fruit Disease Classification</option>
                
                <option value="Fusion">Fusion</option>
                
                <option value="Fuzzy Clustering">Fuzzy Clustering</option>
                
                <option value="GAN Fingerprints">GAN Fingerprints</option>
                
                <option value="Gabor Filters">Gabor Filters</option>
                
                <option value="Gait Recognition">Gait Recognition</option>
                
                <option value="Gender Bias">Gender Bias</option>
                
                <option value="Gender Inference">Gender Inference</option>
                
                <option value="Gender Recognition">Gender Recognition</option>
                
                <option value="Generative Adversarial Networks">Generative Adversarial Networks</option>
                
                <option value="Generative Deep Learning">Generative Deep Learning</option>
                
                <option value="Generative Models">Generative Models</option>
                
                <option value="Geometry">Geometry</option>
                
                <option value="Graph Convolutional Networks">Graph Convolutional Networks</option>
                
                <option value="Graph Neural Networks">Graph Neural Networks</option>
                
                <option value="Graphical Model">Graphical Model</option>
                
                <option value="Graphics">Graphics</option>
                
                <option value="Guidelines">Guidelines</option>
                
                <option value="HDR Imaging">HDR Imaging</option>
                
                <option value="Hair Analysis">Hair Analysis</option>
                
                <option value="Hair Characterization">Hair Characterization</option>
                
                <option value="Hair Segmentation">Hair Segmentation</option>
                
                <option value="Hair Style Recognition">Hair Style Recognition</option>
                
                <option value="Head Pose Estimation">Head Pose Estimation</option>
                
                <option value="Healthcare Monitoring">Healthcare Monitoring</option>
                
                <option value="Human Action Recognition">Human Action Recognition</option>
                
                <option value="Human Action Synthesis">Human Action Synthesis</option>
                
                <option value="Human Activity Analysis">Human Activity Analysis</option>
                
                <option value="Human Attribute Recognition">Human Attribute Recognition</option>
                
                <option value="Human Behavior Analysis">Human Behavior Analysis</option>
                
                <option value="Human Detection">Human Detection</option>
                
                <option value="Human Re-identification">Human Re-identification</option>
                
                <option value="Human Recognition">Human Recognition</option>
                
                <option value="Human vs Machine Performance">Human vs Machine Performance</option>
                
                <option value="Hybrid Encryption">Hybrid Encryption</option>
                
                <option value="Illumination Compensation">Illumination Compensation</option>
                
                <option value="Image Analysis">Image Analysis</option>
                
                <option value="Image Database">Image Database</option>
                
                <option value="Image Dataset">Image Dataset</option>
                
                <option value="Image Enhancement">Image Enhancement</option>
                
                <option value="Image Interpolation">Image Interpolation</option>
                
                <option value="Image Normalization">Image Normalization</option>
                
                <option value="Image Processing">Image Processing</option>
                
                <option value="Image Quality">Image Quality</option>
                
                <option value="Image Registration">Image Registration</option>
                
                <option value="Image Segmentation">Image Segmentation</option>
                
                <option value="Image-Text Mapping">Image-Text Mapping</option>
                
                <option value="In-The-Wild">In-The-Wild</option>
                
                <option value="In-the-Wild">In-the-Wild</option>
                
                <option value="Indexing">Indexing</option>
                
                <option value="Information Processing">Information Processing</option>
                
                <option value="Information Retrieval">Information Retrieval</option>
                
                <option value="Interpolation Methods">Interpolation Methods</option>
                
                <option value="Interpretability">Interpretability</option>
                
                <option value="Interpretable AI">Interpretable AI</option>
                
                <option value="Iris Biometrics">Iris Biometrics</option>
                
                <option value="Iris Recognition">Iris Recognition</option>
                
                <option value="IrisCode">IrisCode</option>
                
                <option value="IrisCodes">IrisCodes</option>
                
                <option value="Iterative Analysis">Iterative Analysis</option>
                
                <option value="Keypoint Correspondences">Keypoint Correspondences</option>
                
                <option value="Keypoint Detection">Keypoint Detection</option>
                
                <option value="Keypoint Matching">Keypoint Matching</option>
                
                <option value="Laplacian Methods">Laplacian Methods</option>
                
                <option value="Leishmania">Leishmania</option>
                
                <option value="Linear Discriminant">Linear Discriminant</option>
                
                <option value="Literature Review">Literature Review</option>
                
                <option value="Long-term Recognition">Long-term Recognition</option>
                
                <option value="MICHE">MICHE</option>
                
                <option value="MICHE Competition">MICHE Competition</option>
                
                <option value="MRI">MRI</option>
                
                <option value="Machine Learning">Machine Learning</option>
                
                <option value="Machine Learning Explainability">Machine Learning Explainability</option>
                
                <option value="Manufacturing">Manufacturing</option>
                
                <option value="Markov Random Field">Markov Random Field</option>
                
                <option value="Markov Random Fields">Markov Random Fields</option>
                
                <option value="Master-Slave Architecture">Master-Slave Architecture</option>
                
                <option value="Master-slave Calibration">Master-slave Calibration</option>
                
                <option value="Master-slave System">Master-slave System</option>
                
                <option value="Medical Imaging">Medical Imaging</option>
                
                <option value="Mobile Biometrics">Mobile Biometrics</option>
                
                <option value="Model Quantization">Model Quantization</option>
                
                <option value="Motion Representation">Motion Representation</option>
                
                <option value="Multi-Agent Systems">Multi-Agent Systems</option>
                
                <option value="Multi-biometrics">Multi-biometrics</option>
                
                <option value="Multi-camera Systems">Multi-camera Systems</option>
                
                <option value="Multi-dimensional Taxonomy">Multi-dimensional Taxonomy</option>
                
                <option value="Multi-output Classification">Multi-output Classification</option>
                
                <option value="Multi-spectral Data">Multi-spectral Data</option>
                
                <option value="Multi-task Learning">Multi-task Learning</option>
                
                <option value="Multimodal Biometrics">Multimodal Biometrics</option>
                
                <option value="Multimodal Learning">Multimodal Learning</option>
                
                <option value="Multimodal Recognition">Multimodal Recognition</option>
                
                <option value="Multiple Signatures">Multiple Signatures</option>
                
                <option value="NICE Challenge">NICE Challenge</option>
                
                <option value="NICE Contest">NICE Contest</option>
                
                <option value="Natural Language Processing">Natural Language Processing</option>
                
                <option value="Near-Infrared">Near-Infrared</option>
                
                <option value="Nearest Neighbor">Nearest Neighbor</option>
                
                <option value="Negative Sampling">Negative Sampling</option>
                
                <option value="Neural Networks">Neural Networks</option>
                
                <option value="Noise Detection">Noise Detection</option>
                
                <option value="Noisy Environments">Noisy Environments</option>
                
                <option value="Noisy Images">Noisy Images</option>
                
                <option value="Non-Cooperative">Non-Cooperative</option>
                
                <option value="Non-Cooperative Recognition">Non-Cooperative Recognition</option>
                
                <option value="Non-cooperative Recognition">Non-cooperative Recognition</option>
                
                <option value="Ocular Biometrics">Ocular Biometrics</option>
                
                <option value="Ocular Recognition">Ocular Recognition</option>
                
                <option value="Ocular Region">Ocular Region</option>
                
                <option value="Oculomotor Plant Characteristics">Oculomotor Plant Characteristics</option>
                
                <option value="Outdoor Recognition">Outdoor Recognition</option>
                
                <option value="PTZ Camera">PTZ Camera</option>
                
                <option value="Pattern Matching">Pattern Matching</option>
                
                <option value="Pattern Recognition">Pattern Recognition</option>
                
                <option value="Peaches Detection">Peaches Detection</option>
                
                <option value="Pedestrian Attributes Recognition">Pedestrian Attributes Recognition</option>
                
                <option value="Pedestrian Detection">Pedestrian Detection</option>
                
                <option value="Pedestrian Tracking">Pedestrian Tracking</option>
                
                <option value="Performance Analysis">Performance Analysis</option>
                
                <option value="Performance Evaluation">Performance Evaluation</option>
                
                <option value="Periocular Biometrics">Periocular Biometrics</option>
                
                <option value="Periocular Recognition">Periocular Recognition</option>
                
                <option value="Periocular Region">Periocular Region</option>
                
                <option value="Person Re-identification">Person Re-identification</option>
                
                <option value="Planar Surfaces">Planar Surfaces</option>
                
                <option value="Pose Compensation">Pose Compensation</option>
                
                <option value="Pose Estimation">Pose Estimation</option>
                
                <option value="Precision Agriculture">Precision Agriculture</option>
                
                <option value="Privacy">Privacy</option>
                
                <option value="Privacy Protection">Privacy Protection</option>
                
                <option value="Pupil Detection">Pupil Detection</option>
                
                <option value="Quadruplet Loss">Quadruplet Loss</option>
                
                <option value="Quality Assessment">Quality Assessment</option>
                
                <option value="Quantum Computing">Quantum Computing</option>
                
                <option value="RANSAC">RANSAC</option>
                
                <option value="Railway Control">Railway Control</option>
                
                <option value="Re-identification">Re-identification</option>
                
                <option value="Real-Time Processing">Real-Time Processing</option>
                
                <option value="Real-time Detection">Real-time Detection</option>
                
                <option value="Real-time Processing">Real-time Processing</option>
                
                <option value="Reasoning">Reasoning</option>
                
                <option value="Receptive Fields">Receptive Fields</option>
                
                <option value="Region-Based CNN">Region-Based CNN</option>
                
                <option value="Research Overview">Research Overview</option>
                
                <option value="Retrieval">Retrieval</option>
                
                <option value="Reversible Anonymization">Reversible Anonymization</option>
                
                <option value="Reversible De-identification">Reversible De-identification</option>
                
                <option value="Sclera Recognition">Sclera Recognition</option>
                
                <option value="Score-Level Fusion">Score-Level Fusion</option>
                
                <option value="Search Models">Search Models</option>
                
                <option value="Security Solutions">Security Solutions</option>
                
                <option value="Security Systems">Security Systems</option>
                
                <option value="Segmentation">Segmentation</option>
                
                <option value="Segmentation Robustness">Segmentation Robustness</option>
                
                <option value="Segmentation-less">Segmentation-less</option>
                
                <option value="Self-Supervised Learning">Self-Supervised Learning</option>
                
                <option value="Self-supervised Learning">Self-supervised Learning</option>
                
                <option value="Semantic Segmentation">Semantic Segmentation</option>
                
                <option value="Semi-Supervised Learning">Semi-Supervised Learning</option>
                
                <option value="Shape Features">Shape Features</option>
                
                <option value="Signal Processing">Signal Processing</option>
                
                <option value="Single-View Metrology">Single-View Metrology</option>
                
                <option value="Skeleton-based Animation">Skeleton-based Animation</option>
                
                <option value="Skeleton-based Data">Skeleton-based Data</option>
                
                <option value="Skeleton-based Recognition">Skeleton-based Recognition</option>
                
                <option value="Sketch Understanding">Sketch Understanding</option>
                
                <option value="Soft Biometrics">Soft Biometrics</option>
                
                <option value="Sparse Representation">Sparse Representation</option>
                
                <option value="Sparse Representations">Sparse Representations</option>
                
                <option value="Spatial Transformation">Spatial Transformation</option>
                
                <option value="Special Issue">Special Issue</option>
                
                <option value="Speech Analysis">Speech Analysis</option>
                
                <option value="Squeeze and Excitation">Squeeze and Excitation</option>
                
                <option value="Stochastic Differential Equation">Stochastic Differential Equation</option>
                
                <option value="Stochastic Differential Equations">Stochastic Differential Equations</option>
                
                <option value="Structural Pattern Analysis">Structural Pattern Analysis</option>
                
                <option value="Super-Resolution">Super-Resolution</option>
                
                <option value="Super-resolution">Super-resolution</option>
                
                <option value="Surveillance">Surveillance</option>
                
                <option value="Survey">Survey</option>
                
                <option value="Sustainability">Sustainability</option>
                
                <option value="Synthetic Data">Synthetic Data</option>
                
                <option value="Systems">Systems</option>
                
                <option value="Tensor Processing Unit">Tensor Processing Unit</option>
                
                <option value="Text Retrieval">Text Retrieval</option>
                
                <option value="Textile Industry">Textile Industry</option>
                
                <option value="Tone Mapping">Tone Mapping</option>
                
                <option value="Tracking">Tracking</option>
                
                <option value="Transfer Learning">Transfer Learning</option>
                
                <option value="Trends and Controversies">Trends and Controversies</option>
                
                <option value="Triplet Loss">Triplet Loss</option>
                
                <option value="Unconstrained Biometrics">Unconstrained Biometrics</option>
                
                <option value="Unconstrained Conditions">Unconstrained Conditions</option>
                
                <option value="Unconstrained Environments">Unconstrained Environments</option>
                
                <option value="Unconstrained Recognition">Unconstrained Recognition</option>
                
                <option value="Unconstrained Scenarios">Unconstrained Scenarios</option>
                
                <option value="Uncontrolled Environments">Uncontrolled Environments</option>
                
                <option value="Unmanned Aerial Vehicles">Unmanned Aerial Vehicles</option>
                
                <option value="Vantage-Point trees">Vantage-Point trees</option>
                
                <option value="Video Anomaly Detection">Video Anomaly Detection</option>
                
                <option value="Visible Wavelength">Visible Wavelength</option>
                
                <option value="Vision Transformers">Vision Transformers</option>
                
                <option value="Visual Explanations">Visual Explanations</option>
                
                <option value="Visual Interpretability">Visual Interpretability</option>
                
                <option value="Visual Surveillance">Visual Surveillance</option>
                
                <option value="Weak Supervision">Weak Supervision</option>
                
                <option value="Weakly Supervised Learning">Weakly Supervised Learning</option>
                
                <option value="Web-Based Tools">Web-Based Tools</option>
                
                <option value="Weed Control">Weed Control</option>
                
                <option value="Wild Conditions">Wild Conditions</option>
                
                <option value="Yield Estimation">Yield Estimation</option>
                
                <option value="Zero-Shot Learning">Zero-Shot Learning</option>
                
            </select>
        </div>

        <!-- Sort By -->
        <div>
            <label for="sort-by" class="block text-sm font-medium text-gray-700 mb-2">Sort By</label>
            <select id="sort-by" class="w-full rounded-lg border-gray-300 shadow-sm focus:border-blue-600 focus:ring focus:ring-blue-600 focus:ring-opacity-50">
                <option value="year">Year (Newest First)</option>
                <option value="title">Title</option>
                <option value="venue">Venue</option>
            </select>
        </div>
    </div>
    
    <!-- Search Publications -->
    <div class="mt-4 relative">
        <input type="text" id="publication-search" placeholder="Search publications..." class="w-full px-4 py-2 rounded-lg border border-gray-300 focus:ring-2 focus:ring-blue-600 focus:border-blue-600 outline-none pl-10">
        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 absolute left-3 top-1/2 transform -translate-y-1/2 text-gray-400" fill="none" viewBox="0 0 24 24" stroke="currentColor">
            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z" />
        </svg>
    </div>
    
    <!-- Results Count -->
    <div class="mt-4 text-sm text-gray-600">
        Showing <span id="publications-count">0</span> publications
    </div>
</div>

<!-- Publications List -->
<div id="publications-list" class="space-y-6">
    
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2025" 
         data-type="journal"
         data-title="bias: a body-based interpretable active speaker approach"
         data-authors="tiago roxo, joana c. costa, pedro inácio, hugo proença"
         data-venue="ieee transactions on biometrics, behaviour and identity science"
         data-tags="active speaker detection,biometrics,interpretable ai,multimodal learning"
         data-id="1">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/tbiom.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">BIAS: A Body-based Interpretable Active Speaker Approach</h3>
                <p class="text-gray-600 mb-2">Tiago Roxo, Joana C. Costa, Pedro Inácio, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Biometrics, Behaviour and Identity Science</span> (2025)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Active Speaker Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Interpretable AI</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Multimodal Learning</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TBIOM.2024.3520030" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/BIAS_TBIOM.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="1">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="1">
            <h2>BIAS: A Body-based Interpretable Active Speaker Approach</h2>
            <p class="authors">Tiago Roxo, Joana C. Costa, Pedro Inácio, Hugo Proença</p>
            <p class="venue">IEEE Transactions on Biometrics, Behaviour and Identity Science (2025)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>State-of-the-art Active Speaker Detection (ASD) approaches heavily rely on audio and facial features to perform, which is not a sustainable approach in wild scenarios. Although these methods achieve good results in the standard AVA-ActiveSpeaker set, a recent wilder ASD dataset (WASD) showed the limitations of such models and raised the need for new approaches. As such, we propose BIAS, a model that, for the first time, combines audio, face, and body information, to accurately predict active speakers in varying/challenging conditions. Additionally, we design BIAS to provide interpretability by proposing a novel use for Squeeze-and-Excitation blocks, namely in attention heat-maps creation and feature importance assessment. For a full interpretability setup, we annotate an ASD-related actions dataset (ASD-Text) to fine-tune a ViT-GPT2 for text scene description to complement BIAS interpretability. The results show that BIAS is state-of-the-art in challenging conditions where body-based features are of utmost importance (Columbia, open settings, and WASD), and yields competitive results in AVAActiveSpeaker, where face is more influential than body for ASD. BIAS interpretability also shows the features/aspects more relevant towards ASD prediction in varying settings, making it a strong baseline for further developments in interpretable ASD models, and is available at https://github.com/Tiago-Roxo/BIAS.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TBIOM.2024.3520030" target="_blank">10.1109/TBIOM.2024.3520030</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/BIAS_TBIOM.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2025" 
         data-type="journal"
         data-title="synthesizing multilevel abstraction ear sketches for enhanced biometric recognition"
         data-authors="david freire-obregón, ziga emersic, blaz meden, joão c. neves, modesto castrillon-santana, hugo proença"
         data-venue="elsevier image and vision computing"
         data-tags="biometric recognition,ear sketches,sketch understanding,triplet loss,abstraction levels"
         data-id="2">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-166.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Synthesizing Multilevel Abstraction Ear Sketches for Enhanced Biometric Recognition</h3>
                <p class="text-gray-600 mb-2">David Freire-Obregón, Ziga Emersic, Blaz Meden, João C. Neves, Modesto Castrillon-Santana, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Image and Vision Computing</span> (2025)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Ear Sketches</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Sketch Understanding</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Triplet Loss</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Abstraction Levels</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.imavis.2025.105424" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Obregon_IVC.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="2">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="2">
            <h2>Synthesizing Multilevel Abstraction Ear Sketches for Enhanced Biometric Recognition</h2>
            <p class="authors">David Freire-Obregón, Ziga Emersic, Blaz Meden, João C. Neves, Modesto Castrillon-Santana, Hugo Proença</p>
            <p class="venue">Elsevier Image and Vision Computing (2025)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Sketch understanding is a significant challenge for general-purpose vision algorithms due to the sparse nature of this kind of drawing when compared to natural visual inputs and their semantic ambiguity, as sketches can evoke multiple interpretations simultaneously. Traditionally, research in sketch-based recognition has been predominantly focused on facial data, with particular emphasis on forensics/law enforcement applications. Our work takes a step forward by shifting the focus to ear images and considering the ''sketch-2-image'' matching problem with respect to the level of sketch abstraction. We introduce a novel adaptation of the well-known triplet loss, designed to fuse multiple abstraction levels of sketches during training. Here, the level of abstraction is inversely related to the number of strokes used to illustrate the ear, whereas the number of strokes used in the sketch is inversely correspondent to its abstraction level. Upon the experiments conducted in four well-known ear datasets, we observed a consistently higher performance of our proposal compared to the state-of-the-art. Finally, such results might easily be extended to other biometric traits, which is also positively regarded and raises an interesting research topic.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.imavis.2025.105424" target="_blank">10.1016/j.imavis.2025.105424</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Obregon_IVC.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2025" 
         data-type="journal"
         data-title="causality, machine learning, and feature selection: a survey"
         data-authors="asmae lamsaf, rui carrilho, joão c. neves, hugo proença"
         data-venue="sensors"
         data-tags="causality,machine learning,feature selection,causal discovery,causal inference"
         data-id="3">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-168.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Causality, Machine Learning, and Feature Selection: A survey</h3>
                <p class="text-gray-600 mb-2">Asmae Lamsaf, Rui Carrilho, João C. Neves, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Sensors</span> (2025)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Causality</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Machine Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Feature Selection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Causal Discovery</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Causal Inference</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.3390/s25082373" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Asmae_sensors_2025.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="3">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="3">
            <h2>Causality, Machine Learning, and Feature Selection: A survey</h2>
            <p class="authors">Asmae Lamsaf, Rui Carrilho, João C. Neves, Hugo Proença</p>
            <p class="venue">Sensors (2025)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Causality, which involves distinguishing between cause and effect, is essential for understanding complex relationships in data. This paper provides a review of causality in two key areas: causal discovery and causal inference. Causal discovery transforms data into graphical structures that illustrate how variables influence one another, while causal inference quantifies the impact of these variables on a target outcome. The models are more robust and accurate with the integration of causal reasoning into machine learning, improving applications like prediction and classification. We present various methods used in detecting causal relationships and how these can be applied in selecting or extracting relevant features, particularly from sensor datasets. When causality is used in feature selection, it supports applications like fault detection, anomaly detection, and predictive maintenance applications critical to the maintenance of complex systems. Traditional correlation-based methods of feature selection often overlook significant causal links, leading to incomplete insights. Our research highlights how integrating causality can be integrated and lead to stronger, deeper feature selection and ultimately enable better decision-making in machine learning tasks.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.3390/s25082373" target="_blank">10.3390/s25082373</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Asmae_sensors_2025.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2025" 
         data-type="journal"
         data-title="a laplacian-based quantum graph neural network for semi-supervised learning"
         data-authors="hamed gholipour, farid bozorgnia, kailash hambarde, hamzeh mohammadigheymasi, javier mancilla, andre sequeira, joão c. neves, hugo proença"
         data-venue="springer quantum information processing"
         data-tags="quantum computing,graph neural networks,semi-supervised learning,laplacian methods,entanglement entropy"
         data-id="4">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-167.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">A Laplacian-based Quantum Graph Neural Network for Semi-Supervised Learning</h3>
                <p class="text-gray-600 mb-2">Hamed Gholipour, Farid Bozorgnia, Kailash Hambarde, Hamzeh Mohammadigheymasi, Javier Mancilla, Andre Sequeira, João C. Neves, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Springer Quantum Information Processing</span> (2025)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Quantum Computing</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Graph Neural Networks</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Semi-Supervised Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Laplacian Methods</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Entanglement Entropy</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1007/s11128-025-04725-6" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Hamed_qip.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="4">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="4">
            <h2>A Laplacian-based Quantum Graph Neural Network for Semi-Supervised Learning</h2>
            <p class="authors">Hamed Gholipour, Farid Bozorgnia, Kailash Hambarde, Hamzeh Mohammadigheymasi, Javier Mancilla, Andre Sequeira, João C. Neves, Hugo Proença</p>
            <p class="venue">Springer Quantum Information Processing (2025)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Laplacian learning method is a well-established technique in classical graph-based semi-supervised learning, but its potential in the quantum domain remains largely unexplored. This study investigates the performance of the Laplacian-based Quantum Semi-Supervised Learning (QSSL) method across four benchmark datasets—Iris, Wine, Breast Cancer Wisconsin, and Heart Disease. Further analysis explores the impact of increasing qubit counts, revealing that adding more qubits to a quantum system doesn't always improve performance. The effectiveness of additional qubits depends on the quantum algorithm and how well it matches the dataset. Additionally, we examine the effects of varying entangling layers on entanglement entropy and test accuracy. The performance of Laplacian learning is highly dependent on the number of entangling layers, with optimal configurations varying across different datasets. Typically, moderate levels of entanglement offer the best balance between model complexity and generalization capabilities. These observations highlight the crucial need for precise hyperparameter tuning tailored to each dataset to achieve optimal performance in Laplacian learning methods.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1007/s11128-025-04725-6" target="_blank">10.1007/s11128-025-04725-6</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Hamed_qip.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2024" 
         data-type="conference"
         data-title="cfc-ate: causal feature construction via average treatment effect"
         data-authors="asmae lamsaf, joão c. neves, hugo proença"
         data-venue="ieee international conference on machine learning and applications (icmla'24)"
         data-tags="causal feature construction,average treatment effect,dimensionality reduction,causal discovery,causal inference"
         data-id="5">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-162.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">CFC-ATE: Causal Feature Construction via Average Treatment Effect</h3>
                <p class="text-gray-600 mb-2">Asmae Lamsaf, João C. Neves, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE International Conference on Machine Learning and Applications (ICMLA'24)</span> (2024)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Causal Feature Construction</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Average Treatment Effect</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Dimensionality Reduction</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Causal Discovery</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Causal Inference</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/asmae_icmla_2024.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="5">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="5">
            <h2>CFC-ATE: Causal Feature Construction via Average Treatment Effect</h2>
            <p class="authors">Asmae Lamsaf, João C. Neves, Hugo Proença</p>
            <p class="venue">IEEE International Conference on Machine Learning and Applications (ICMLA'24) (2024)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Dimensionality reduction is a crucial step in data preprocessing, particularly for high-dimensional datasets, where the excessive number of features increases the risk of overfitting in machine learning models. Traditional dimensionality reduction methods rely on statistical associations or the relative position of the feature embeddings in the hyper-space to map original features to a compact subspace that preserves the most relevant information of the data. However, these methods fail to capture the causal relationships among variables during the transformation process, leading to a loss of structural coherence of the data in low-dimensional spaces. By employing causal discovery and causal inference, it is possible to simplify these problems, effectively merging critical features while reducing both complexity and dimensionality. Our paper introduces a novel approach, Causal Feature Construction via Average Treatment Effect (CFC-ATE), which leverages causal discovery and inference to create more interpretable and reliable features for predictive modeling. Our methodology consists of the following phases: i) leveraging the causal structure of data through the inference of the causal graph. ii) transforming features through the use of the average treatment effect conditioned on the causal structure of the data. The experiments on diverse real-world datasets and synthetic datasets demonstrate the effectiveness of CFC-ATE in improving model performance by comparing it with three methods of feature selection and three benchmark dimensionality reduction techniques.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/" target="_blank"></a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/asmae_icmla_2024.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2024" 
         data-type="journal"
         data-title="a novel dataset for fabric defect detection: bridging gaps in anomaly detection"
         data-authors="rui carrilho, kailash hambarde, hugo proença"
         data-venue="mdpi applied sciences"
         data-tags="fabric defect detection,anomaly detection,dataset,textile industry,deep learning"
         data-id="6">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-158.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">A Novel Dataset for Fabric Defect Detection: Bridging Gaps in Anomaly Detection</h3>
                <p class="text-gray-600 mb-2">Rui Carrilho, Kailash Hambarde, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">MDPI Applied Sciences</span> (2024)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Fabric Defect Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Anomaly Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Dataset</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Textile Industry</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Deep Learning</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.3390/app14125298" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/carrilho_as_2024.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="6">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="6">
            <h2>A Novel Dataset for Fabric Defect Detection: Bridging Gaps in Anomaly Detection</h2>
            <p class="authors">Rui Carrilho, Kailash Hambarde, Hugo Proença</p>
            <p class="venue">MDPI Applied Sciences (2024)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Detecting anomalies in texture has become a significant concern across various industrial processes. One prevalent application of this is in inspecting patterned textures, especially in the domain of fabric defect detection, which is a commonly encountered scenario. This task entails dealing with a wide array of colours and textile varieties, spanning a broad spectrum of fabrics. Due to the extensive diversity in colours, textures, and defect characteristics, fabric defect detection presents a complex and formidable challenge within the realm of patterned texture inspection. While recent trends have seen a rise in the utilization of deep learning methods for anomaly detection, there still exist notable gaps in this field. In this paper, we introduce a novel dataset comprising a diverse selection of fabrics and defects from a textile company based in Portugal. Our contributions encompass the provision of this unique dataset and the evaluation of state-of-the-art (SOTA) methods performance on our dataset.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.3390/app14125298" target="_blank">10.3390/app14125298</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/carrilho_as_2024.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2024" 
         data-type="journal"
         data-title="deep learning for iris recognition: a survey"
         data-authors="kien nguyen, hugo proença, fernando alonso-fernandez"
         data-venue="acm computing surveys"
         data-tags="deep learning,iris recognition,biometrics,survey"
         data-id="7">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/csur.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Deep Learning for Iris Recognition: A Survey</h3>
                <p class="text-gray-600 mb-2">Kien Nguyen, Hugo Proença, Fernando Alonso-Fernandez</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">ACM Computing Surveys</span> (2024)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Deep Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Survey</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1145/3651306" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/iris_survey.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="7">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="7">
            <h2>Deep Learning for Iris Recognition: A Survey</h2>
            <p class="authors">Kien Nguyen, Hugo Proença, Fernando Alonso-Fernandez</p>
            <p class="venue">ACM Computing Surveys (2024)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>In this survey, we provide a comprehensive review of more than 200 papers, technical reports, and GitHub repositories published over the last 10 years on the recent developments of deep learning techniques for iris recognition, covering broad topics on algorithm designs, open-source tools, open challenges, and emerging research. First, we conduct a comprehensive analysis of deep learning techniques developed for two main sub-tasks in iris biometrics: segmentation and recognition. Second, we focus on deep learning techniques for the robustness of iris recognition systems against presentation attacks and via human-machine pairing. Third, we delve deep into deep learning techniques for forensic application, especially in post-mortem iris recognition. Fourth, we review open-source resources and tools in deep learning techniques for iris recognition. Finally, we highlight the technical challenges, emerging research trends, and outlook for the future of deep learning in iris recognition.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1145/3651306" target="_blank">10.1145/3651306</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/iris_survey.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2024" 
         data-type="journal"
         data-title="fake it till you recognize it: quality assessment for human action generative models"
         data-authors="bruno degardin, vasco lopes, hugo proença"
         data-venue="ieee transactions on biometrics, behaviour and identity science"
         data-tags="human action recognition,generative models,quality assessment,skeleton-based animation,synthetic data"
         data-id="8">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/tbiom.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Fake It Till You Recognize It: Quality Assessment for Human Action Generative Models</h3>
                <p class="text-gray-600 mb-2">Bruno Degardin, Vasco Lopes, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Biometrics, Behaviour and Identity Science</span> (2024)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Human Action Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Generative Models</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Quality Assessment</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Skeleton-based Animation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Synthetic Data</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TBIOM.2024.3375453" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Degardin_TBIOM_2024.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="8">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="8">
            <h2>Fake It Till You Recognize It: Quality Assessment for Human Action Generative Models</h2>
            <p class="authors">Bruno Degardin, Vasco Lopes, Hugo Proença</p>
            <p class="venue">IEEE Transactions on Biometrics, Behaviour and Identity Science (2024)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Skeleton-based generative modelling is an important research topic to mitigate the heavy annotation process. In this work, we explore the impact of synthetic data on skeleton-based action recognition alongside its evaluation methods for more precise quality extraction. We propose a novel iterative weakly-supervised learning generative strategy for synthesising high-quality human actions. We combine conditional generative models with Bayesian classifiers to select the highest-quality samples. As an essential factor, we designed a discriminator network that, together with a Bayesian classifier relies on the most realistic instances to augment the amount of data available for the next iteration without requiring standard cumbersome annotation processes. Additionally, as a key contribution to assessing the quality of samples, we propose a novel measure based on human kinematics instead of employing commonly used evaluation methods, which are heavily based on images. The rationale is to capture the intrinsic characteristics of human skeleton dynamics, thereby complementing model comparison and alleviating the need to manually select the best samples. Experiments were carried out over four benchmarks of two well-known datasets (NTU RGB+D and NTU-120 RGB+D), where both our framework and model assessment can notably enhance skeleton-based action recognition and generation models by synthesising high-quality and realistic human actions.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TBIOM.2024.3375453" target="_blank">10.1109/TBIOM.2024.3375453</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Degardin_TBIOM_2024.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2024" 
         data-type="journal"
         data-title="towards automated fabric defect detection: a survey of recent computer vision approaches"
         data-authors="rui carrilho, ehsan yaghoubi, josé lindo, kailash hambarde, hugo proença"
         data-venue="mdpi electronics"
         data-tags="fabric defect detection,computer vision,textile industry,survey,machine learning"
         data-id="9">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-161.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Towards Automated Fabric Defect Detection: A Survey of Recent Computer Vision Approaches</h3>
                <p class="text-gray-600 mb-2">Rui Carrilho, Ehsan Yaghoubi, José Lindo, Kailash Hambarde, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">MDPI Electronics</span> (2024)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Fabric Defect Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Computer Vision</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Textile Industry</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Survey</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Machine Learning</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.3390/electronics13183728" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/carrilho_electronics_2024.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="9">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="9">
            <h2>Towards Automated Fabric Defect Detection: A Survey of Recent Computer Vision Approaches</h2>
            <p class="authors">Rui Carrilho, Ehsan Yaghoubi, José Lindo, Kailash Hambarde, Hugo Proença</p>
            <p class="venue">MDPI Electronics (2024)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Defect detection is a crucial part of the pipeline in many industries. In the textile industry, it is especially important, as it will affect the quality and price of the final product. However, it is mostly performed by human agents, who have been reported to have poor performance, along with a costly and time-consuming training process. As such, methods to automate the process have been increasingly explored throughout the last 20 years. While there are many traditional approaches to this problem, with the advent of deep learning, machine learning-based approaches now constitute the majority of all possible approaches. Other articles have explored traditional approaches and machine learning approaches in a more general way, detailing their evolution throughout time. In this review, we will summarize the most important advancements of the last 5 years, and focus mostly on machine learning-based approaches. We also outline the most promising avenues of research in the future.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.3390/electronics13183728" target="_blank">10.3390/electronics13183728</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/carrilho_electronics_2024.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2024" 
         data-type="conference"
         data-title="video anomaly detection in overlapping data: the more cameras, the better?"
         data-authors="silas santiago, josé everardo maia, hugo proença"
         data-venue="ieee international joint conference on biometrics (ijcb'24)"
         data-tags="video anomaly detection,multi-camera systems,weakly supervised learning,surveillance"
         data-id="10">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-160.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Video Anomaly Detection in Overlapping Data: The More Cameras, the Better?</h3>
                <p class="text-gray-600 mb-2">Silas Santiago, José Everardo Maia, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE International Joint Conference on Biometrics (IJCB'24)</span> (2024)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Video Anomaly Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Multi-camera Systems</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Weakly Supervised Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Surveillance</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/henrique_ijcb_2024.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="10">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="10">
            <h2>Video Anomaly Detection in Overlapping Data: The More Cameras, the Better?</h2>
            <p class="authors">Silas Santiago, José Everardo Maia, Hugo Proença</p>
            <p class="venue">IEEE International Joint Conference on Biometrics (IJCB'24) (2024)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Video anomaly detection (VAD) has been densely explored in the last few years, mostly in single-camera scenarios. Despite significant advancements in this field, effectiveness is still seriously compromised in challenging environments (e.g., varying lighting conditions, under partial occlusions, and in crowded environments). For the sake of affordable data annotation, the most relevant methods assume the weakly supervised paradigm, where the label is available only at the video level (WS-VAD). Also, these methods are conventionally designed for single-camera mode and do not consider the multi-view information yielded from overlapping surveillance cameras, which is very common in practical scenarios. In this work, we started by systematically evaluating the WS-VAD performance that can be attained when different camera combinations are used as data sources. Interestingly, we observed that the rule 'the more cameras, the better' should not be assumed, as there were always particular subsets of cameras that consistently outperformed the remaining configurations. Upon these conclusions, we present a semi-automated procedure to identify the optimal camera sources based on the image features/characteristics (distance, pose, and lighting) each one is capturing. Extensive experiments were carried out in three overlapping multi-camera datasets, which suggest that 1) multi-camera schemes consistently outperform single-camera methods and - most interestingly - 2) the correlation between the data acquired by the different cameras severely impacts performance, turning the selection of cameras a crucial step in VAD. Our findings open an intriguing research topic about methods/algorithms that filter out/select the camera sources we should use in overlapping camera scenarios.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/" target="_blank"></a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/henrique_ijcb_2024.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2024" 
         data-type="conference"
         data-title="towards zero-shot interpretable human recognition: a 2d-3d registration framework"
         data-authors="henrique jesus, hugo proença"
         data-venue="ieee international joint conference on biometrics (ijcb'24)"
         data-tags="zero-shot learning,human recognition,2d-3d registration,interpretability,synthetic data"
         data-id="11">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-159.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Towards Zero-Shot Interpretable Human Recognition: A 2D-3D Registration Framework</h3>
                <p class="text-gray-600 mb-2">Henrique Jesus, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE International Joint Conference on Biometrics (IJCB'24)</span> (2024)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Zero-Shot Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Human Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">2D-3D Registration</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Interpretability</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Synthetic Data</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/henrique_ijcb_2024.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="11">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="11">
            <h2>Towards Zero-Shot Interpretable Human Recognition: A 2D-3D Registration Framework</h2>
            <p class="authors">Henrique Jesus, Hugo Proença</p>
            <p class="venue">IEEE International Joint Conference on Biometrics (IJCB'24) (2024)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Large vision models based in deep learning architectures have been consistently advancing the state-of-the-art in biometric recognition. However, three weaknesses are commonly reported for such kind of approaches: 1) their extreme demands in terms of learning data; 2) the difficulties in generalising between different domains; and 3) the lack of interpretability/explainability, with biometrics being of particular interest, as it is important to provide evidence able to be used for forensics/legal purposes (e.g., in courts). To the best of our knowledge, this paper describes the first recognition framework/strategy that aims at addressing the three weaknesses simultaneously. At first, it relies exclusively in synthetic samples for learning purposes. Instead of requiring a large amount and variety of samples for each subject, the idea is to exclusively enroll a 3D point cloud per identity. Then, using generative strategies, we synthesize a very large (potentially infinite) number of samples, containing all the desired covariates (poses, clothing, distances, perspectives, lighting, occlusions,...). Upon the synthesizing method used, it is possible to adapt precisely to different kind of domains, which accounts for generalization purposes. Such data are then used to learn a model that performs local registration between image pairs, establishing positive correspondences between body parts that are the key, not only to recognition (according to cardinality and distribution), but also to provide an interpretable description of the response (e.g.: 'both samples are from the same person, as they have similar facial shape, hair color and legs thickness').</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/" target="_blank"></a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/henrique_ijcb_2024.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2024" 
         data-type="conference"
         data-title="how to squeeze an explanation out of your model"
         data-authors="tiago roxo, joana c. costa, pedro inácio, hugo proença"
         data-venue="4th workshop on explainable & interpretable artificial intelligence for biometrics (xai4biometrics), european conference on computer vision (eccv)"
         data-tags="interpretability,explainable ai,squeeze and excitation,biometrics,attention heatmaps"
         data-id="12">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-163.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">How to Squeeze An Explanation Out of Your Model</h3>
                <p class="text-gray-600 mb-2">Tiago Roxo, Joana C. Costa, Pedro Inácio, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">4th Workshop On Explainable & Interpretable Artificial Intelligence for Biometrics (xAI4Biometrics), European Conference on Computer Vision (ECCV)</span> (2024)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Interpretability</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Explainable AI</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Squeeze and Excitation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Attention Heatmaps</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/tiago_eccvw_2024.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="12">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="12">
            <h2>How to Squeeze An Explanation Out of Your Model</h2>
            <p class="authors">Tiago Roxo, Joana C. Costa, Pedro Inácio, Hugo Proença</p>
            <p class="venue">4th Workshop On Explainable & Interpretable Artificial Intelligence for Biometrics (xAI4Biometrics), European Conference on Computer Vision (ECCV) (2024)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Deep learning models are widely used nowadays for their reliability in performing various tasks. However, they do not typically provide the reasoning behind their decision, which is a significant drawback, particularly for more sensitive areas such as biometrics, security and healthcare. The most commonly used approaches to provide interpretability create visual attention heatmaps of regions of interest on an image based on models gradient backpropagation. Although this is a viable approach, current methods are targeted toward image settings and default/standard deep learning models, meaning that they require significant adaptations to work on video/multi-modal settings and custom architectures. This paper proposes an approach for interpretability that is model-agnostic, based on a novel use of the Squeeze and Excitation (SE) block that creates visual attention heatmaps. By including an SE block prior to the classification layer of any model, we are able to retrieve the most influential features via SE vector manipulation, one of the key components of the SE block. Our results show that this new SE-based interpretability can be applied to various models in image and video/multi-modal settings, namely biometrics of facial features with CelebA and behavioral biometrics using Active Speaker Detection datasets. Furthermore, our proposal does not compromise model performance toward the original task, and has competitive results with current interpretability approaches in state-of-the-art object datasets, highlighting its robustness to perform in varying data aside from the biometric context.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/" target="_blank"></a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/tiago_eccvw_2024.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2024" 
         data-type="journal"
         data-title="cutting-edge biometrics research: selected best papers from ijcb 2023"
         data-authors="anderson rocha, kevin bowyer, luisa verdoliva, zhen lei, hugo proença"
         data-venue="ieee transactions on biometrics, behaviour and identity science"
         data-tags="biometrics,special issue,research overview,conference papers"
         data-id="13">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-164.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Cutting-Edge Biometrics Research: Selected Best Papers From IJCB 2023</h3>
                <p class="text-gray-600 mb-2">Anderson Rocha, Kevin Bowyer, Luisa Verdoliva, Zhen Lei, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Biometrics, Behaviour and Identity Science</span> (2024)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Special Issue</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Research Overview</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Conference Papers</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TBIOM.2024.3459108" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/IJCB2023_TBIOM.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="13">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="13">
            <h2>Cutting-Edge Biometrics Research: Selected Best Papers From IJCB 2023</h2>
            <p class="authors">Anderson Rocha, Kevin Bowyer, Luisa Verdoliva, Zhen Lei, Hugo Proença</p>
            <p class="venue">IEEE Transactions on Biometrics, Behaviour and Identity Science (2024)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Welcome to the special issue of best-reviewed papers from the 2023 International Joint Conference on Biometrics (IJCB 2023), the premier forum for cutting-edge research and innovation in the field of biometrics. IJCB combines the previous two major biometrics conferences, the IEEE International Conference on Biometrics Theory, Applications, and Systems, and the IAPR International Conference on Biometrics. IJCB was made possible through a special agreement between the IEEE Biometrics Council and the IAPR Technical Committee on Biometrics (TC-4). As in the previous editions, the IJCB 2023 conference attracted high-quality submissions on a broad range of topics related to biometrics and supporting technologies. The conference received 199 papers, which underwent a rigorous peer-review procedure by the Program Chairs and 26 Area Chairs. More than 230 reviewers helped with the reviewing process. Ultimately, 72 (36.2%) of the highest-quality papers were accepted for presentation, out of which 30 (15.1%) were scheduled as orals and the remaining 42 (21.1%) as posters. Among the papers presented at the conference, selected authors of 13 papers with the best review ratings were invited to submit an extended version of their work to this special issue of the IEEE Transactions on Biometrics, Behavior, and Identity Science (TBIOM). The submissions in this special issue went through the normal peer review cycle at TBIOM, with revisions by the authors based on the reviews and a second round of reviews. One of the Guest Associate Editors recommended them for acceptance. We want to thank the authors and the reviewers for keeping to an ambitious timeline that enabled us to assemble this special issue promptly. We hope that you enjoy the papers in this special issue and that they will increase your interest in both IJCB and TBIOM.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TBIOM.2024.3459108" target="_blank">10.1109/TBIOM.2024.3459108</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/IJCB2023_TBIOM.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2024" 
         data-type="journal"
         data-title="wasd: a wilder active speaker detection dataset"
         data-authors="tiago roxo, joana c. costa, pedro inácio, hugo proença"
         data-venue="ieee transactions on biometrics, behaviour and identity science"
         data-tags="active speaker detection,dataset,surveillance,audio-visual processing,body features"
         data-id="14">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-157.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">WASD: A Wilder Active Speaker Detection Dataset</h3>
                <p class="text-gray-600 mb-2">Tiago Roxo, Joana C. Costa, Pedro Inácio, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Biometrics, Behaviour and Identity Science</span> (2024)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Active Speaker Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Dataset</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Surveillance</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Audio-Visual Processing</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Body Features</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TBIOM.2024.3412821" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/tiago_wasd_2024.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="14">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="14">
            <h2>WASD: A Wilder Active Speaker Detection Dataset</h2>
            <p class="authors">Tiago Roxo, Joana C. Costa, Pedro Inácio, Hugo Proença</p>
            <p class="venue">IEEE Transactions on Biometrics, Behaviour and Identity Science (2024)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Current Active Speaker Detection (ASD) models achieve good results on cooperative settings with reliable face access using only sound and facial features, which is not suited for less constrained conditions. To demonstrate this limitation of current datasets, we propose a Wilder Active Speaker Detection (WASD) dataset, with increased difficulty by targeting the key components of current ASD: audio and face. Grouped into 5 categories, WASD contains incremental challenges for ASD with tactical impairment of audio and face data, and provides a new source for ASD via subject body annotations. To highlight the new challenges of WASD, we divide it into Easy (cooperative settings) and Hard (audio and/or face are specifically degraded) groups, and assess state-of-the-art models performance in WASD and in the most challenging available ASD dataset: AVA-ActiveSpeaker. The results show that: 1) AVAActiveSpeaker prepares models for cooperative settings but not wilder ones (surveillance); and 2) current ASD approaches can not reliably perform in wilder settings, even if trained with challenging data. To prove the importance of body for wild ASD, we propose a baseline that complements body with face and audio information that surpass state-of-the-art models, particularly in the most challenging settings. All contributions are available at https://github.com/Tiago-Roxo/WASD</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TBIOM.2024.3412821" target="_blank">10.1109/TBIOM.2024.3412821</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/tiago_wasd_2024.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2024" 
         data-type="conference"
         data-title="defying limits: super-resolution refinement with diffusion guidance"
         data-authors="marcelo santos, joão c. neves, hugo proença, david menotti"
         data-venue="proceedings of the 19th international joint conference on computer vision, imaging and computer graphics theory and applications (visapp 2024)"
         data-tags="super-resolution,diffusion models,facial recognition,stochastic differential equation,image enhancement"
         data-id="15">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-149.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Defying Limits: Super-Resolution Refinement with Diffusion Guidance</h3>
                <p class="text-gray-600 mb-2">Marcelo Santos, João C. Neves, Hugo Proença, David Menotti</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the 19th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISAPP 2024)</span> (2024)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Super-Resolution</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Diffusion Models</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Facial Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Stochastic Differential Equation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Enhancement</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Marcelo_VISAPP_24.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="15">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="15">
            <h2>Defying Limits: Super-Resolution Refinement with Diffusion Guidance</h2>
            <p class="authors">Marcelo Santos, João C. Neves, Hugo Proença, David Menotti</p>
            <p class="venue">Proceedings of the 19th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications (VISAPP 2024) (2024)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Facial recognition has become widely applied across diverse environments. However, in unconstrained settings, face images often suffer from undesired effects such as low resolution, leading to a notable decline in recognition performance. The use of super-resolution (SR) algorithms is effective in supporting facial recognition in these cases. In the context of SR and image synthesis, diffusion models have consistently exhibited superior results. Moreover, diffusion models, as a whole, have garnered significant attention in recent years, surpassing the performance of traditional Generative Adversarial Networks (GANs) and achieving remarkable outcomes across various tasks. Additionally, by combining diffusion models with the gradient of a classifier, it becomes possible to generate data from a specific class. In this paper, we employ a diffusion model based on a Stochastic Differential Equation (SDE) to generate refined SR face images with an upsampling factor of 8× and 16× and address the challenges posed by unconstrained environments in facial recognition. The main contribution of our work lies in utilizing the gradient from a classifier to refine the SR results. This is performed by using soft biometrics such as gender and facial features to guide the SR process. To the best of our knowledge, this is the first time classifier guidance has been used to refine SR results of images from surveillance cameras. We conducted experiments on the CelebA and Quis-Campi datasets to evaluate our approach. The refined SR images exhibit enhanced details and improved visual quality. The quantitative performance is assessed using commonly used SR metrics as well as metrics from face recognition.The experimental results demonstrate the superior performance of our SR algorithm, surpassing other existing methods when applied to images from unconstrained scenarios.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/" target="_blank"></a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Marcelo_VISAPP_24.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2024" 
         data-type="conference"
         data-title="advancing manufacturing energy efficiency: the role of ai and web-based tools"
         data-authors="asmae lamsaf, pranita samale, hugo proença, joão c. neves, kailash hambarde"
         data-venue="proceedings of the 2024 international conference on emerging smart computing & informatics (esci 2024)"
         data-tags="manufacturing,energy efficiency,artificial intelligence,web-based tools,sustainability"
         data-id="16">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-151.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Advancing Manufacturing Energy Efficiency: The Role of AI and Web-Based Tools</h3>
                <p class="text-gray-600 mb-2">Asmae Lamsaf, Pranita Samale, Hugo Proença, João C. Neves, Kailash Hambarde</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the 2024 International Conference on Emerging Smart Computing & Informatics (ESCI 2024)</span> (2024)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Manufacturing</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Energy Efficiency</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Artificial Intelligence</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Web-Based Tools</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Sustainability</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Asmae_ESCI_24.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="16">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="16">
            <h2>Advancing Manufacturing Energy Efficiency: The Role of AI and Web-Based Tools</h2>
            <p class="authors">Asmae Lamsaf, Pranita Samale, Hugo Proença, João C. Neves, Kailash Hambarde</p>
            <p class="venue">Proceedings of the 2024 International Conference on Emerging Smart Computing & Informatics (ESCI 2024) (2024)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>This paper introduces a web-based application that simplifies the data analysis processing chain by automating the analysis of arbitrary variables. In particular, our application allows users to easily upload and process data for the analysis of a target variable by exploiting machine learning and evolutionary algorithms for precise forecasting and optimization. We demonstrate the system's efficacy using a dataset from a textile company, where our application successfully predicted the target variables with a high level of R-squared of 0.78, using the best regression model. These results not only highlight its real-world applicability but also played an important role in enhancing sustainable manufacturing practices. This innovative application offers a significant step towards sustainable and efficient manufacturing, addressing the challenges of high energy consumption and environmental impact in the industry.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/" target="_blank"></a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Asmae_ESCI_24.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2024" 
         data-type="journal"
         data-title="image-based human re-identification: which covariates are actually (the most) important?"
         data-authors="kailash hambarde, hugo proença"
         data-venue="elsevier image and vision computing"
         data-tags="human re-identification,biometric menagerie,covariates,visual surveillance,bias analysis"
         data-id="17">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-152.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Image-Based Human Re-identification: Which Covariates are Actually (the most) Important?</h3>
                <p class="text-gray-600 mb-2">Kailash Hambarde, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Image and Vision Computing</span> (2024)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Human Re-identification</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Menagerie</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Covariates</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visual Surveillance</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Bias Analysis</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.imavis.2024.104917" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Kailash_IVC_2024.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="17">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="17">
            <h2>Image-Based Human Re-identification: Which Covariates are Actually (the most) Important?</h2>
            <p class="authors">Kailash Hambarde, Hugo Proença</p>
            <p class="venue">Elsevier Image and Vision Computing (2024)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Human re-identification (re-ID) is nowadays among the most popular topics in computer vision, due to the increasing importance given to safety/security in modern societies. Being expected to sun in totally uncontrolled data acquisition settings (e.g., visual surveillance) automated re-ID not only depends on various factors that may occur in non-controlled data acquisition settings, but - most importantly - performance varies with respect to different subject features (e.g., gender, height, ethnicity, clothing, and action being performed), which may result in highly biased and undesirable automata. While many efforts have been putted in increase the robustness of identification to uncontrolled settings, a systematic assessment of the actual variations in performance with respect to each subject feature remains to be done. Accordingly, the contributions of this paper are threefold: 1) we report the correlation between the performance of three state-of-the-art re-ID models and different subject features; 2) we discuss the most concerning features and report valuable insights about the roles of the various features in re-ID performance, which can be used to develop more effective and unbiased re-ID systems; and 3) we leverage the concept of biometric menagerie, in order to identify the groups of individuals that typically fall into the most common menagerie families (e.g., goats, lambs, and wolves). Our findings not only contribute to a better understanding of the factors affecting re-ID performance, but also may offer practical guidance for researchers and practitioners concerned on human re-identification development.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.imavis.2024.104917" target="_blank">10.1016/j.imavis.2024.104917</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Kailash_IVC_2024.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2024" 
         data-type="journal"
         data-title="a novel and secured email classification and emotion detection using hybrid deep neural network"
         data-authors="parthiban krishnamoorthya, mithileysh sathiyanarayanan, hugo proença"
         data-venue="international journal of cognitive computing in engineering"
         data-tags="email classification,emotion detection,deep neural networks,cloud security,hybrid encryption"
         data-id="18">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-153.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">A novel and secured email classification and emotion detection using hybrid deep neural network</h3>
                <p class="text-gray-600 mb-2">Parthiban Krishnamoorthya, Mithileysh Sathiyanarayanan, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">International Journal of Cognitive Computing in Engineering</span> (2024)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Email Classification</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Emotion Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Deep Neural Networks</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Cloud Security</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Hybrid Encryption</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.ijcce.2024.01.002" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Krishnamoorthya_2024.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="18">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="18">
            <h2>A novel and secured email classification and emotion detection using hybrid deep neural network</h2>
            <p class="authors">Parthiban Krishnamoorthya, Mithileysh Sathiyanarayanan, Hugo Proença</p>
            <p class="venue">International Journal of Cognitive Computing in Engineering (2024)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Compared to other social media data, email data differs from it in various topic-specific ways, including extensive replies, formal language, significant length disparities, high levels of anomalies, and indirect linkages. In this paper, the creation of a potent and computationally effective classifier to categorize spam and ham email documents is proposed. To assess and validate spam texts, this paper employs a variety of data mining-based classification approaches. On the benchmark Enron dataset, which is open to the public, tests were run. The final 7 Enron datasets were created by combining the six different types of Enron datasets that we had acquired. We preprocess the dataset at an early stage to exclude any useless phrases. This method falls under several categories, including Logistic Regression (LR), Convolutional Neural Networks (CNN), Random Forests (RF), Recurrent Neural Networks (RNN), Long Short-Term Memory (LSTM), and suggested Deep Neural Networks (DNN). Using Bidirectional Long Short-Term Memory (BiLSTM), email documents may be screened for spam and labeled as such. In performance comparisons, DNN-BiLSTM outperforms other classifiers in terms of accuracy on all seven Enron datasets. In comparison to other machine learning classifiers, the findings demonstrate that DNN- BiLSTM and Convolutional Neural Networks can categorize spam with 96.39 % and 98.69 % accuracy, respectively. The report also covers the dangers of managing cloud data and the security problems that might occur. To safeguard data in the cloud while maintaining privacy, hybrid encryption is examined in this white paper. In the AES-Rabit hybrid encryption system, the symmetric session key exchange-based Rabit technique is combined with the benefits of the AES algorithm for faster data encryption.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.ijcce.2024.01.002" target="_blank">10.1016/j.ijcce.2024.01.002</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Krishnamoorthya_2024.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2024" 
         data-type="journal"
         data-title="how deep learning sees the world: a survey on adversarial attacks & defenses"
         data-authors="joana c. costa, tiago roxo, hugo proença, pedro inácio"
         data-venue="ieee access"
         data-tags="deep learning,adversarial attacks,adversarial defenses,neural networks,vision transformers"
         data-id="19">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-156.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">How Deep Learning Sees the World: A Survey on Adversarial Attacks & Defenses</h3>
                <p class="text-gray-600 mb-2">Joana C. Costa, Tiago Roxo, Hugo Proença, Pedro Inácio</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Access</span> (2024)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Deep Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Adversarial Attacks</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Adversarial Defenses</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Neural Networks</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Vision Transformers</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/ACCESS.2024.3395118" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/joana_survey_access_2024.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="19">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="19">
            <h2>How Deep Learning Sees the World: A Survey on Adversarial Attacks & Defenses</h2>
            <p class="authors">Joana C. Costa, Tiago Roxo, Hugo Proença, Pedro Inácio</p>
            <p class="venue">IEEE Access (2024)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Deep Learning is currently used to perform multiple tasks, such as object recognition, face recognition, and natural language processing. However, Deep Neural Networks (DNNs) are vulnerable to perturbations that alter the network prediction (adversarial examples), raising concerns regarding its usage in critical areas, such as self-driving vehicles, malware detection, and healthcare. This paper compiles the most recent adversarial attacks, grouped by the attacker capacity, and modern defenses clustered by protection strategies. We also present the new advances regarding Vision Transformers, summarize the datasets and metrics used in the context of adversarial settings, and compare the state-of-the-art results under different attacks, finishing with the proposal of possible directions for future works.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/ACCESS.2024.3395118" target="_blank">10.1109/ACCESS.2024.3395118</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/joana_survey_access_2024.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2023" 
         data-type="journal"
         data-title="periocular biometrics: a modality for unconstrained scenarios"
         data-authors="fernando alonso-fernandez, josef bigun, julian fierrez, naser damer, hugo proença, arun ross"
         data-venue="computer"
         data-tags="periocular biometrics,unconstrained recognition,ocular region,biometric identification,covid-19"
         data-id="20">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-147.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Periocular Biometrics: A Modality for Unconstrained Scenarios</h3>
                <p class="text-gray-600 mb-2">Fernando Alonso-Fernandez, Josef Bigun, Julian Fierrez, Naser Damer, Hugo Proença, Arun Ross</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Computer</span> (2023)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Unconstrained Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Ocular Region</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Identification</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">COVID-19</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/MC.2023.3298095" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Periocular_Computer_2023.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="20">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="20">
            <h2>Periocular Biometrics: A Modality for Unconstrained Scenarios</h2>
            <p class="authors">Fernando Alonso-Fernandez, Josef Bigun, Julian Fierrez, Naser Damer, Hugo Proença, Arun Ross</p>
            <p class="venue">Computer (2023)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Periocular refers to the externally visible region of the face that surrounds the eye socket. This feature-rich area can provide accurate identification in unconstrained or uncooperative scenarios, where the iris or face modalities may not offer sufficient biometric cues due to factors such as partial occlusion or high subject-to-camera distance. The COVID-19 pandemic has further highlighted its importance, as the ocular region remained the only visible facial area even in controlled settings due to the widespread use of masks. This paper discusses the state of the art in periocular biometrics, presenting an overall framework encompassing its most significant research aspects, which include: (a) ocular definition, acquisition, and detection; (b) identity recognition, including combination with other modalities and use of various spectra; and (c) ocular soft-biometric analysis. Finally, we conclude by addressing current challenges and proposing future directions.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/MC.2023.3298095" target="_blank">10.1109/MC.2023.3298095</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Periocular_Computer_2023.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2023" 
         data-type="journal"
         data-title="information retrieval: recent advances and beyond"
         data-authors="kailash hambarde, hugo proença"
         data-venue="ieee access"
         data-tags="information retrieval,survey,literature review,search models,information processing"
         data-id="21">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-148.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Information Retrieval: Recent Advances and Beyond</h3>
                <p class="text-gray-600 mb-2">Kailash Hambarde, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Access</span> (2023)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Information Retrieval</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Survey</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Literature Review</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Search Models</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Information Processing</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/ACCESS.2023.3295776" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Survey_Information_Retrieval_2023.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="21">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="21">
            <h2>Information Retrieval: Recent Advances and Beyond</h2>
            <p class="authors">Kailash Hambarde, Hugo Proença</p>
            <p class="venue">IEEE Access (2023)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>This paper provides an extensive and thorough overview of the models and techniques utilized in the first and second stages of the typical information retrieval processing chain. Our discussion encompasses the current state-of-the-art models, covering a wide range of methods and approaches in the field of information retrieval. We delve into the historical development of these models, analyze the key advancements and breakthroughs, and address the challenges and limitations faced by researchers and practitioners in the domain. By offering a comprehensive understanding of the field, this survey is a valuable resource for researchers, practitioners, and newcomers to the information retrieval domain, fostering knowledge growth, innovation, and the development of novel ideas and techniques.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/ACCESS.2023.3295776" target="_blank">10.1109/ACCESS.2023.3295776</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Survey_Information_Retrieval_2023.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2023" 
         data-type="conference"
         data-title="on exploring audio anomaly in speech"
         data-authors="tiago roxo, joana c. costa, pedro inácio, hugo proença"
         data-venue="proceedings of the 2023 ieee international workshop on information forensics and security (wifs 2023)"
         data-tags="audio anomaly,speech analysis,active speaker detection,anomaly detection,anomaly localization"
         data-id="22">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-150.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">On Exploring Audio Anomaly in Speech</h3>
                <p class="text-gray-600 mb-2">Tiago Roxo, Joana C. Costa, Pedro Inácio, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the 2023 IEEE International Workshop on Information Forensics and Security (WIFS 2023)</span> (2023)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Audio Anomaly</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Speech Analysis</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Active Speaker Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Anomaly Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Anomaly Localization</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/WIFS58808.2023.10374734" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Tiago_WIFS_23.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="22">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="22">
            <h2>On Exploring Audio Anomaly in Speech</h2>
            <p class="authors">Tiago Roxo, Joana C. Costa, Pedro Inácio, Hugo Proença</p>
            <p class="venue">Proceedings of the 2023 IEEE International Workshop on Information Forensics and Security (WIFS 2023) (2023)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Existing anomaly detection works mainly focus on abnormal activities in image and video settings, while assessing audio manipulation, namely the presence of anomalous audio in speech, has not yet been explored. To overcome this limitation, we propose a setup in the context of Active Speaker Detection (ASD) by defining a methodology to perceive audio anomaly, assessing the performance of anomaly models, and establishing setup variations. This way, we evaluate models performance in identifying the presence of anomalies (detection) and localizing the timeframe where they occur (localization). To complement anomaly detection, we propose Anomaly Score (AS), a metric to assess anomaly localization that balances precision and mis-localization. Given the sequential nature of audio, we explore the performance of a density-based approach for video anomaly (CPD) and recurrent models (LSTM and RNN) on detecting and localizing audio anomalies. The results show that: 1) anomaly inclusion in talking portions increases models resilience toward anomaly localization; 2) CPD is superior in anomaly detection, while recurrent models perform better in anomaly localization; 3) anomaly with distinctive audio benefits precise anomaly localization; and 4) using original ASD audio is overall the best approach, relative to other processing approaches. The setup and experiments of this work serve as a baseline for future works on speech anomaly detection.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/WIFS58808.2023.10374734" target="_blank">10.1109/WIFS58808.2023.10374734</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Tiago_WIFS_23.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2023" 
         data-type="journal"
         data-title="visual and textual explainability for a biometric verification system based on piecewise facial attribute analysis"
         data-authors="lucia cascone, chiara pero, hugo proença"
         data-venue="elsevier image and vision computing"
         data-tags="explainable ai,biometric verification,facial attributes,machine learning,image-text mapping"
         data-id="23">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-142.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Visual and Textual Explainability for a Biometric Verification System based on Piecewise Facial Attribute Analysis</h3>
                <p class="text-gray-600 mb-2">Lucia Cascone, Chiara Pero, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Image and Vision Computing</span> (2023)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Explainable AI</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Verification</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Facial Attributes</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Machine Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image-Text Mapping</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.imavis.2023.104645" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/IVC_Lucia_Chiara.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="23">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="23">
            <h2>Visual and Textual Explainability for a Biometric Verification System based on Piecewise Facial Attribute Analysis</h2>
            <p class="authors">Lucia Cascone, Chiara Pero, Hugo Proença</p>
            <p class="venue">Elsevier Image and Vision Computing (2023)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The decisions behind the mechanics of a biometric verification system based on Machine Learning (ML) are difficult to comprehend. Although there is now well-established research in various fields of application, such as health or justice, the use of ML-based methods is accompanied by a lack of confidence that results in their limited use. The explainability of a ML system and the comprehension of what lies behind its prediction is one of the numerous characteristics that define 'trust' in these systems. Over the years, face-based biometric authentication has been the subject of extensive research in both academia and industry. However, existing biometric authentication systems still have problems regarding accuracy, robustness and, explainability. Still lacking in the literature is a comprehensive examination of the use of post-hoc explainability techniques for such systems. Cognitive neuroscience has always been interested in the method by which people perceive faces; local elements such as the nose, eyes, and mouth are critical to the perception and recognition of a face. In this work, starting from this assumption, we propose a framework of visual and textual explainability based on the parts of a face by analyzing them with respect to the facial attributes reported in the CelebA dataset. The primary objective is to be able to explain why two pictures of different subjects are distinct. This is done by sinthesizing pairs of images that illustrate how dissimilar the various parts of the face under investigation are and incisive and direct textual explanations of the distinguishing features are generated. A further study analyzes an interpretable mapping between the semantic space of the text and the space of the image.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.imavis.2023.104645" target="_blank">10.1016/j.imavis.2023.104645</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/IVC_Lucia_Chiara.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2023" 
         data-type="journal"
         data-title="syper: synthetic periocular data for quantized light-weight recognition in the nir and visible domains"
         data-authors="jan niklas kolf, jurek elliesen, fadi boutros, hugo proença, naser damer"
         data-venue="elsevier image and vision computing"
         data-tags="periocular recognition,synthetic data,model quantization,near-infrared,edge computing"
         data-id="24">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-145.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">SyPer: Synthetic Periocular Data for Quantized light-weight Recognition in the NIR and Visible Domains</h3>
                <p class="text-gray-600 mb-2">Jan Niklas Kolf, Jurek Elliesen, Fadi Boutros, Hugo Proença, Naser Damer</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Image and Vision Computing</span> (2023)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Synthetic Data</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Model Quantization</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Near-Infrared</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Edge Computing</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.imavis.2023.104692" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Naser_IVC_2023.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="24">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="24">
            <h2>SyPer: Synthetic Periocular Data for Quantized light-weight Recognition in the NIR and Visible Domains</h2>
            <p class="authors">Jan Niklas Kolf, Jurek Elliesen, Fadi Boutros, Hugo Proença, Naser Damer</p>
            <p class="venue">Elsevier Image and Vision Computing (2023)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Deep-learning based periocular recognition systems typically use overparameterized deep neural networks associated with high computational costs and memory requirements. This is especially problematic for mobile and embedded devices in shared resource environments. To perform model quantization for lightweight periocular recognition in a privacy-aware manner, we propose and release SyPer, a synthetic dataset and generation model of periocular images. To enable this, we propose to perform the knowledge transfer in the quantization process on the embedding level and thus not identity-labeled data. This does not only allow the use of synthetic data for quantization, but it also successfully allows to perform the quantization on different domains to additionally boost the performance in new domains. In a variety of experiments on a diverse set of model backbones, we demonstrate the ability to build compact and accurate models through an embedding-level knowledge transfer using synthetic data. We also demonstrate very successfully the use of embedding-level knowledge transfer for near-infrared quantized models towards accurate and efficient periocular recognition on near-infrared images. The SyPer dataset, together with the evaluation protocol, the training code, and model checkpoints are made publicly available at https://github.com/jankolf/SyPe.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.imavis.2023.104692" target="_blank">10.1016/j.imavis.2023.104692</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Naser_IVC_2023.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2023" 
         data-type="journal"
         data-title="adaptive spatial transformation networks for periocular recognition"
         data-authors="diana borza, ehsan yaghoubi, simone frintrop, hugo proença"
         data-venue="mdpi sensors"
         data-tags="periocular recognition,deep learning,spatial transformation,covid-19,biometrics"
         data-id="25">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-143.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Adaptive Spatial Transformation Networks for Periocular Recognition</h3>
                <p class="text-gray-600 mb-2">Diana Borza, Ehsan Yaghoubi, Simone Frintrop, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">MDPI Sensors</span> (2023)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Deep Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Spatial Transformation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">COVID-19</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.3390/s23052456" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Diana_Ehsan_Sensors_2023.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="25">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="25">
            <h2>Adaptive Spatial Transformation Networks for Periocular Recognition</h2>
            <p class="authors">Diana Borza, Ehsan Yaghoubi, Simone Frintrop, Hugo Proença</p>
            <p class="venue">MDPI Sensors (2023)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Periocular recognition has emerged as a particularly valuable biometric identification method in challenging scenarios, where the acquired data is of poor quality. In this context, partially occluded faces (e.g., due to COVID-19 masks or in VR-applications where the face is not visible due to a head-mounted display) are particularly concerning, where facial recognition cannot be applicable. This work presents a periocular recognition framework based on deep learning architectures, that autonomously localize and analyze the most important areas in the periocular region, for recognition purposes. The main idea is to derive several parallel local branches from a neural network architecture, which in a semi-supervised manner learns the most discriminative areas in the feature map and solves the identification problem solely upon the corresponding cues. Here, each local branch learns a transformation matrix that allows for basic geometrical transformations (cropping and scaling), which is used to select a region of interest in the feature map, further analysed by a set of shared convolutional layers. Finally, the information extracted by the local branches and the main global branch is fused together for recognition. The experiments carried out on the challenging UBIRIS-v2 benchmark show that by integrating the proposed framework with various ResNet architectures, we consistently obtain an improvement in mAP of more than 4% over the 'vanilla' architecture. In addition, extensive ablation studies were performed to better understand the behavior of the network and how the spatial transformation and the local branches influence the overall performance of the model. The proposed method can be easily adapted to other computer vision problems, which is also regarded as one of its strengths.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.3390/s23052456" target="_blank">10.3390/s23052456</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Diana_Ehsan_Sensors_2023.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2023" 
         data-type="conference"
         data-title="wsrr: weighted rank-relevance sampling for dense text retrieval"
         data-authors="kailash hambarde"
         data-venue="proceedings of the 7th international conference on information and communication technology for intelligent systems"
         data-tags="text retrieval,negative sampling,contrastive learning,information retrieval,natural language processing"
         data-id="26">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-144.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">WSRR: Weighted Rank-Relevance Sampling for Dense Text Retrieval</h3>
                <p class="text-gray-600 mb-2">Kailash Hambarde</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the 7th International Conference on Information and Communication Technology for Intelligent Systems</span> (2023)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Text Retrieval</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Negative Sampling</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Contrastive Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Information Retrieval</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Natural Language Processing</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Hambarde_ICTIS_2023.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="26">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="26">
            <h2>WSRR: Weighted Rank-Relevance Sampling for Dense Text Retrieval</h2>
            <p class="authors">Kailash Hambarde</p>
            <p class="venue">Proceedings of the 7th International Conference on Information and Communication Technology for Intelligent Systems (2023)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>As in many other domains based in the contrastive learning paradigm, negative sampling is seen as a particular sensitive problem for appropriately training dense text retrieval models. For most cases, it is accepted that the existing techniques often suffer from the problem of uninformative or false negatives, which reduces the computational effectiveness of the learning phase and even reduces the probability of convergence of the whole process. Upon these limitations, in this paper we present a new approach for dense text retrieval (termed WRRS: Weighted Rank-Relevance Sampling) that addresses the limitations of current negative sampling strategies. WRRS assigns probabilities to negative samples based on their relevance scores and ranks, which consistently leads to improvements in retrieval performance. Under this perspective, WRRS offers a solution to uninformative or false negatives in traditional negative sampling techniques, which is seen as a valuable contribution to the field. Our empirical evaluation was carried out against the AR2 baseline on two well known datasets (NQ and MS Doc), pointing for consistent improvements over the SOTA performance.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/" target="_blank"></a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Hambarde_ICTIS_2023.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2023" 
         data-type="journal"
         data-title="special issue on the conference on graphics, patterns and images - sibgrapi 2021"
         data-authors="hugo proença, david menotti, afonso paiva, gladimir baranoski"
         data-venue="elsevier pattern recognition letters"
         data-tags="special issue,graphics,pattern recognition,image processing,computer vision"
         data-id="27">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-141.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Special issue on the Conference on Graphics, Patterns and Images - SIBGRAPI 2021</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, David Menotti, Afonso Paiva, Gladimir Baranoski</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Pattern Recognition Letters</span> (2023)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Special Issue</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Graphics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Pattern Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Processing</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Computer Vision</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.patrec.2022.12.002" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/PRL_SIBGRAPI21.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="27">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="27">
            <h2>Special issue on the Conference on Graphics, Patterns and Images - SIBGRAPI 2021</h2>
            <p class="authors">Hugo Proença, David Menotti, Afonso Paiva, Gladimir Baranoski</p>
            <p class="venue">Elsevier Pattern Recognition Letters (2023)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p></p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.patrec.2022.12.002" target="_blank">10.1016/j.patrec.2022.12.002</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/PRL_SIBGRAPI21.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2023" 
         data-type="journal"
         data-title="atom: self-supervised human action recognition using atomic motion representation learning"
         data-authors="bruno degardin, vasco lopes, hugo proença"
         data-venue="elsevier image and vision computing"
         data-tags="self-supervised learning,human action recognition,skeleton-based data,motion representation,transfer learning"
         data-id="28">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-146.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">ATOM: Self-supervised Human Action Recognition using Atomic Motion Representation Learning</h3>
                <p class="text-gray-600 mb-2">Bruno Degardin, Vasco Lopes, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Image and Vision Computing</span> (2023)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Self-supervised Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Human Action Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Skeleton-based Data</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Motion Representation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Transfer Learning</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.imavis.2023.104750" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Degardin_IVC_ATOM_2023.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="28">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="28">
            <h2>ATOM: Self-supervised Human Action Recognition using Atomic Motion Representation Learning</h2>
            <p class="authors">Bruno Degardin, Vasco Lopes, Hugo Proença</p>
            <p class="venue">Elsevier Image and Vision Computing (2023)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Self-supervised learning (SSL) is a promising method for gaining perception and common sense from unlabelled data. Existing approaches to analyzing human body skeletons address the problem similar to SSL models for image and video understanding, but pixel data is far more challenging than coordinates. This paper presents ATOM, an SSL model designed for skeleton-based data analysis. Unlike video-based SSL approaches, ATOM leverages atomic movements within skeleton actions to achieve a more fine-grained representation. The proposed architecture predicts the action order at the frame level, leading to improved perceptions and representations of each action. ATOM outperforms state-of-the-art approaches in two well-known datasets (NTU RGB+D and NTU-120 RGB+D), and its weight transferability enables performance improvements on supervised and semi-supervised tasks, up to 4.4% (3.3% p.p.) and 14.1% (6.3% p.p.), respectively, in Top-1 Accuracy.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.imavis.2023.104750" target="_blank">10.1016/j.imavis.2023.104750</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Degardin_IVC_ATOM_2023.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2022" 
         data-type="journal"
         data-title="deepgabor: a learning-based framework to augment iriscodes permanence"
         data-authors="hugo proença"
         data-venue="ieee transactions on information forensics and security"
         data-tags="iris recognition,deep learning,biometrics,gabor filters,iriscode"
         data-id="29">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/tifs.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">DeepGabor: A Learning-Based Framework to Augment IrisCodes Permanence</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Information Forensics and Security</span> (2022)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Deep Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Gabor Filters</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">IrisCode</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIFS.2022.3214098" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/tifs_deep_gabor_22.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="29">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="29">
            <h2>DeepGabor: A Learning-Based Framework to Augment IrisCodes Permanence</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">IEEE Transactions on Information Forensics and Security (2022)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>For over three decades, the Gabor-based IrisCode approach has been acknowledged as the gold standard for iris recognition, mainly due to the high entropy and binary nature sgn() of its signatures. This method is highly effective in large scale environments (e.g., national ID applications), where millions of comparisons per second are required. However, it is known that non-linear deformations in the iris texture, with fibers vanishing/appearing in response to pupil dilation/contraction, often flip the signature coefficients, being the main cause for the increase of false rejections. This paper addresses this problem, describing a customised Deep Learning (DL) framework that: 1) virtually emulates the IrisCode feature encoding phase; while also 2) detects the deformations in the iris texture that may lead to bit flipping, and autonomously adapts the filter configurations for such cases. The proposed DL architecture seamlessly integrates the Gabor kernels that extract the IrisCode and a multi-scale texture analyzer, from where the biometric signatures yield. In this sense, it can be seen as an adaptive encoder that is fully compatible to the IrisCode approach, while increasing the permanence of the signatures. The experiments were conducted in two well known datasets (CASIA-Iris-Lamp and CASIA-Iris-Thousand) and showed a notorious decrease of the mean/standard deviation values of the genuines distribution, at expenses of only a marginal deterioration in the impostors scores. The resulting decision environments consistently reduce the levels of false rejections with respect to the baseline for most operating levels (e.g., over 50% at 1e−3 FAR values). The source code of the DeepGabor encoder is available at: https://github.com/hugomcp/DeepGabor.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIFS.2022.3214098" target="_blank">10.1109/TIFS.2022.3214098</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/tifs_deep_gabor_22.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2022" 
         data-type="journal"
         data-title="the uu-net: reversible face de-identification for visual surveillance video footage"
         data-authors="hugo proença"
         data-venue="ieee transactions on circuits and systems for video technology"
         data-tags="face de-identification,visual surveillance,privacy protection,generative adversarial networks,reversible anonymization"
         data-id="30">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/tcsvt.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">The UU-Net: Reversible Face De-Identification for Visual Surveillance Video Footage</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Circuits and Systems for Video Technology</span> (2022)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Face De-Identification</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visual Surveillance</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Privacy Protection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Generative Adversarial Networks</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Reversible Anonymization</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TCSVT.2021.3066054" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/UUNet.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="30">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="30">
            <h2>The UU-Net: Reversible Face De-Identification for Visual Surveillance Video Footage</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">IEEE Transactions on Circuits and Systems for Video Technology (2022)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>We propose a reversible face de-identification method for video surveillance data, where landmark-based techniques cannot be reliably used. Our solution generates a photorealistic de-identified stream that meets the data protection regulations and can be publicly released under minimal privacy concerns. Notably, such stream still encapsulates the information required to later reconstruct the original scene, which is useful for scenarios, such as crime investigation, where subjects identification is of most importance. Our learning process jointly optimizes two main components: 1) a public module, that receives the raw data and generates the de-identified stream; and 2) a private module, designed for security authorities, that receives the public stream and reconstructs the original data, disclosing the actual IDs of the subjects in a scene. The proposed solution is landmarks-free and uses a conditional generative adversarial network to obtain synthetic faces that preserve pose, lighting, background information and even facial expressions. Also, we keep full control over the set of soft facial attributes to be preserved/changed between the raw/de-identified data, which extends the range of applications for the proposed solution. Our experiments were conducted in three visual surveillance datasets (BIODI, MARS and P-DESTRE) plus one video face data set (YouTube Faces), showing highly encouraging results. The source code is available at https://github.com/hugomcp/uu-net.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TCSVT.2021.3066054" target="_blank">10.1109/TCSVT.2021.3066054</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/UUNet.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2022" 
         data-type="conference"
         data-title="generative adversarial graph convolutional networks for human action synthesis"
         data-authors="bruno degardin, joão c. neves, vasco lopes, joão brito, ehsan yaghoubi, hugo proença"
         data-venue="proceedings of the 2022 ieee/cvf winter conference on applications of computer vision (wacv)"
         data-tags="human action synthesis,graph convolutional networks,generative adversarial networks,action recognition,skeleton-based animation"
         data-id="31">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/cvf.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Generative Adversarial Graph Convolutional Networks for Human Action Synthesis</h3>
                <p class="text-gray-600 mb-2">Bruno Degardin, João C. Neves, Vasco Lopes, João Brito, Ehsan Yaghoubi, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</span> (2022)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Human Action Synthesis</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Graph Convolutional Networks</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Generative Adversarial Networks</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Action Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Skeleton-based Animation</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/WACV51458.2022.00281" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/wacv21.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    <a href="https://github.com/DegardinBruno/Kinetic-GAN" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg class="h-5 w-5 mr-1" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true">
                            <path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z" clip-rule="evenodd" />
                        </svg>
                        Code
                    </a>
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="31">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="31">
            <h2>Generative Adversarial Graph Convolutional Networks for Human Action Synthesis</h2>
            <p class="authors">Bruno Degardin, João C. Neves, Vasco Lopes, João Brito, Ehsan Yaghoubi, Hugo Proença</p>
            <p class="venue">Proceedings of the 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) (2022)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Synthesising the spatial and temporal dynamics of the human body skeleton remains a challenging task, not only in terms of the quality of the generated shapes, but also of their diversity, particularly to synthesise realistic body movements of a specific action (action conditioning). In this paper, we propose Kinetic-GAN, a novel architecture that leverages the benefits of Generative Adversarial Networks and Graph Convolutional Networks to synthesise the kinetics of the human body. The proposed adversarial architecture can condition up to 120 different actions over local and global body movements while improving sample quality and diversity through latent space disentanglement and stochastic variations. Our experiments were carried out in three well-known datasets, where Kinetic-GAN notably surpasses the state-of-the-art methods in terms of distribution quality metrics while having the ability to synthesise more than one order of magnitude regarding the number of different actions. Our code and models are publicly available at https://github.com/DegardinBruno/Kinetic-GAN.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/WACV51458.2022.00281" target="_blank">10.1109/WACV51458.2022.00281</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/wacv21.pdf" target="_blank">PDF</a>
                
                
                <a href="https://github.com/DegardinBruno/Kinetic-GAN" target="_blank">Code</a>
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2022" 
         data-type="journal"
         data-title="regina - reasoning graph convolutional networks in human action recognition"
         data-authors="bruno degardin, vasco lopes, hugo proença"
         data-venue="ieee transactions on information forensics and security"
         data-tags="human action recognition,graph convolutional networks,skeleton-based recognition,reasoning,feature engineering"
         data-id="32">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/tifs.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">REGINA - Reasoning Graph Convolutional Networks in Human Action Recognition</h3>
                <p class="text-gray-600 mb-2">Bruno Degardin, Vasco Lopes, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Information Forensics and Security</span> (2022)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Human Action Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Graph Convolutional Networks</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Skeleton-based Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Reasoning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Feature Engineering</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIFS.2021.3130437" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/degardin_tifs_2021.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    <a href="https://github.com/DegardinBruno" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg class="h-5 w-5 mr-1" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true">
                            <path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z" clip-rule="evenodd" />
                        </svg>
                        Code
                    </a>
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="32">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="32">
            <h2>REGINA - Reasoning Graph Convolutional Networks in Human Action Recognition</h2>
            <p class="authors">Bruno Degardin, Vasco Lopes, Hugo Proença</p>
            <p class="venue">IEEE Transactions on Information Forensics and Security (2022)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>It is known that the kinematics of the human body skeleton reveals valuable information in action recognition. Recently, modelling skeletons as spatio-temporal graphs with Graph Convolutional Networks (GCNs) has been reported to solidly advance the state-of-the-art performance. However, GCN-based approaches exclusively learn from raw skeleton data, and are expected to extract the inherent structural information on their own. This paper describes REGINA, introducing a novel way to REasoning Graph convolutional networks IN Human Action recognition. The rationale is to provide to the GCNs additional knowledge about the skeleton data, obtained by hand-crafted features, in order to facilitate the learning process, while guaranteeing that it remains fully trainable in an end-to-end manner. The challenge is to capture complementary information over the dynamics between consecutive frames, which is the key information extracted by state-of-the-art GCN techniques. Moreover, the proposed strategy can be easily integrated in the existing GCN-based methods, which we also regard positively. Our experiments were carried out in well known action recognition datasets and enabled to conclude that REGINA contributes for solid improvements in performance when incorporated to other GCN-based approaches, without any other adjustment regarding the original method. For reproducibility, the REGINA code and all the experiments carried out will be publicly available at https://github.com/DegardinBruno.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIFS.2021.3130437" target="_blank">10.1109/TIFS.2021.3130437</a></p>
            
            <div class="links">
                
                <a href="http://di.ubi.pt/~hugomcp/doc/degardin_tifs_2021.pdf" target="_blank">PDF</a>
                
                
                <a href="https://github.com/DegardinBruno" target="_blank">Code</a>
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2022" 
         data-type="journal"
         data-title="real-time image detection for edge device: a peach fruit detection application"
         data-authors="eduardo assunção, pedro d. gaspar, khadijeh alibabaei, maria p. simões, hugo proença, vasco n. g. j. soares, joão m. l. p. caldeira"
         data-venue="mdpi future internet"
         data-tags="edge computing,fruit detection,precision agriculture,tensor processing unit,real-time detection"
         data-id="33">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-140.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Real-time Image Detection for Edge Device: A Peach Fruit Detection Application</h3>
                <p class="text-gray-600 mb-2">Eduardo Assunção, Pedro D. Gaspar, Khadijeh Alibabaei, Maria P. Simões, Hugo Proença, Vasco N. G. J. Soares, João M. L. P. Caldeira</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">MDPI Future Internet</span> (2022)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Edge Computing</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Fruit Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Precision Agriculture</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Tensor Processing Unit</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Real-time Detection</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.3390/fi14110323" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/mdpi_future_internet.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="33">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="33">
            <h2>Real-time Image Detection for Edge Device: A Peach Fruit Detection Application</h2>
            <p class="authors">Eduardo Assunção, Pedro D. Gaspar, Khadijeh Alibabaei, Maria P. Simões, Hugo Proença, Vasco N. G. J. Soares, João M. L. P. Caldeira</p>
            <p class="venue">MDPI Future Internet (2022)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Within the scope of precision agriculture, many applications have been developed to support decision-making and yield enhancement. Fruit detection has attracted considerable attention from researchers, and can be used offline. In contrast, some applications, such as robot vision in orchards, require computer vision models to run on edge devices while performing inference at high speed. In this area, most modern applications use an integrated graphics processing unit (GPU). In this work, we propose to use a Tensor Processing Unit (TPU) accelerator with the Raspberry Pi target device and the state-of-the-art, lightweight, and hardware-aware MobileDet detector model. Our contribution is to extend the possibilities of using accelerators (TPU) for edge devices in precision agriculture. The proposed method was evaluated in a novel dataset of peaches with three cultivars, which will be made available for further studies. The model achieved an average precision (AP) of 88.2% and a performance of 19.84 frame per second (FPS) at an image size of 640 × 480. The results obtained show that the TPU accelerator can be an excellent alternative for processing on the edge in precision agriculture.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.3390/fi14110323" target="_blank">10.3390/fi14110323</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/mdpi_future_internet.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2022" 
         data-type="journal"
         data-title="deepgabor: a learning-based framework to augment iriscodes permanence"
         data-authors="hugo proença"
         data-venue="ieee transactions on information forensics and security"
         data-tags="iris recognition,iriscode,deep learning,gabor filters,biometrics"
         data-id="34">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-139.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">DeepGabor: A Learning-Based Framework to Augment IrisCodes Permanence</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Information Forensics and Security</span> (2022)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">IrisCode</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Deep Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Gabor Filters</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIFS.2022.3214098" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/tifs_deep_gabor_22.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="34">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="34">
            <h2>DeepGabor: A Learning-Based Framework to Augment IrisCodes Permanence</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">IEEE Transactions on Information Forensics and Security (2022)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>For over three decades, the Gabor-based IrisCode approach has been acknowledged as the gold standard for iris recognition, mainly due to the high entropy and binary nature sgn() of its signatures. This method is highly effective in large scale environments (e.g., national ID applications), where millions of comparisons per second are required. However, it is known that non-linear deformations in the iris texture, with fibers vanishing/appearing in response to pupil dilation/contraction, often flip the signature coefficients, being the main cause for the increase of false rejections. This paper addresses this problem, describing a customised Deep Learning (DL) framework that: 1) virtually emulates the IrisCode feature encoding phase; while also 2) detects the deformations in the iris texture that may lead to bit flipping, and autonomously adapts the filter configurations for such cases. The proposed DL architecture seamlessly integrates the Gabor kernels that extract the IrisCode and a multi-scale texture analyzer, from where the biometric signatures yield. In this sense, it can be seen as an adaptive encoder that is fully compatible to the IrisCode approach, while increasing the permanence of the signatures. The experiments were conducted in two well known datasets (CASIA-Iris-Lamp and CASIA-Iris-Thousand) and showed a notorious decrease of the mean/standard deviation values of the genuines distribution, at expenses of only a marginal deterioration in the impostors scores. The resulting decision environments consistently reduce the levels of false rejections with respect to the baseline for most operating levels (e.g., over 50% at 1e−3 FAR values). The source code of the DeepGabor encoder is available at: https://github.com/hugomcp/DeepGabor.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIFS.2022.3214098" target="_blank">10.1109/TIFS.2022.3214098</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/tifs_deep_gabor_22.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2022" 
         data-type="conference"
         data-title="super-resolution using stochastic differential equations and potential applications on face recognition"
         data-authors="marcelo santos, rayson laroca, rafael o. ribeiro, joão c. neves, hugo proença, david menotti"
         data-venue="proceedings of the 35th conference on graphics, patterns and images - sibgrapi 2022"
         data-tags="super-resolution,stochastic differential equations,diffusion models,face recognition,image enhancement"
         data-id="35">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-137.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Super-resolution Using Stochastic Differential Equations and Potential Applications on Face Recognition</h3>
                <p class="text-gray-600 mb-2">Marcelo Santos, Rayson Laroca, Rafael O. Ribeiro, João C. Neves, Hugo Proença, David Menotti</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the 35th Conference on Graphics, Patterns and Images - SIBGRAPI 2022</span> (2022)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Super-resolution</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Stochastic Differential Equations</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Diffusion Models</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Face Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Enhancement</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/SIBGRAPI.2022.000xx" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/sibgrapi_22.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="35">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="35">
            <h2>Super-resolution Using Stochastic Differential Equations and Potential Applications on Face Recognition</h2>
            <p class="authors">Marcelo Santos, Rayson Laroca, Rafael O. Ribeiro, João C. Neves, Hugo Proença, David Menotti</p>
            <p class="venue">Proceedings of the 35th Conference on Graphics, Patterns and Images - SIBGRAPI 2022 (2022)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Diffusion models have proven effective for various applications such as images, audio and graphs generation. Other important applications are image super-resolution and the solution of inverse problems. More recently, some works have used stochastic differential equations (SDEs) to generalize diffusion models to continuous time. In this work, we introduce SDE to generate super-resolution face images. To the best of our knowledge, this is the first time SDEs have been used for such an application. The proposed method provides promising peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM) and consistency than existing super-resolution methods based on diffusion models. We also demonstrated the potential applications of this method for the face recognition task. For this purpose, a generic facial feature extractor is used to compare the super-resolution images with the ground truth and superior results were obtained compared with other methods.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/SIBGRAPI.2022.000xx" target="_blank">10.1109/SIBGRAPI.2022.000xx</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/sibgrapi_22.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2022" 
         data-type="book chapter"
         data-title="gan fingerprints in face image synthesis"
         data-authors="joão c. neves, ruben tolosana, ruben vera-rodriguez, vasco lopes, hugo proença, julian fierrez"
         data-venue="springer-verlag book series, lecture notes on electrical engineering"
         data-tags="gan fingerprints,face image synthesis,fake detection"
         data-id="36">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-130.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">GAN Fingerprints in Face Image Synthesis</h3>
                <p class="text-gray-600 mb-2">João C. Neves, Ruben Tolosana, Ruben Vera-Rodriguez, Vasco Lopes, Hugo Proença, Julian Fierrez</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Springer-Verlag book series, Lecture Notes on Electrical Engineering</span> (2022)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge book chapter">Book chapter</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">GAN Fingerprints</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Face Image Synthesis</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Fake Detection</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/JSTSP.2020.3007250" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/paper-130.png" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="36">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="36">
            <h2>GAN Fingerprints in Face Image Synthesis</h2>
            <p class="authors">João C. Neves, Ruben Tolosana, Ruben Vera-Rodriguez, Vasco Lopes, Hugo Proença, Julian Fierrez</p>
            <p class="venue">Springer-Verlag book series, Lecture Notes on Electrical Engineering (2022)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The availability of large-scale facial databases, together with the remarkable progresses of deep learning technologies, in particular Generative Adversarial Networks (GANs), have led to the generation of extremely realistic fake facial content, raising obvious concerns about the potential for misuse. Such concerns have fostered the research on manipulation detection methods that, contrary to humans, have already achieved astonishing results in various scenarios. This chapter is focused on the analysis of GAN fingerprints in face image synthesis. In particular, it covers an in-depth literature analysis of state-of-the-art detection approaches for the entire face synthesis manipulation. It also describes a recent approach to spoof fake detectors based on a GAN-fingerprint Removal autoencoder (GANprintR). A thorough experimental framework is included in the chapter, highlighting (i) the potential of GANprintR to spoof fake detectors, and (ii) the poor generalisation capability of current fake detectors.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/JSTSP.2020.3007250" target="_blank">10.1109/JSTSP.2020.3007250</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/paper-130.png" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2022" 
         data-type="conference"
         data-title="generative adversarial graph convolutional networks for human action synthesis"
         data-authors="bruno degardin, joão c. neves, vasco lopes, joão brito, ehsan yaghoubi, hugo proença"
         data-venue="proceedings of the 2022 ieee/cvf winter conference on applications of computer vision - wacv 2022"
         data-tags="human action synthesis,generative adversarial networks,graph convolutional networks"
         data-id="37">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/cvf.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Generative Adversarial Graph Convolutional Networks for Human Action Synthesis</h3>
                <p class="text-gray-600 mb-2">Bruno Degardin, João C. Neves, Vasco Lopes, João Brito, Ehsan Yaghoubi, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the 2022 IEEE/CVF Winter Conference on Applications of Computer Vision - WACV 2022</span> (2022)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Human Action Synthesis</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Generative Adversarial Networks</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Graph Convolutional Networks</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/WACV51458.2022.00281" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/wacv21.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="37">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="37">
            <h2>Generative Adversarial Graph Convolutional Networks for Human Action Synthesis</h2>
            <p class="authors">Bruno Degardin, João C. Neves, Vasco Lopes, João Brito, Ehsan Yaghoubi, Hugo Proença</p>
            <p class="venue">Proceedings of the 2022 IEEE/CVF Winter Conference on Applications of Computer Vision - WACV 2022 (2022)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Synthesising the spatial and temporal dynamics of the human body skeleton remains a challenging task, not only in terms of the quality of the generated shapes, but also of their diversity, particularly to synthesise realistic body movements of a specific action (action conditioning). In this paper, we propose Kinetic-GAN, a novel architecture that leverages the benefits of Generative Adversarial Networks and Graph Convolutional Networks to synthesise the kinetics of the human body. The proposed adversarial architecture can condition up to 120 different actions over local and global body movements while improving sample quality and diversity through latent space disentanglement and stochastic variations. Our experiments were carried out in three well-known datasets, where Kinetic-GAN notably surpasses the state-of-the-art methods in terms of distribution quality metrics while having the ability to synthesise more than one order of magnitude regarding the number of different actions.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/WACV51458.2022.00281" target="_blank">10.1109/WACV51458.2022.00281</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/wacv21.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2022" 
         data-type="journal"
         data-title="peaches detection using deep learning technique – a contribution to yield estimation, resources management and circular economy"
         data-authors="eduardo assunção, pedro d. gaspar, ricardo mesquita, maria paula simões, antónio ramos, hugo proença, pedro inácio"
         data-venue="mdpi climate"
         data-tags="peaches detection,deep learning,cnn,yield estimation"
         data-id="38">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-132.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Peaches detection using Deep Learning technique – A contribution to yield estimation, resources management and circular economy</h3>
                <p class="text-gray-600 mb-2">Eduardo Assunção, Pedro D. Gaspar, Ricardo Mesquita, Maria Paula Simões, António Ramos, Hugo Proença, Pedro Inácio</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">MDPI Climate</span> (2022)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Peaches Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Deep Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">CNN</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Yield Estimation</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.3390/cli1010000" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/climate.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="38">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="38">
            <h2>Peaches detection using Deep Learning technique – A contribution to yield estimation, resources management and circular economy</h2>
            <p class="authors">Eduardo Assunção, Pedro D. Gaspar, Ricardo Mesquita, Maria Paula Simões, António Ramos, Hugo Proença, Pedro Inácio</p>
            <p class="venue">MDPI Climate (2022)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The fruit detection is crucial for yield estimation and fruit picking system performance. Many state-of-art methods for fruit detection use convolutional neural network (CNN). This paper presents the results for peaches detection applying the framework Faster R-CNN in images captured in an outdoor orchard. Although this method has been used in other studies to detect fruits, there is no research works with peaches. Since the fruit colors, it sizes, it shape, tree branches, fruit bunches and the distribution in the tree are particular, the development of the fruit detection procedure is specific. The results show a large potential for using this method to detect this type of fruit. A detection accuracy of 0.90 using the metric average precision (AP) was achieved for the fruit detection. Precision agriculture applications such Deep Neural Networks (DNN) as proposed in this paper can help to mitigate climate change due to horticultural activities by accurate products prediction leading to improved resources management such as irrigation water, nutrients, herbicides, pesticides, as well as helping reducing food loss and waste by improved agricultural activities scheduling.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.3390/cli1010000" target="_blank">10.3390/cli1010000</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/climate.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2022" 
         data-type="journal"
         data-title="the uu-net: reversible face de-identification for visual surveillance video footage"
         data-authors="hugo proença"
         data-venue="ieee transactions on circuits and systems for video technology"
         data-tags="face de-identification,visual surveillance,privacy,reversible de-identification"
         data-id="39">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/tcsvt.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">The UU-Net: Reversible Face De-Identification for Visual Surveillance Video Footage</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Circuits and Systems for Video Technology</span> (2022)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Face De-Identification</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visual Surveillance</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Privacy</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Reversible De-identification</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TCSVT.2021.3066054" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/UU-Net.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="39">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="39">
            <h2>The UU-Net: Reversible Face De-Identification for Visual Surveillance Video Footage</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">IEEE Transactions on Circuits and Systems for Video Technology (2022)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>We propose a reversible face de-identification method for video surveillance data, where landmark-based techniques cannot be reliably used. Our solution generates a photorealistic de-identified stream that meets the data protection regulations and can be publicly released under minimal privacy concerns. Notably, such stream still encapsulates the information required to later reconstruct the original scene, which is useful for scenarios, such as crime investigation, where subjects identification is of most importance. Our learning process jointly optimizes two main components: 1) a public module, that receives the raw data and generates the de-identified stream; and 2) a private module, designed for security authorities, that receives the public stream and reconstructs the original data, disclosing the actual IDs of the subjects in a scene. The proposed solution is landmarks-free and uses a conditional generative adversarial network to obtain synthetic faces that preserve pose, lighting, background information and even facial expressions. Also, we keep full control over the set of soft facial attributes to be preserved/changed between the raw/de-identified data, which extends the range of applications for the proposed solution. Our experiments were conducted in three visual surveillance datasets (BIODI, MARS and P-DESTRE) plus one video face data set (YouTube Faces), showing highly encouraging results.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TCSVT.2021.3066054" target="_blank">10.1109/TCSVT.2021.3066054</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/UU-Net.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2022" 
         data-type="journal"
         data-title="yinyang-net: complementing face and body information for wild gender recognition"
         data-authors="tiago roxo, hugo proença"
         data-venue="ieee access"
         data-tags="gender recognition,soft biometrics,face and body information,wild conditions"
         data-id="40">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-133.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">YinYang-Net: Complementing Face and Body Information for Wild Gender Recognition</h3>
                <p class="text-gray-600 mb-2">Tiago Roxo, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Access</span> (2022)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Gender Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Soft Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Face and Body Information</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Wild Conditions</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/ACCESS.2022.3157857" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Tiago_Roxo_IEEEAccess.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="40">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="40">
            <h2>YinYang-Net: Complementing Face and Body Information for Wild Gender Recognition</h2>
            <p class="authors">Tiago Roxo, Hugo Proença</p>
            <p class="venue">IEEE Access (2022)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Soft biometrics inference in surveillance scenarios is a topic of interest for various applications, particularly in security-related areas. However, soft biometric analysis is not extensively reported in wild conditions. In particular, previous works on gender recognition report their results in face datasets, with relatively good image quality and frontal poses. Given the uncertainty of the availability of the facial region in wild conditions, we consider that these methods are not adequate for surveillance settings. To overcome these limitations, we: 1) present frontal and wild face versions of three well-known surveillance datasets; and 2) propose YinYang-Net (YY-Net), a model that effectively and dynamically complements facial and body information, which makes it suitable for gender recognition in wild conditions. The frontal and wild face datasets derive from widely used Pedestrian Attribute Recognition (PAR) sets (PETA, PA-100K, and RAP), using a pose-based approach to filter the frontal samples and facial regions. This approach retrieves the facial region of images with varying image/subject conditions, where the state-of-the-art face detectors often fail. YY-Net combines facial and body information through a learnable fusion matrix and a channel-attention sub-network, focusing on the most influential body parts according to the specific image/subject features. We compare it with five PAR methods, consistently obtaining state-of-the-art results on gender recognition, and reducing the prediction errors by up to 24% in frontal samples. The announced PAR datasets versions and YY-Net serve as the basis for wild soft biometrics classification and are available in here.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/ACCESS.2022.3157857" target="_blank">10.1109/ACCESS.2022.3157857</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Tiago_Roxo_IEEEAccess.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2022" 
         data-type="journal"
         data-title="25th icpr—real‐time visual surveillance as‐a‐service (vsaas) for smart security solutions"
         data-authors="michele nappi, hugo proença, guodong guo, sambit bakshi"
         data-venue="iet biometrics"
         data-tags="visual surveillance,real-time processing,security solutions"
         data-id="41">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/ietbiometrics.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">25th ICPR—Real‐time Visual Surveillance as‐a‐Service (VSaaS) for smart security solutions</h3>
                <p class="text-gray-600 mb-2">Michele Nappi, Hugo Proença, Guodong Guo, Sambit Bakshi</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IET Biometrics</span> (2022)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visual Surveillance</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Real-time Processing</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Security Solutions</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1049/bme2.12089" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ICPR_2022.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="41">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="41">
            <h2>25th ICPR—Real‐time Visual Surveillance as‐a‐Service (VSaaS) for smart security solutions</h2>
            <p class="authors">Michele Nappi, Hugo Proença, Guodong Guo, Sambit Bakshi</p>
            <p class="venue">IET Biometrics (2022)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>With the advent of ever‐fast computing, real‐time processing of visual data has been gaining importance in the field of surveillance. Also, automated decision‐making by visual surveillance systems has been contributing to a huge leap in the capability of such systems, and of course their relevance in social security. This special issue aimed to discuss cloud‐based architectures of surveillance frameworks as a service. Such systems, especially when deployed to work in real‐time, are required to be fast, efficient, and sustainable with a varying load of visual data. Four papers were selected for inclusion in this special issue.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1049/bme2.12089" target="_blank">10.1049/bme2.12089</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ICPR_2022.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2022" 
         data-type="journal"
         data-title="a fuzzy consensus clustering algorithm for mri brain tissue segmentation"
         data-authors="sv aruna kumar, ehsan yaghoubi, hugo proença"
         data-venue="applied sciences"
         data-tags="brain tissue segmentation,fuzzy clustering,mri,machine learning"
         data-id="42">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/as.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">A Fuzzy Consensus Clustering Algorithm for MRI Brain Tissue Segmentation</h3>
                <p class="text-gray-600 mb-2">SV Aruna Kumar, Ehsan Yaghoubi, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Applied Sciences</span> (2022)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Brain Tissue Segmentation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Fuzzy Clustering</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">MRI</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Machine Learning</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.3390/app12157385" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/applsci-12-07385.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="42">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="42">
            <h2>A Fuzzy Consensus Clustering Algorithm for MRI Brain Tissue Segmentation</h2>
            <p class="authors">SV Aruna Kumar, Ehsan Yaghoubi, Hugo Proença</p>
            <p class="venue">Applied Sciences (2022)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Brain tissue segmentation is an important component of the clinical diagnosis of brain diseases using multi-modal magnetic resonance imaging (MR). Brain tissue segmentation has been developed by many unsupervised methods in the literature. The most commonly used unsupervised methods are K-Means, Expectation-Maximization, and Fuzzy Clustering. Fuzzy clustering methods offer considerable benefits compared with the aforementioned methods as they are capable of handling brain images that are complex, largely uncertain, and imprecise. However, this approach suffers from the intrinsic noise and intensity inhomogeneity (IIH) in the data resulting from the acquisition process. To resolve these issues, we propose a fuzzy consensus clustering algorithm that defines a membership function resulting from a voting schema to cluster the pixels. In particular, we first pre-process the MRI data and employ several segmentation techniques based on traditional fuzzy sets and intuitionistic sets. Then, we adopted a voting schema to fuse the results of the applied clustering methods. Finally, to evaluate the proposed method, we used the well-known performance measures (boundary measure, overlap measure, and volume measure) on two publicly available datasets (OASIS and IBSR18). The experimental results show the superior performance of the proposed method in comparison with the recent state of the art. The performance of the proposed method is also presented using a real-world Autism Spectrum Disorder Detection problem with better accuracy compared to other existing methods.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.3390/app12157385" target="_blank">10.3390/app12157385</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/applsci-12-07385.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2022" 
         data-type="journal"
         data-title="real-time weed control application using a jetson nano edge device and a spray mechanism"
         data-authors="eduardo assunção, pedro d. gaspar, ricardo mesquita, maria p. simões, khadijeh alibabaei, andré veiros, hugo proença"
         data-venue="mdpi remote sensing"
         data-tags="deep neural networks,edge computing,weed control,semantic segmentation"
         data-id="43">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/rs.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Real-Time Weed Control Application Using a Jetson Nano Edge Device and a Spray Mechanism</h3>
                <p class="text-gray-600 mb-2">Eduardo Assunção, Pedro D. Gaspar, Ricardo Mesquita, Maria P. Simões, Khadijeh Alibabaei, André Veiros, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">MDPI Remote Sensing</span> (2022)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Deep Neural Networks</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Edge Computing</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Weed Control</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Semantic Segmentation</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.3390/rs14174217" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/rs_22.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="43">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="43">
            <h2>Real-Time Weed Control Application Using a Jetson Nano Edge Device and a Spray Mechanism</h2>
            <p class="authors">Eduardo Assunção, Pedro D. Gaspar, Ricardo Mesquita, Maria P. Simões, Khadijeh Alibabaei, André Veiros, Hugo Proença</p>
            <p class="venue">MDPI Remote Sensing (2022)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Portable devices play an essential role where edge computing is necessary and mobility is required (e.g., robots in agriculture within remote-sensing applications). With the increasing applications of deep neural networks (DNNs) and accelerators for edge devices, several methods and applications have been proposed for simultaneous crop and weed detection. Although preliminary studies have investigated the performance of inference time for semantic segmentation of crops and weeds in edge devices, performance degradation has not been evaluated in detail when the required optimization is applied to the model for operation in such edge devices. This paper investigates the relationship between model tuning hyperparameters to improve inference time and its effect on segmentation performance. The study was conducted using semantic segmentation model DeeplabV3 with a MobileNet backbone. Different datasets (Cityscapes, PASCAL and ADE20K) were analyzed for a transfer learning strategy. The results show that, when using a model hyperparameter depth multiplier (DM) of 0.5 and the TensorRT framework, segmentation performance mean intersection over union (mIOU) decreased by 14.7% compared to that of a DM of 1.0 and no TensorRT. However, inference time accelerated dramatically by a factor of 14.8. At an image resolution of 1296 × 966, segmentation performance of 64% mIOU and inference of 5.9 frames per second (FPS) was achieved in Jetson Nano's device. With an input image resolution of 513 × 513, and hyperparameters output stride OS = 32 and DM = 0.5, an inference time of 0.04 s was achieved resulting in 25 FPS. The results presented in this paper provide a deeper insight into how the performance of the semantic segmentation model of crops and weeds degrades when optimization is applied to adapt the model to run on edge devices. Lastly, an application is described for the semantic segmentation of weeds embedded in the edge device (Jetson Nano) and integrated with the robotic orchard. The results show good spraying accuracy and feasibility of the method.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.3390/rs14174217" target="_blank">10.3390/rs14174217</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/rs_22.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2022" 
         data-type="journal"
         data-title="regina - reasoning graph convolutional networks in human action recognition"
         data-authors="bruno degardin, vasco lopes, hugo proença"
         data-venue="ieee transactions on information forensics and security"
         data-tags="human action recognition,graph convolutional networks,skeleton-based recognition"
         data-id="44">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/tifs.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">REGINA - Reasoning Graph Convolutional Networks in Human Action Recognition</h3>
                <p class="text-gray-600 mb-2">Bruno Degardin, Vasco Lopes, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Information Forensics and Security</span> (2022)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Human Action Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Graph Convolutional Networks</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Skeleton-based Recognition</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIFS.2021.3130437" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/degardin_tifs_2021.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="44">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="44">
            <h2>REGINA - Reasoning Graph Convolutional Networks in Human Action Recognition</h2>
            <p class="authors">Bruno Degardin, Vasco Lopes, Hugo Proença</p>
            <p class="venue">IEEE Transactions on Information Forensics and Security (2022)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>It is known that the kinematics of the human body skeleton reveals valuable information in action recognition. Recently, modelling skeletons as spatio-temporal graphs with Graph Convolutional Networks (GCNs) has been reported to solidly advance the state-of-the-art performance. However, GCN-based approaches exclusively learn from raw skeleton data, and are expected to extract the inherent structural information on their own. This paper describes REGINA, introducing a novel way to REasoning Graph convolutional networks IN Human Action recognition. The rationale is to provide to the GCNs additional knowledge about the skeleton data, obtained by hand-crafted features, in order to facilitate the learning process, while guaranteeing that it remains fully trainable in an end-to-end manner. The challenge is to capture complementary information over the dynamics between consecutive frames, which is the key information extracted by state-of-the-art GCN techniques. Moreover, the proposed strategy can be easily integrated in the existing GCN-based methods, which we also regard positively. Our experiments were carried out in well known action recognition datasets and enabled to conclude that REGINA contributes for solid improvements in performance when incorporated to other GCN-based approaches, without any other adjustment regarding the original method.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIFS.2021.3130437" target="_blank">10.1109/TIFS.2021.3130437</a></p>
            
            <div class="links">
                
                <a href="http://di.ubi.pt/~hugomcp/doc/degardin_tifs_2021.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2021" 
         data-type="special issue"
         data-title="new trends on pattern recognition, applications and systems"
         data-authors="hugo proença, joão c. neves"
         data-venue="applied sciences"
         data-tags="pattern recognition,special issue,applications,systems"
         data-id="45">
        <div class="flex items-start">
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">New Trends on Pattern Recognition, Applications and Systems</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, João C. Neves</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Applied Sciences</span> (2021)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge special issue">Special issue</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Pattern Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Special Issue</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Applications</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Systems</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="45">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="45">
            <h2>New Trends on Pattern Recognition, Applications and Systems</h2>
            <p class="authors">Hugo Proença, João C. Neves</p>
            <p class="venue">Applied Sciences (2021)</p>
            
            
            <div class="links">
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2021" 
         data-type="journal"
         data-title="you look so different! haven't i seen you a long time ago?"
         data-authors="ehsan yaghoubi, diana borza, bruno degardin, hugo proença"
         data-venue="elsevier image and vision computing"
         data-tags="person re-identification,long-term recognition,clothing-invariant features,deep learning"
         data-id="46">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/ivc.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">You Look So Different! Haven't I Seen You a Long Time Ago?</h3>
                <p class="text-gray-600 mb-2">Ehsan Yaghoubi, Diana Borza, Bruno Degardin, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Image and Vision Computing</span> (2021)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Person Re-identification</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Long-term Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Clothing-invariant Features</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Deep Learning</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.imavis.2021.104288" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/IVC_Ehsan_2021.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="46">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="46">
            <h2>You Look So Different! Haven't I Seen You a Long Time Ago?</h2>
            <p class="authors">Ehsan Yaghoubi, Diana Borza, Bruno Degardin, Hugo Proença</p>
            <p class="venue">Elsevier Image and Vision Computing (2021)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Person re-identification (re-id) aims to match a query identity (ID) to an element in a gallery set, composed of elements collected from multiple cameras. Most of the existing re-id methods assume the short-term setting, where the query/gallery samples share the clothing style. In this setting, the optimal feature representations are based in the visual appearance of clothes, which considerably drops the identification performance for long-term settings. Having this problem in mind, we propose a model that learns long-term representations of persons by ignoring any features previously learned by a short-term re-id model, which naturally makes it invariant to clothing styles. We start by synthesizing a data set in which we distort the most relevant biometric information (based in face, body shape, height, and weight cues), keeping the short-term cues (color and texture of clothes) unchanged. This way, while the original data contains both ID-related and other varying features, the synthesized representations are composed mostly of short-term attributes. Then, the key to obtaining stable long-term representations is to learn embeddings of the original data that maximize the dissimilarity with the previously inferred short-term embeddings. In practice, we use the synthetic data to learn a model that embeds the ID-unrelated features and then learn a second model from the original data, where long-term embeddings are obtained, keeping their independence with respect to the previously obtained ID-unrelated features. Our experiments were performed on three challenging cloth-changing sets (LTCC, PRCC, and NKUP) and the results support the effectiveness of the proposed method, for both short and long-term re-id settings.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.imavis.2021.104288" target="_blank">10.1016/j.imavis.2021.104288</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/IVC_Ehsan_2021.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2021" 
         data-type="journal"
         data-title="a quadruplet loss for enforcing semantically coherent embeddings in multi-output classification problems"
         data-authors="hugo proença, ehsan yaghoubi, pendar alirezazadeh"
         data-venue="ieee transactions on information forensics and security"
         data-tags="deep learning,multi-output classification,feature embedding,triplet loss,soft biometrics"
         data-id="47">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/tifs.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">A Quadruplet Loss for Enforcing Semantically Coherent Embeddings in Multi-output Classification Problems</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Ehsan Yaghoubi, Pendar Alirezazadeh</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Information Forensics and Security</span> (2021)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Deep Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Multi-output Classification</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Feature Embedding</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Triplet Loss</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Soft Biometrics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIFS.2020.3023304" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Quadruplet.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="47">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="47">
            <h2>A Quadruplet Loss for Enforcing Semantically Coherent Embeddings in Multi-output Classification Problems</h2>
            <p class="authors">Hugo Proença, Ehsan Yaghoubi, Pendar Alirezazadeh</p>
            <p class="venue">IEEE Transactions on Information Forensics and Security (2021)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>This paper describes one objective function for learning semantically coherent feature embeddings in multi-output classification problems, i.e., when the response variables have dimension higher than one. Such coherent embeddings can be used simultaneously for different tasks, such as identity retrieval and soft biometrics labelling. We propose a generalization of the triplet loss that: 1) defines a metric that considers the number of agreeing labels between pairs of elements; 2) introduces the concept of similar classes, according to the values provided by the metric; and 3) disregards the notion of anchor, sampling four arbitrary elements at each time, from where two pairs are defined. The distances between elements in each pair are imposed according to their semantic similarity (i.e., the number of agreeing labels). Likewise the triplet loss, our proposal also privileges small distances between positive pairs. However, the key novelty is to additionally enforce that the distance between elements of any other pair corresponds inversely to their semantic similarity. The proposed loss yields embeddings with a strong correspondence between the classes centroids and their semantic descriptions. In practice, it is a natural choice to jointly infer coarse (soft biometrics) + fine (ID) labels, using simple rules such as k-neighbours. Also, in opposition to its triplet counterpart, the proposed loss appears to be agnostic with regard to demanding criteria for mining learning instances (such as the semi-hard pairs). Our experiments were carried out in five different datasets (BIODI, LFW, IJB-A, Megaface and PETA) and validate our assumptions, showing results that are comparable to the state-of-the-art in both the identity retrieval and soft biometrics labelling tasks.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIFS.2020.3023304" target="_blank">10.1109/TIFS.2020.3023304</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Quadruplet.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2021" 
         data-type="special issue"
         data-title="computer vision & biometrics in healthcare monitoring, diagnosis and treatment"
         data-authors="michele nappi, sambit bakshi, hugo proença, vittorio murino"
         data-venue="elsevier computer vision and image understanding"
         data-tags="computer vision,biometrics,healthcare monitoring,diagnosis"
         data-id="48">
        <div class="flex items-start">
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Computer Vision & Biometrics in Healthcare Monitoring, Diagnosis and Treatment</h3>
                <p class="text-gray-600 mb-2">Michele Nappi, Sambit Bakshi, Hugo Proença, Vittorio Murino</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Computer Vision and Image Understanding</span> (2021)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge special issue">Special issue</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Computer Vision</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Healthcare Monitoring</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Diagnosis</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="48">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="48">
            <h2>Computer Vision & Biometrics in Healthcare Monitoring, Diagnosis and Treatment</h2>
            <p class="authors">Michele Nappi, Sambit Bakshi, Hugo Proença, Vittorio Murino</p>
            <p class="venue">Elsevier Computer Vision and Image Understanding (2021)</p>
            
            
            <div class="links">
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2021" 
         data-type="journal"
         data-title="person re-identification: implicitly defining the receptive fields of deep learning classification frameworks"
         data-authors="ehsan yaghoubi, diana borza, sv aruna kumar, hugo proença"
         data-venue="elsevier pattern recognition letters"
         data-tags="person re-identification,receptive fields,deep learning,classification"
         data-id="49">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/prl.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Person re-identification: Implicitly Defining the Receptive Fields of Deep Learning Classification Frameworks</h3>
                <p class="text-gray-600 mb-2">Ehsan Yaghoubi, Diana Borza, SV Aruna Kumar, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Pattern Recognition Letters</span> (2021)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Person Re-identification</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Receptive Fields</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Deep Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Classification</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.patrec.2021.01.035" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/RF_PRL.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="49">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="49">
            <h2>Person re-identification: Implicitly Defining the Receptive Fields of Deep Learning Classification Frameworks</h2>
            <p class="authors">Ehsan Yaghoubi, Diana Borza, SV Aruna Kumar, Hugo Proença</p>
            <p class="venue">Elsevier Pattern Recognition Letters (2021)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The receptive fields of deep learning models determine the most significant regions of the input data for providing correct decisions. Up to now, the primary way to learn such receptive fields is to train the models upon masked data, which helps the networks to ignore any unwanted regions, but also has two major drawbacks: (1) it yields edge-sensitive decision processes; and (2) it augments considerably the computational cost of the inference phase. Having theses weaknesses in mind, this paper describes a solution for implicitly enhancing the inference of the networks' receptive fields, by creating synthetic learning data composed of interchanged segments considered apriori important or irrelevant for the network decision. In practice, we use a segmentation module to distinguish between the foreground (important) versus background (irrelevant) parts of each learning instance, and randomly swap segments between image pairs, while keeping the class label exclusively consistent with the label of the segments deemed important. This strategy typically drives the networks to interpret that the identity and clutter descriptions are not correlated. Moreover, the proposed solution has other interesting properties: (1) it is parameter-learning-free; (2) it fully preserves the label information; and (3) it is compatible with the data augmentation techniques typically used. In our empirical evaluation, we considered the person re-identification problem, and the well known RAP, Market1501 and MSMT-V2 datasets for two different settings (upper-body and full-body), having observed highly competitive results over the state-of-the-art.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.patrec.2021.01.035" target="_blank">10.1016/j.patrec.2021.01.035</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/RF_PRL.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2021" 
         data-type="journal"
         data-title="is gender "in-the-wild'' inference really a solved problem?"
         data-authors="tiago roxo, hugo proença"
         data-venue="ieee transactions on biometrics, behaviour and identity science"
         data-tags="gender inference,soft biometrics,wild conditions,feature analysis"
         data-id="50">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/tbiom.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Is Gender "In-the-Wild'' Inference Really a Solved Problem?</h3>
                <p class="text-gray-600 mb-2">Tiago Roxo, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Biometrics, Behaviour and Identity Science</span> (2021)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Gender Inference</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Soft Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Wild Conditions</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Feature Analysis</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TBIOM.2021.3100926" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/TBIOM_Tiago_Roxo.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="50">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="50">
            <h2>Is Gender "In-the-Wild'' Inference Really a Solved Problem?</h2>
            <p class="authors">Tiago Roxo, Hugo Proença</p>
            <p class="venue">IEEE Transactions on Biometrics, Behaviour and Identity Science (2021)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Soft biometrics analysis is seen as an important research topic, given its relevance to various applications. However, even though it is frequently seen as a solved task, it can still be very hard to perform in wild conditions, under varying image conditions, uncooperative poses, and occlusions. Considering the gender trait as our topic of study, we report an extensive analysis of the feasibility of its inference regarding image (resolution, luminosity, and blurriness) and subject-based features (face and body keypoints confidence). Using three state-of-the-art datasets (PETA, PA-100K, RAP) and five Person Attribute Recognition models, we correlate feature analysis with gender inference accuracy using the Shapley value, enabling us to perceive the importance of each image/subject-based feature. Furthermore, we analyze face-based gender inference and assess the pose effect on it. Our results suggest that: 1) image-based features are more influential for low-quality data; 2) an increase in image quality translates into higher subject-based feature importance; 3) face-based gender inference accuracy correlates with image quality increase; and 4) subjects' frontal pose promotes an implicit attention towards the face. The reported results are seen as a basis for subsequent developments of inference approaches in uncontrolled outdoor environments, which typically correspond to visual surveillance conditions.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TBIOM.2021.3100926" target="_blank">10.1109/TBIOM.2021.3100926</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/TBIOM_Tiago_Roxo.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2021" 
         data-type="journal"
         data-title="human behavior analysis: a survey on action recognition"
         data-authors="bruno degardin, hugo proença"
         data-venue="mdpi applied sciences"
         data-tags="human behavior analysis,action recognition,survey,computer vision"
         data-id="51">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/As.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Human Behavior Analysis: A Survey on Action Recognition</h3>
                <p class="text-gray-600 mb-2">Bruno Degardin, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">MDPI Applied Sciences</span> (2021)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Human Behavior Analysis</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Action Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Survey</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Computer Vision</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.3390/app11188324" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/AS_Degardin.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="51">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="51">
            <h2>Human Behavior Analysis: A Survey on Action Recognition</h2>
            <p class="authors">Bruno Degardin, Hugo Proença</p>
            <p class="venue">MDPI Applied Sciences (2021)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The visual recognition and understanding of human actions remain an active research domain of computer vision, being the scope of various research works over the last two decades. The problem is challenging due to its many interpersonal variations in appearance and motion dynamics between humans, without forgetting the environmental heterogeneity between different video images. This complexity splits the problem into two major categories: action classification, recognising the action being performed in the scene, and spatiotemporal action localisation, concerning recognising multiple localised human actions present in the scene. Previous surveys mainly focus on the evolution of this field, from handcrafted features to deep learning architectures. However, this survey presents an overview of both categories and respective evolution within each one, the guidelines that should be followed and the current benchmarks employed for performance comparison between the state- of-the-art methods.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.3390/app11188324" target="_blank">10.3390/app11188324</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/AS_Degardin.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2021" 
         data-type="journal"
         data-title="editorial to special issue on novel insights on ocular biometrics"
         data-authors="maria de marsico, hugo proença, sambit bakshi, abhijit das"
         data-venue="elsevier image and vision computing"
         data-tags="ocular biometrics,periocular recognition,sclera recognition,editorial"
         data-id="52">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/Ivc.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Editorial to special issue on novel insights on ocular biometrics</h3>
                <p class="text-gray-600 mb-2">Maria De Marsico, Hugo Proença, Sambit Bakshi, Abhijit Das</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Image and Vision Computing</span> (2021)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Ocular Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Sclera Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Editorial</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.imavis.2021.104227" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/IVC_SI.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="52">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="52">
            <h2>Editorial to special issue on novel insights on ocular biometrics</h2>
            <p class="authors">Maria De Marsico, Hugo Proença, Sambit Bakshi, Abhijit Das</p>
            <p class="venue">Elsevier Image and Vision Computing (2021)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Ocular biometrics have a great potential to support biometric applications, due to the unique features of the ocular traits. Notwithstanding this, the related lines of research still present several open issues, which justify the ongoing research efforts. For instance, the relatively recent emergence of the periocular and sclera traits makes it worth recording the progresses in those areas. Furthermore, wider and deeper investigations regarding all the traits underlying the ocular region and the best way to combine them still needs to be thoroughly undertaken. This would not only improve the recognition robustness, but also make perceiving the potential of this kind of solutions in solving problems in the biometrics domain. Moreover, 'systems interpretability', 'weakly/partial supervised recognition' or 'forensics evidence and biometric recognition' add interest to an already rich field of research. This special issue aims at providing a platform to publish and record the recent research on ocular biometrics in order to push the state-of-the-art forward.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.imavis.2021.104227" target="_blank">10.1016/j.imavis.2021.104227</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/IVC_SI.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2021" 
         data-type="journal"
         data-title="a quadruplet loss for enforcing semantically coherent embeddings in multi-output classification problems"
         data-authors="hugo proença, ehsan yaghoubi, pendar alirezazadeh"
         data-venue="ieee transactions on information forensics and security"
         data-tags="quadruplet loss,multi-output classification,feature embeddings,soft biometrics"
         data-id="53">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/tifs.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">A Quadruplet Loss for Enforcing Semantically Coherent Embeddings in Multi-output Classification Problems</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Ehsan Yaghoubi, Pendar Alirezazadeh</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Information Forensics and Security</span> (2021)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Quadruplet Loss</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Multi-output Classification</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Feature Embeddings</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Soft Biometrics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIFS.2020.3023304" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Quadruplet.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="53">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="53">
            <h2>A Quadruplet Loss for Enforcing Semantically Coherent Embeddings in Multi-output Classification Problems</h2>
            <p class="authors">Hugo Proença, Ehsan Yaghoubi, Pendar Alirezazadeh</p>
            <p class="venue">IEEE Transactions on Information Forensics and Security (2021)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>This paper describes one objective function for learning semantically coherent feature embeddings in multi-output classification problems, i.e., when the response variables have dimension higher than one. Such coherent embeddings can be used simultaneously for different tasks, such as identity retrieval and soft biometrics labelling. We propose a generalization of the triplet loss that: 1) defines a metric that considers the number of agreeing labels between pairs of elements; 2) introduces the concept of similar classes, according to the values provided by the metric; and 3) disregards the notion of anchor, sampling four arbitrary elements at each time, from where two pairs are defined. The distances between elements in each pair are imposed according to their semantic similarity (i.e., the number of agreeing labels). Likewise the triplet loss, our proposal also privileges small distances between positive pairs. However, the key novelty is to additionally enforce that the distance between elements of any other pair corresponds inversely to their semantic similarity. The proposed loss yields embeddings with a strong correspondence between the classes centroids and their semantic descriptions. In practice, it is a natural choice to jointly infer coarse (soft biometrics) + fine (ID) labels, using simple rules such as k-neighbours. Also, in opposition to its triplet counterpart, the proposed loss appears to be agnostic with regard to demanding criteria for mining learning instances (such as the semi-hard pairs). Our experiments were carried out in five different datasets (BIODI, LFW, IJB-A, Megaface and PETA) and validate our assumptions, showing results that are comparable to the state-of-the-art in both the identity retrieval and soft biometrics labelling tasks.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIFS.2020.3023304" target="_blank">10.1109/TIFS.2020.3023304</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Quadruplet.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2021" 
         data-type="journal"
         data-title="the p-destre: a fully annotated dataset for pedestrian detection, tracking and short/long-term re-identification from aerial devices"
         data-authors="sv aruna kumar, ehsan yaghoubi, abhijit das, b.s. harish, hugo proença"
         data-venue="ieee transactions on information forensics and security"
         data-tags="pedestrian detection,pedestrian tracking,re-identification,unmanned aerial vehicles"
         data-id="54">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/tifs.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">The P-DESTRE: A Fully Annotated Dataset for Pedestrian Detection, Tracking and Short/Long-term Re-Identification from Aerial Devices</h3>
                <p class="text-gray-600 mb-2">SV Aruna Kumar, Ehsan Yaghoubi, Abhijit Das, B.S. Harish, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Information Forensics and Security</span> (2021)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Pedestrian Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Pedestrian Tracking</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Re-identification</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Unmanned Aerial Vehicles</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIFS.2020.3040881" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/PDESTRE.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="54">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="54">
            <h2>The P-DESTRE: A Fully Annotated Dataset for Pedestrian Detection, Tracking and Short/Long-term Re-Identification from Aerial Devices</h2>
            <p class="authors">SV Aruna Kumar, Ehsan Yaghoubi, Abhijit Das, B.S. Harish, Hugo Proença</p>
            <p class="venue">IEEE Transactions on Information Forensics and Security (2021)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Over the years, unmanned aerial vehicles (UAVs) have been regarded as a potential solution to surveil public spaces, providing a cheap way for data collection, while covering large and difficult-to-reach areas. This kind of solutions can be particularly useful to detect, track and identify subjects of interest in crowds, for security/safety purposes. In this context, various datasets are publicly available, yet most of them are only suitable for evaluating detection, tracking and short-term re-identification techniques. This paper announces the free availability of the P-DESTRE dataset, the first of its kind to provide video/UAV-based data for pedestrian long-term re-identification research, with ID annotations consistent across data collected in different days. As a secondary contribution, we provide the results attained by the state-of-the-art pedestrian detection, tracking, short/long term re-identification techniques in well-known surveillance datasets, used as baselines for the corresponding effectiveness observed in the P-DESTRE data. This comparison highlights the discriminating characteristics of P-DESTRE with respect to similar sets. Finally, we identify the most problematic data degradation factors and co-variates for UAV-based automated data analysis, which should be considered in subsequent technologic/conceptual advances in this field.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIFS.2020.3040881" target="_blank">10.1109/TIFS.2020.3040881</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/PDESTRE.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2021" 
         data-type="conference"
         data-title="a deep adversarial framework for visually interpretable biometric recognition"
         data-authors="joão brito, hugo proença"
         data-venue="proceedings of the ieee computer society conference on computer vision and pattern recognition 'biometrics' workshop – cvprw 2021"
         data-tags="visual interpretability,biometric recognition,adversarial generative techniques,periocular recognition"
         data-id="55">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/cvf.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">A Deep Adversarial Framework for Visually Interpretable Biometric Recognition</h3>
                <p class="text-gray-600 mb-2">João Brito, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 'Biometrics' Workshop – CVPRW 2021</span> (2021)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visual Interpretability</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Adversarial Generative Techniques</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Recognition</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/CVPRW53098.2021.00161" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/cvprw2021.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="55">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="55">
            <h2>A Deep Adversarial Framework for Visually Interpretable Biometric Recognition</h2>
            <p class="authors">João Brito, Hugo Proença</p>
            <p class="venue">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 'Biometrics' Workshop – CVPRW 2021 (2021)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>In the biometrics context, the ability to provide the reasoning behind a decision has been at the core of major research efforts. Explanations serve not only to increase the trust amongst the users of a system, but also to augment the system's overall accountability and transparency. In this work, we describe a periocular recognition framework that not only performs biometric recognition, but also provides visual representations of the features/regions that supported a decision. Being particularly designed to explain non-match ('impostors') decisions, our solution uses adversarial generative techniques to synthesise a large set of 'genuine' image pairs, from where the most similar elements with respect to a query are retrieved. Then, assuming the alignment between the query/retrieved pairs, the element-wise differences between the query and a weighted average of the retrieved elements yields a visual explanation of the regions in the query pair that would have to be different to transform it into a 'genuine' pair. Our quantitative and qualitative experiments validate the proposed solution, yielding recognition rates that are similar to the state-of-the-art, but - most importantly - also providing the visual explanations for every decision.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/CVPRW53098.2021.00161" target="_blank">10.1109/CVPRW53098.2021.00161</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/cvprw2021.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2021" 
         data-type="journal"
         data-title="sss-pr: a short survey of surveys in person re-identification"
         data-authors="ehsan yaghoubi, sv aruna kumar, hugo proença"
         data-venue="elsevier pattern recognition letters"
         data-tags="person re-identification,survey,multi-dimensional taxonomy"
         data-id="56">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/prl.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">SSS-PR: A Short Survey of Surveys in Person Re-identification</h3>
                <p class="text-gray-600 mb-2">Ehsan Yaghoubi, SV Aruna Kumar, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Pattern Recognition Letters</span> (2021)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Person Re-identification</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Survey</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Multi-dimensional Taxonomy</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.patrec.2020.12.017" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/SSS-PR.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="56">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="56">
            <h2>SSS-PR: A Short Survey of Surveys in Person Re-identification</h2>
            <p class="authors">Ehsan Yaghoubi, SV Aruna Kumar, Hugo Proença</p>
            <p class="venue">Elsevier Pattern Recognition Letters (2021)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Person re-identification (re-id) addresses the problem of whether 'a query image corresponds to an identity in the database' and is believed to play a fundamental role in security enforcement in the near future, particularly in crowded urban environments. Due to many possibilities in selecting appropriate model architectures, datasets, and settings, the performance reported by the state-of-the-art re-id methods oscillates significantly among the published surveys. Therefore, it is difficult to understand the mainstream trends and emerging research difficulties in person re-id. This paper proposes a multi-dimensional taxonomy to categorize the most relevant researches according to different perspectives and tries to unify the categorization of re-id methods and fill the gap between the recently published surveys. Furthermore, we discuss the open challenges with a focus on privacy concerns and the issues caused by the exponential increase in the number of re-id publications over the recent years. Finally, we discuss several challenging directions for future studies.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.patrec.2020.12.017" target="_blank">10.1016/j.patrec.2020.12.017</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/SSS-PR.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2021" 
         data-type="journal"
         data-title="iterative weak/self-supervised classification framework for abnormal events detection"
         data-authors="bruno degardin, hugo proença"
         data-venue="elsevier pattern recognition letters"
         data-tags="abnormal events detection,weak supervision,self-supervised learning,visual surveillance"
         data-id="57">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/prl.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Iterative Weak/Self-Supervised Classification Framework for Abnormal Events Detection</h3>
                <p class="text-gray-600 mb-2">Bruno Degardin, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Pattern Recognition Letters</span> (2021)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Abnormal Events Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Weak Supervision</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Self-Supervised Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visual Surveillance</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.patrec.2021.01.031" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Events_PRL.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="57">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="57">
            <h2>Iterative Weak/Self-Supervised Classification Framework for Abnormal Events Detection</h2>
            <p class="authors">Bruno Degardin, Hugo Proença</p>
            <p class="venue">Elsevier Pattern Recognition Letters (2021)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The detection of abnormal events in surveillance footage remains a challenge and has been the scope of various research works. Having observed that the state-of-the-art performance is still unsatisfactory, this paper provides a novel solution to the problem, with four-fold contributions: 1) upon the work of Sultani et al., we introduce one iterative learning framework composed of two experts working in the weak and self-supervised paradigms and providing additional amounts of learning data to each other, where the novel instances at each iteration are filtered by a Bayesian framework that supports the iterative data augmentation task; 2) we describe a novel term that is added to the baseline loss to spread the scores in the unit interval, which is crucial for the performance of the iterative framework; 3) we propose a Random Forest ensemble that fuses at the score level the top performing methods and reduces the EER values about 20% over the state-of-the-art; and 4) we announce the availability of the 'UBI-Fights' dataset, fully annotated at the frame level, that can be freely used by the research community.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.patrec.2021.01.031" target="_blank">10.1016/j.patrec.2021.01.031</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Events_PRL.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2021" 
         data-type="journal"
         data-title="a short survey on machine learning explainability: an application to periocular recognition"
         data-authors="joão brito, hugo proença"
         data-venue="mdpi electronics"
         data-tags="machine learning explainability,periocular recognition,visual explanations,interpretable ai"
         data-id="58">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/mdpielectronics.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">A short survey on machine learning explainability: an application to periocular recognition</h3>
                <p class="text-gray-600 mb-2">João Brito, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">MDPI Electronics</span> (2021)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Machine Learning Explainability</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visual Explanations</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Interpretable AI</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.3390/electronics10151861" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Electronics_JBrito.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="58">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="58">
            <h2>A short survey on machine learning explainability: an application to periocular recognition</h2>
            <p class="authors">João Brito, Hugo Proença</p>
            <p class="venue">MDPI Electronics (2021)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Interpretability has made significant strides in recent years, enabling the formerly black-box models to reach new levels of transparency. These kinds of models can be particularly useful to broaden the applicability of machine learning-based systems to domains where—apart from the predictions—appropriate justifications are also required (e.g., forensics and medical image analysis). In this context, techniques that focus on visual explanations are of particular interest here, due to their ability to directly portray the reasons that support a given prediction. Therefore, in this document, we focus on presenting the core principles of interpretability and describing the main methods that deliver visual cues (including one that we designed for periocular recognition in particular). Based on these intuitions, the experiments performed show explanations that attempt to highlight the most important periocular components towards a non-match decision. Then, some particularly challenging scenarios are presented to naturally sustain our conclusions and thoughts regarding future directions.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.3390/electronics10151861" target="_blank">10.3390/electronics10151861</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Electronics_JBrito.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2020" 
         data-type="conference"
         data-title="human activity analysis: iterative weak/self-supervised learning frameworks for detecting abnormal events"
         data-authors="bruno degardin, hugo proença"
         data-venue="proceedings of the international joint conference on biometrics – ijcb 2020"
         data-tags="human activity analysis,abnormal events detection,self-supervised learning,weak supervision"
         data-id="59">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/ieeexplore.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Human Activity Analysis: Iterative Weak/Self-Supervised Learning Frameworks for Detecting Abnormal Events</h3>
                <p class="text-gray-600 mb-2">Bruno Degardin, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the International Joint Conference on Biometrics – IJCB 2020</span> (2020)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Human Activity Analysis</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Abnormal Events Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Self-Supervised Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Weak Supervision</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/IJCB48548.2020.9304905" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ijcb2020_degardin.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="59">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="59">
            <h2>Human Activity Analysis: Iterative Weak/Self-Supervised Learning Frameworks for Detecting Abnormal Events</h2>
            <p class="authors">Bruno Degardin, Hugo Proença</p>
            <p class="venue">Proceedings of the International Joint Conference on Biometrics – IJCB 2020 (2020)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Having observed the unsatisfactory state-of-the-art performance in detecting abnormal events, this paper describes an iterative self-supervised learning method for such purpose. The proposed solution is composed of two experts that - at each step - find the most confidently classified instances to augment the amount of data available for the next iteration. Our contributions are four-fold: 1) we describe the iterative learning framework composed of experts working in the weak/self-supervised paradigms and providing learning data to each other, with the novel instances being filtered by a Bayesian framework; 2) upon Sultani et al.'s work, we suggest a novel term the loss function that spreads the scores in the unit interval and is important for the performance of the iterative framework; 3) we propose a late decision fusion scheme, in which an ensemble of Decision Trees learned from bootstrap samples fuses the scores of the top-3 methods, reducing the EER values about 20% over the state-of-the-art; and 4) we announce the 'Fights' dataset, fully annotated at the frame level, that can be freely used by the research community.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/IJCB48548.2020.9304905" target="_blank">10.1109/IJCB48548.2020.9304905</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ijcb2020_degardin.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2020" 
         data-type="conference"
         data-title="all-in-one "hairnet": a deep neural model for joint hair segmentation and characterization"
         data-authors="diana borza, ehsan yaghoubi, joão c. neves, hugo proença"
         data-venue="proceedings of the international joint conference on biometrics – ijcb 2020"
         data-tags="hair segmentation,hair characterization,deep learning,soft biometrics"
         data-id="60">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/ieeexplore.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">All-in-one "HairNet": A Deep Neural Model for Joint Hair Segmentation and Characterization</h3>
                <p class="text-gray-600 mb-2">Diana Borza, Ehsan Yaghoubi, João C. Neves, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the International Joint Conference on Biometrics – IJCB 2020</span> (2020)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Hair Segmentation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Hair Characterization</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Deep Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Soft Biometrics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/IJCB48548.2020.9304904" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="60">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="60">
            <h2>All-in-one "HairNet": A Deep Neural Model for Joint Hair Segmentation and Characterization</h2>
            <p class="authors">Diana Borza, Ehsan Yaghoubi, João C. Neves, Hugo Proença</p>
            <p class="venue">Proceedings of the International Joint Conference on Biometrics – IJCB 2020 (2020)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The hair appearance is among the most valuable soft biometric traits when performing human recognition at-a-distance. Even in degraded data, the hair's appearance is instinctively used by humans to distinguish between individuals. In this paper we propose a multi-task deep neural model capable of segmenting the hair region, while also inferring the hair color, shape and style, all from in-the-wild images. Our main contributions are two-fold: 1) the design of an all-in-one neural network, based on depth-wise separable convolutions to extract the features; and 2) the use convolutional feature masking layer as an attention mechanism that enforces the analysis only within the 'hair' regions. In a conceptual perspective, the strength of our model is that the segmentation mask is used by the other tasks to perceive - at feature-map level - only the regions relevant to the attribute characterization task. This paradigm allows the network to analyze features from non-rectangular areas of the input data, which is particularly important, considering the irregularity of hair regions. Our experiments showed that the proposed approach reaches a hair segmentation performance comparable to the state-of-the-art, having as main advantage the fact of performing multiple levels of analysis in a single-shot paradigm.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/IJCB48548.2020.9304904" target="_blank">10.1109/IJCB48548.2020.9304904</a></p>
            
            <div class="links">
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2020" 
         data-type="conference"
         data-title="decision-making support system for fruit diseases classification using deep learning"
         data-authors="eduardo assunção, catarina diniz, pedro d. gaspar, hugo proença"
         data-venue="proceedings of the 2020 international conference on decision aid sciences and application – dasa 2020"
         data-tags="fruit disease classification,deep learning,computer vision,decision support systems"
         data-id="61">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/ieeexplore.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Decision-making support system for fruit diseases classification using Deep Learning</h3>
                <p class="text-gray-600 mb-2">Eduardo Assunção, Catarina Diniz, Pedro D. Gaspar, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the 2020 International Conference on Decision Aid Sciences and Application – DASA 2020</span> (2020)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Fruit Disease Classification</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Deep Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Computer Vision</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Decision Support Systems</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/DASA51403.2020.9317219" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/DASA_2020.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="61">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="61">
            <h2>Decision-making support system for fruit diseases classification using Deep Learning</h2>
            <p class="authors">Eduardo Assunção, Catarina Diniz, Pedro D. Gaspar, Hugo Proença</p>
            <p class="venue">Proceedings of the 2020 International Conference on Decision Aid Sciences and Application – DASA 2020 (2020)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Fruit diseases are a continuous hazard to farmers. By applying computer vision-based techniques, precision agriculture can support the farmers in the decision making for fruit disease control. Features extraction is an essential task for the computer vision pipeline. Nowadays, in general, feature extraction for fruit diseases are handcrafted. However, empirical results in different domains confirm that features learned by Convolutional neural networks (CNNs) provide significant improvements in accuracy over handcrafted features. CNNs have been applied in many computer vision tasks, replacing the hand-engineered models. In general, a large-scale image dataset is necessary for training a CNN. However, there are not many fruit disease images available to compose the dataset. We propose to train a tiny and efficient deep convolutional network developed to run in the mobile devices to classify healthy peach fruits and three peach diseases. Based on transfer learning techniques and data augmentation strategies, the proposed model achieves a Macroaverage F1-score of 0.96. The model does not misclassify any disease class. This achievement shows the potential of using small CNN models for fruit disease classification when having a small quantity of training data.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/DASA51403.2020.9317219" target="_blank">10.1109/DASA51403.2020.9317219</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/DASA_2020.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2020" 
         data-type="journal"
         data-title="editorial for special section at pattern recognition letters - ibpria 2019"
         data-authors="manuel j. marin-jimenez, aythami morales, julian fierrez, antonio pertusa, hugo proença, j. salvador sanchez"
         data-venue="elsevier pattern recognition letters"
         data-tags="editorial,pattern recognition,image analysis,machine learning"
         data-id="62">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/prl.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Editorial for special section at Pattern Recognition Letters - IbPRIA 2019</h3>
                <p class="text-gray-600 mb-2">Manuel J. Marin-Jimenez, Aythami Morales, Julian Fierrez, Antonio Pertusa, Hugo Proença, J. Salvador Sanchez</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Pattern Recognition Letters</span> (2020)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Editorial</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Pattern Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Analysis</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Machine Learning</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.patrec.2020.03.024" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ibpria19_PRL.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="62">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="62">
            <h2>Editorial for special section at Pattern Recognition Letters - IbPRIA 2019</h2>
            <p class="authors">Manuel J. Marin-Jimenez, Aythami Morales, Julian Fierrez, Antonio Pertusa, Hugo Proença, J. Salvador Sanchez</p>
            <p class="venue">Elsevier Pattern Recognition Letters (2020)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Now in its 9th edition, IbPRIA has become a key research event in pattern recognition and image analysis in the Iberian Peninsula organized by the national IAPR associations for pattern recognition in Spain (AERFAI) and Portugal (APRP). Most of the research presented during the event, therefore, came from authors from Spain and Portugal. Out of the 401 authors who published in IbPRIA 2019, 29% were from Spain and 20% were from Portugal. More than 50% of the authors were from another 32 countries from all around the world, with high representation from countries like: Algeria, Brazil, Colombia, India, Italy, or Mexico. Our efforts to strengthen the bonds between the research conducted in the Iberian Peninsula and other countries was patent in the program, which emphasized interactive poster sessions, and included a special session dedicated to international research cooperation. IbPRIA 2019 received 137 submissions. The review process for IbPRIA 2019 was diligent and required careful consideration of more than 400 reviews from 100 reviewers who spent significant time and effort in reviewing the papers. In the end 99 papers were accepted, which is a 72% of acceptance. To form the final program 30 papers were selected for oral presentations (22% acceptance rate) and 69 as poster presentations. The program consisted of 7 oral sessions on the following topics: machine learning, image representation, image processing, biometrics, and document analysis. Three poster sessions included papers on all previous topics and also on the most important applications of nowadays technologies.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.patrec.2020.03.024" target="_blank">10.1016/j.patrec.2020.03.024</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ibpria19_PRL.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2020" 
         data-type="journal"
         data-title="ganprintr: improved fakes and evaluation of the state of the art in face manipulation detection"
         data-authors="joão c. neves, ruben tolosana, ruben vera-rodriguez, vasco lopes, hugo proença, julian fierrez"
         data-venue="ieee journal of selected topics in signal processing"
         data-tags="face manipulation detection,gan fingerprints,deep learning,digital forensics"
         data-id="63">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-130.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">GANprintR: Improved Fakes and Evaluation of the State of the Art in Face Manipulation Detection</h3>
                <p class="text-gray-600 mb-2">João C. Neves, Ruben Tolosana, Ruben Vera-Rodriguez, Vasco Lopes, Hugo Proença, Julian Fierrez</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Journal of Selected Topics in Signal Processing</span> (2020)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Face Manipulation Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">GAN Fingerprints</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Deep Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Digital Forensics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/JSTSP.2020.3007250" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="63">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="63">
            <h2>GANprintR: Improved Fakes and Evaluation of the State of the Art in Face Manipulation Detection</h2>
            <p class="authors">João C. Neves, Ruben Tolosana, Ruben Vera-Rodriguez, Vasco Lopes, Hugo Proença, Julian Fierrez</p>
            <p class="venue">IEEE Journal of Selected Topics in Signal Processing (2020)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The availability of large-scale facial databases, together with the remarkable progresses of deep learning technologies, in particular Generative Adversarial Networks (GANs), have led to the generation of extremely realistic fake facial content, raising obvious concerns about the potential for misuse. Such concerns have fostered the research on manipulation detection methods that, contrary to humans, have already achieved astonishing results in various scenarios. This chapter is focused on the analysis of GAN fingerprints in face image synthesis. In particular, it covers an in-depth literature analysis of state-of-the-art detection approaches for the entire face synthesis manipulation. It also describes a recent approach to spoof fake detectors based on a GAN-fingerprint Removal autoencoder (GANprintR). A thorough experimental framework is included in the chapter, highlighting (i) the potential of GANprintR to spoof fake detectors, and (ii) the poor generalisation capability of current fake detectors.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/JSTSP.2020.3007250" target="_blank">10.1109/JSTSP.2020.3007250</a></p>
            
            <div class="links">
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2020" 
         data-type="journal"
         data-title="an attention-based deep learning model for multiple pedestrian attributes recognition"
         data-authors="ehsan yaghoubi, diana borza, joão c. neves, sv aruna kumar, hugo proença"
         data-venue="elsevier image and vision computing"
         data-tags="pedestrian attributes recognition,attention mechanism,deep learning,multi-task learning"
         data-id="64">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/ivc.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">An Attention-Based Deep Learning Model for Multiple Pedestrian Attributes Recognition</h3>
                <p class="text-gray-600 mb-2">Ehsan Yaghoubi, Diana Borza, João C. Neves, SV Aruna Kumar, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Image and Vision Computing</span> (2020)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Pedestrian Attributes Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Attention Mechanism</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Deep Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Multi-task Learning</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.imavis.2020.103981" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="64">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="64">
            <h2>An Attention-Based Deep Learning Model for Multiple Pedestrian Attributes Recognition</h2>
            <p class="authors">Ehsan Yaghoubi, Diana Borza, João C. Neves, SV Aruna Kumar, Hugo Proença</p>
            <p class="venue">Elsevier Image and Vision Computing (2020)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The automatic characterization of pedestrians in surveillance footage is a tough challenge, particularly when the data is extremely diverse with cluttered backgrounds, and subjects are captured from varying distances, under multiple poses, with partial occlusion. Having observed that the state-of-the-art performance is still unsatisfactory, this paper provides a novel solution to the problem, with two-fold contributions: 1) considering the strong semantic correlation between the different full-body attributes, we propose a multi-task deep model that uses an element-wise multiplication layer to extract more comprehensive feature representations. In practice, this layer serves as a filter to remove irrelevant background features, and is particularly important to handle complex, cluttered data; and 2) we introduce a weighted-sum term to the loss function that not only relativizes the contribution of each task but also is crucial for performance improvement in multiple-attribute inference settings. Our experiments were performed on two well-known datasets (RAP and PETA) and point for the superiority of the proposed method with respect to the state-of-the-art.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.imavis.2020.103981" target="_blank">10.1016/j.imavis.2020.103981</a></p>
            
            <div class="links">
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2020" 
         data-type="journal"
         data-title="human attribute recognition: a comprehensive survey"
         data-authors="ehsan yaghoubi, farhad khezeli, diana borza, sv aruna kumar, joão c. neves, hugo proença"
         data-venue="mdpi applied sciences"
         data-tags="human attribute recognition,survey,computer vision,pattern recognition"
         data-id="65">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/as.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Human Attribute Recognition: A Comprehensive Survey</h3>
                <p class="text-gray-600 mb-2">Ehsan Yaghoubi, Farhad Khezeli, Diana Borza, SV Aruna Kumar, João C. Neves, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">MDPI Applied Sciences</span> (2020)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Human Attribute Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Survey</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Computer Vision</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Pattern Recognition</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.3390/app10165608" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Ehsan_MDPI_AS.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="65">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="65">
            <h2>Human Attribute Recognition: A Comprehensive Survey</h2>
            <p class="authors">Ehsan Yaghoubi, Farhad Khezeli, Diana Borza, SV Aruna Kumar, João C. Neves, Hugo Proença</p>
            <p class="venue">MDPI Applied Sciences (2020)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Human Attribute Recognition (HAR) is a highly active research field in computer vision and pattern recognition domains with various applications such as surveillance or fashion. Several approaches have been proposed to tackle the particular challenges in HAR. However, these approaches have dramatically changed over the last decade, mainly due to the improvements brought by deep learning solutions. To provide insights for future algorithm design and dataset collections, in this survey, (1) we provide an in-depth analysis of existing HAR techniques, concerning the advances proposed to address the HAR's main challenges; (2) we provide a comprehensive discussion over the publicly available datasets for the development and evaluation of novel HAR approaches; (3) we outline the applications and typical evaluation metrics used in the HAR context.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.3390/app10165608" target="_blank">10.3390/app10165608</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Ehsan_MDPI_AS.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2020" 
         data-type="conference"
         data-title="unconstrained periocular recognition: using generative deep learning frameworks for attribute normalization"
         data-authors="luiz a. zanlorensi, hugo proença, david menotti"
         data-venue="proceedings of the ieee international conference on image processing – icip 2020"
         data-tags="periocular recognition,attribute normalization,generative deep learning,unconstrained environments"
         data-id="66">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/springer.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Unconstrained Periocular Recognition: Using Generative Deep Learning Frameworks for Attribute Normalization</h3>
                <p class="text-gray-600 mb-2">Luiz A. Zanlorensi, Hugo Proença, David Menotti</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the IEEE International Conference on Image Processing – ICIP 2020</span> (2020)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Attribute Normalization</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Generative Deep Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Unconstrained Environments</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/ICIP40778.2020.9191251" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/icip2020.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="66">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="66">
            <h2>Unconstrained Periocular Recognition: Using Generative Deep Learning Frameworks for Attribute Normalization</h2>
            <p class="authors">Luiz A. Zanlorensi, Hugo Proença, David Menotti</p>
            <p class="venue">Proceedings of the IEEE International Conference on Image Processing – ICIP 2020 (2020)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Ocular biometric systems working in unconstrained environments usually face the problem of small within-class compactness caused by the multiple factors that jointly degrade the quality of the obtained data. In this work, we propose an attribute normalization strategy based on deep learning generative frameworks, that reduces the variability of the samples used in pairwise comparisons, without reducing their discriminability. The proposed method can be seen as a preprocessing step that contributes for data regularization and improves the recognition accuracy, being fully agnostic to the recognition strategy used. As proof of concept, we consider the 'eyeglasses' and 'gaze' factors, comparing the levels of performance of five different recognition methods with/without using the proposed normalization strategy. Also, we introduce a new dataset for unconstrained periocular recognition, composed of images acquired by mobile devices, particularly suited to perceive the impact of 'wearing eyeglasses' in recognition effectiveness. Our experiments were performed in two different datasets, and support the usefulness of our attribute normalization scheme to improve the recognition performance.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/ICIP40778.2020.9191251" target="_blank">10.1109/ICIP40778.2020.9191251</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/icip2020.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2019" 
         data-type="conference"
         data-title="region-based cnns for pedestrian gender recognition in visual surveillance environments"
         data-authors="ehsan yaghoubi, pendar alirezazadeh, eduardo assunção, joão c. neves, hugo proença"
         data-venue="proceedings of the 18th international conference of the biometrics special interest group – biosig 2019"
         data-tags="gender recognition,visual surveillance,region-based cnn,soft biometrics"
         data-id="67">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/springer.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Region-Based CNNs for Pedestrian Gender Recognition in Visual Surveillance Environments</h3>
                <p class="text-gray-600 mb-2">Ehsan Yaghoubi, Pendar Alirezazadeh, Eduardo Assunção, João C. Neves, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the 18th International Conference of the Biometrics Special Interest Group – BIOSIG 2019</span> (2019)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Gender Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visual Surveillance</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Region-Based CNN</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Soft Biometrics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/biosig2019_ehsan.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="67">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="67">
            <h2>Region-Based CNNs for Pedestrian Gender Recognition in Visual Surveillance Environments</h2>
            <p class="authors">Ehsan Yaghoubi, Pendar Alirezazadeh, Eduardo Assunção, João C. Neves, Hugo Proença</p>
            <p class="venue">Proceedings of the 18th International Conference of the Biometrics Special Interest Group – BIOSIG 2019 (2019)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Inferring soft biometric labels in totally uncontrolled outdoor environments, such as surveillance scenarios, remains a challenge due to the low resolution of data and its covariates that might seriously compromise performance (e.g., occlusions and subjects pose). In this kind of data, even state-of-the-art deep-learning frameworks (such as ResNet) working in a holistic way, attain relatively poor performance, which was the main motivation for the work described in this paper. In particular, having noticed the main effect of the subjects' 'pose' factor, in this paper we describe a method that uses the body keypoints to estimate the subjects pose and define a set of regions of interest (e.g., head, torso, and legs). This information is used to learn appropriate classification models, specialized in different poses/body parts, which contributes to solid improvements in performance. This conclusion is supported by the experiments we conducted in multiple real-world outdoor scenarios, using the data acquired from advertising panels placed in crowded urban environments.</p>
            </div>
            
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/biosig2019_ehsan.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2019" 
         data-type="journal"
         data-title="deep representations for cross-spectral ocular biometrics"
         data-authors="luiz a. zanlorensi, diego r. lucio, alceu s. britto jr., hugo proença, david menotti"
         data-venue="iet biometrics"
         data-tags="cross-spectral biometrics,ocular recognition,deep learning,convolutional neural networks"
         data-id="68">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/ietbiometrics.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Deep representations for cross-spectral ocular biometrics</h3>
                <p class="text-gray-600 mb-2">Luiz A. Zanlorensi, Diego R. Lucio, Alceu S. Britto Jr., Hugo Proença, David Menotti</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IET Biometrics</span> (2019)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Cross-spectral Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Ocular Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Deep Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Convolutional Neural Networks</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1049/iet-bmt.2019.0116" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Deep_IET.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="68">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="68">
            <h2>Deep representations for cross-spectral ocular biometrics</h2>
            <p class="authors">Luiz A. Zanlorensi, Diego R. Lucio, Alceu S. Britto Jr., Hugo Proença, David Menotti</p>
            <p class="venue">IET Biometrics (2019)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>One of the major challenges in ocular biometrics is the cross-spectral scenario, i.e., how to match images acquired in different wavelengths (typically visible (VIS) against near-infrared (NIR)). This article designs and extensively evaluates cross-spectral ocular verification methods, for both the closed and open-world settings, using well known deep learning representations based on the iris and periocular regions. Using as inputs the bounding boxes of non-normalized iris/periocular regions, we fine-tune Convolutional Neural Network (CNN) models (based either on VGG16 or ResNet-50 architectures), originally trained for face recognition. Based on the experiments carried out in two publicly available cross-spectral ocular databases, we report results for intra-spectral and cross-spectral scenarios, with the best performance being observed when fusing ResNet-50 deep representations from both the periocular and iris regions. When compared to the state-of-the-art, we observed that the proposed solution consistently reduces the Equal Error Rate (EER) values by 90% / 93% / 96% and 61% / 77% / 83% on the cross-spectral scenario and in the PolyU Bi-spectral and Cross-eye-cross-spectral datasets.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1049/iet-bmt.2019.0116" target="_blank">10.1049/iet-bmt.2019.0116</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Deep_IET.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2019" 
         data-type="book chapter"
         data-title="miche competitions: a realistic experience with uncontrolled eye region acquisition"
         data-authors="silvio barra, maria de marsico, hugo proença, michele nappi"
         data-venue="selfie biometrics, springer verlag book series"
         data-tags="miche competition,eye region acquisition,uncontrolled environments,mobile biometrics"
         data-id="69">
        <div class="flex items-start">
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">MICHE competitions: a realistic experience with uncontrolled eye region acquisition</h3>
                <p class="text-gray-600 mb-2">Silvio Barra, Maria De Marsico, Hugo Proença, Michele Nappi</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Selfie Biometrics, Springer Verlag book series</span> (2019)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge book chapter">Book chapter</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">MICHE Competition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Eye Region Acquisition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Uncontrolled Environments</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Mobile Biometrics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/BC_MICHE.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="69">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="69">
            <h2>MICHE competitions: a realistic experience with uncontrolled eye region acquisition</h2>
            <p class="authors">Silvio Barra, Maria De Marsico, Hugo Proença, Michele Nappi</p>
            <p class="venue">Selfie Biometrics, Springer Verlag book series (2019)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Any user in the world that has to access a protected service or location, or that simply wants to protect its owned devices, has to struggle with assuring a secure access to them. This is a first aspect that characterizes self-handled authentication strategies. Actually, the use of special signs, objects or passphrases goes back to the very origins of human communities. Watchwords asked by sentinels, or the 5-pointed pentagon tattooed on the palm of members of the Pythagorean school are examples of a kind of authentication often seen in the literature. The first attempt to use computer support for authentication is represented by passwords, that first appeared at the Massachusetts Institute of Technology in the mid-1960s, where a massive compatible time-sharing computer (CTSS) was used to pioneer many of the milestones of computing, including password-based authentication. In those times, a single password was sufficient to access one's virtual space and files, which afterall were the only resources to protect. Afterwards and beyond any forecasting, computers massively entered every-day life, with Internet allowing the creation of an increasing number of remote services of various kinds.</p>
            </div>
            
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/BC_MICHE.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2019" 
         data-type="conference"
         data-title="facegenderid: exploiting gender information in dcnns face recognition systems"
         data-authors="marta blásquez, aythami morales, ester gonzalez, joão c. neves, hugo proença, rúben vera-rodriguez"
         data-venue="proceedings of the ieee computer society conference on computer vision and pattern recognition 'biometrics' workshop – cvprw 2019"
         data-tags="face recognition,gender bias,dcnns,biometrics"
         data-id="70">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/cvf.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">FaceGenderID: Exploiting Gender Information in DCNNs Face Recognition Systems</h3>
                <p class="text-gray-600 mb-2">Marta Blásquez, Aythami Morales, Ester Gonzalez, João C. Neves, Hugo Proença, Rúben Vera-Rodriguez</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 'Biometrics' Workshop – CVPRW 2019</span> (2019)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Face Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Gender Bias</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">DCNNs</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/CVPRW.2019.00278" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/cvprw_2019.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="70">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="70">
            <h2>FaceGenderID: Exploiting Gender Information in DCNNs Face Recognition Systems</h2>
            <p class="authors">Marta Blásquez, Aythami Morales, Ester Gonzalez, João C. Neves, Hugo Proença, Rúben Vera-Rodriguez</p>
            <p class="venue">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 'Biometrics' Workshop – CVPRW 2019 (2019)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>This paper addresses the effect of gender as a covariate in face verification systems. Even though pre-trained models based on Deep Convolutional Neural Networks (DCNNs), such as VGG-Face or ResNet-50, achieve very high performance, they are trained on very large datasets comprising millions of images, which have biases regarding demographic aspects like the gender and the ethnicity among others. In this work, we first analyse the separate performance of these state-of-the-art models for males and females. We observe a gap between face verification performances obtained by both gender classes. These results suggest that features obtained by biased models are affected by the gender covariate. We propose a gender-dependent training approach to improve the feature representation for both genders, and develop both: i) gender specific DCNNs models, and ii) a gender balanced DCNNs model. Our results show significant and consistent improvements in face verification performance for both genders, individually and in general with our proposed approach. Finally, we announce the availability (at GitHub) of the FaceGenderID DCNNs models proposed in this work, which can support further experiments on this topic.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/CVPRW.2019.00278" target="_blank">10.1109/CVPRW.2019.00278</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/cvprw_2019.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2019" 
         data-type="conference"
         data-title="segmentation-less and non-holistic deep-learning framework for iris recognition"
         data-authors="hugo proença, joão c. neves"
         data-venue="proceedings of the ieee computer society conference on computer vision and pattern recognition 'bias estimation in face analytics' workshop, – cvprw 2019"
         data-tags="iris recognition,segmentation-less,deep learning,biometrics"
         data-id="71">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/cvf.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Segmentation-less and Non-holistic Deep-Learning Framework for Iris Recognition</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, João C. Neves</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 'Bias Estimation in Face Analytics' Workshop, – CVPRW 2019</span> (2019)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Segmentation-less</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Deep Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/CVPRW.2019.00283" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/CVPRW19.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="71">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="71">
            <h2>Segmentation-less and Non-holistic Deep-Learning Framework for Iris Recognition</h2>
            <p class="authors">Hugo Proença, João C. Neves</p>
            <p class="venue">Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 'Bias Estimation in Face Analytics' Workshop, – CVPRW 2019 (2019)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Driven by the pioneer iris biometrics approach, the most relevant recognition methods published over the years are 'phase-based', and segment/normalize the iris to obtain dimensionless representations of the data that attenuate the differences in scale, translation, rotation and pupillary dilation. In this paper we present a recognition method that dispenses the iris segmentation, noise detection and normalization phases, and is agnostic to the levels of pupillary dilation, while maintaining state-of-the-art performance. Based on deep-learning classification models, we analyze the displacements between biologically corresponding patches in pairs of iris images, to discriminate between genuine and impostor comparisons. Such corresponding patches are firstly learned in the normalized representations of the irises - the domain where they are optimally distinguishable - but are remapped into a segmentation-less polar coordinate system that uniquely requires iris detection. In recognition time, samples are only converted into this segmentation-less coordinate system, where matching is performed. In the experiments, we considered the challenging open-world setting, and used three well known data sets (CASIA-4-Lamp, CASIA-4-Thousand and WVU), concluding positively about the effectiveness of the proposed algorithm, particularly in cases where accurately segmenting the iris is a challenge.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/CVPRW.2019.00283" target="_blank">10.1109/CVPRW.2019.00283</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/CVPRW19.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2019" 
         data-type="journal"
         data-title="a reminiscence of "mastermind": iris/periocular biometrics by "in-set" cnn iterative analysis"
         data-authors="hugo proença, joão c. neves"
         data-venue="ieee transactions on information forensics and security"
         data-tags="iris recognition,periocular biometrics,convolutional neural networks,iterative analysis"
         data-id="72">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/tifs.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">A Reminiscence of "Mastermind": Iris/Periocular Biometrics by "In-Set" CNN Iterative Analysis</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, João C. Neves</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Information Forensics and Security</span> (2019)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Convolutional Neural Networks</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iterative Analysis</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIFS.2018.2883853" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Mastermind_TIFS.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="72">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="72">
            <h2>A Reminiscence of "Mastermind": Iris/Periocular Biometrics by "In-Set" CNN Iterative Analysis</h2>
            <p class="authors">Hugo Proença, João C. Neves</p>
            <p class="venue">IEEE Transactions on Information Forensics and Security (2019)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Convolutional neural networks (CNNs) have emerged as the most popular classification models in biometrics research. Under the discriminative paradigm of pattern recognition, CNNs are used typically in one of two ways: 1) verification mode ('are samples from the same person?'), where pairs of images are provided to the network to distinguish between genuine and impostor instances; and 2) identification mode ('whom is this sample from?'), where appropriate feature representations that map images to identities are found. This paper postulates a novel mode for using CNNs in biometric identification, by learning models that answer to the question 'is the query's identity among this set?'. The insight is a reminiscence of the classical Mastermind game: by iteratively analysing the network responses when multiple random samples of k gallery elements are compared to the query, we obtain weakly correlated matching scores that - altogether - provide solid cues to infer the most likely identity. In this setting, identification is regarded as a variable selection and regularization problem, with sparse linear regression techniques being used to infer the matching probability with respect to each gallery identity. As main strength, this strategy is highly robust to outlier matching scores, which are known to be a primary error source in biometric recognition.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIFS.2018.2883853" target="_blank">10.1109/TIFS.2018.2883853</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Mastermind_TIFS.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2019" 
         data-type="conference"
         data-title="pose switch-based convolutional neural network for clothing analysis in visual surveillance environments"
         data-authors="pendar alirezazadeh, ehsan yaghoubi, eduardo assunção, joão c. neves, hugo proença"
         data-venue="proceedings of the 18th international conference of the biometrics special interest group – biosig 2019"
         data-tags="clothing analysis,visual surveillance,pose estimation,cnn"
         data-id="73">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/ieeexplore.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Pose Switch-Based Convolutional Neural Network for Clothing Analysis in Visual Surveillance Environments</h3>
                <p class="text-gray-600 mb-2">Pendar Alirezazadeh, Ehsan Yaghoubi, Eduardo Assunção, João C. Neves, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the 18th International Conference of the Biometrics Special Interest Group – BIOSIG 2019</span> (2019)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Clothing Analysis</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visual Surveillance</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Pose Estimation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">CNN</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/biosig2019_pendar.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="73">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="73">
            <h2>Pose Switch-Based Convolutional Neural Network for Clothing Analysis in Visual Surveillance Environments</h2>
            <p class="authors">Pendar Alirezazadeh, Ehsan Yaghoubi, Eduardo Assunção, João C. Neves, Hugo Proença</p>
            <p class="venue">Proceedings of the 18th International Conference of the Biometrics Special Interest Group – BIOSIG 2019 (2019)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Recognizing pedestrian clothing types and styles in outdoor scenes and totally uncontrolled conditions is appealing to emerging applications such as security, intelligent customer profile analysis and computer-aided fashion design. Recognition of clothing categories from videos remains a challenge, mainly due to the poor data resolution and the data covariates that compromise the effectiveness of automated image analysis techniques (e.g., poses, shadows and partial occlusions). While state-of-the-art methods typically analyze clothing attributes without paying attention to variation of human poses, here we claim for the importance of a feature representation derived from human poses to improve classification rate. Estimating the pose of pedestrians is important to fed guided features into recognizing system. In this paper, we introduce pose switch-based convolutional neural network for recognizing the types of clothes of pedestrians, using data acquired in crowded urban environments. In particular, we compare the effectiveness attained when using CNNs without respect to human poses variant, and assess the improvements in performance attained by pose feature extraction. The observed results enable us to conclude that pose information can improve the performance of clothing recognition system. We focus on the key role of pose information in pedestrian clothing analysis, which can be employed as an interesting topic for further works.</p>
            </div>
            
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/biosig2019_pendar.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2019" 
         data-type="journal"
         data-title="visual surveillance and biometrics: practices, challenges, and possibilities"
         data-authors="sambit bakshi, guodong guo, hugo proença, massimo tistarelli"
         data-venue="ieee access"
         data-tags="visual surveillance,biometrics,security systems,privacy"
         data-id="74">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/ieeeaccess.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Visual Surveillance and Biometrics: Practices, Challenges, and Possibilities</h3>
                <p class="text-gray-600 mb-2">Sambit Bakshi, Guodong Guo, Hugo Proença, Massimo Tistarelli</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Access</span> (2019)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visual Surveillance</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Security Systems</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Privacy</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/ACCESS.2019.2940175" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/IEEE_Access_SI_2019.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="74">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="74">
            <h2>Visual Surveillance and Biometrics: Practices, Challenges, and Possibilities</h2>
            <p class="authors">Sambit Bakshi, Guodong Guo, Hugo Proença, Massimo Tistarelli</p>
            <p class="venue">IEEE Access (2019)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Visual surveillance is the latest paradigm for social security through machine intelligence. It includes the use of visual data captured by infrared sensors or visible-light cameras mounted in cars, corridors, traffic signals etc. Visual surveillance facilitates the classification of human behavior, crowd activity, and gesture analysis to achieve application-specific objectives. Biometrics is the science of uniquely identifying or verifying an individual among a set of people by exploring the user's physiological or behavioral characteristics. Due to their ease of use in many application scenarios (including time attendance systems, border control, access control for high security, etc.), biometric systems are currently being introduced in many everyday activities. In the past, some solutions developed for visual surveillance systems have also been applied for biometric identification. Recently, various research efforts have been devoted to merge these two technologies, especially for adverse and covert scenarios. This Special Section, 'Visual Surveillance and Biometrics: Practices, Challenges, and Possibilities,' serves as a cross-platform to cover the recent advances at the intersection of 'visual surveillance' and 'biometrics.' It contains 20 cutting-edge research articles by leading researchers from more than fifteen countries, discussing the current challenges and possible solutions in both fields.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/ACCESS.2019.2940175" target="_blank">10.1109/ACCESS.2019.2940175</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/IEEE_Access_SI_2019.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2019" 
         data-type="special issue"
         data-title="novel insights on ocular biometrics"
         data-authors="sambit bakshi, abhijit das, maria de marsico, hugo proença"
         data-venue="elsevier image and vision computing"
         data-tags="ocular biometrics,iris recognition,periocular recognition,special issue"
         data-id="75">
        <div class="flex items-start">
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Novel Insights on Ocular Biometrics</h3>
                <p class="text-gray-600 mb-2">Sambit Bakshi, Abhijit Das, Maria De Marsico, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Image and Vision Computing</span> (2019)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge special issue">Special issue</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Ocular Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Special Issue</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="75">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="75">
            <h2>Novel Insights on Ocular Biometrics</h2>
            <p class="authors">Sambit Bakshi, Abhijit Das, Maria De Marsico, Hugo Proença</p>
            <p class="venue">Elsevier Image and Vision Computing (2019)</p>
            
            
            <div class="links">
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2018" 
         data-type="journal"
         data-title="a leopard cannot change its spots: improving face recognition using 3d-based caricatures"
         data-authors="joão c. neves, hugo proença"
         data-venue="ieee transactions on information forensics and security"
         data-tags="face recognition,caricatures,3d modeling"
         data-id="76">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-96.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">A Leopard Cannot Change Its Spots: Improving Face Recognition Using 3D-based Caricatures</h3>
                <p class="text-gray-600 mb-2">João C. Neves, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Information Forensics and Security</span> (2018)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Face Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Caricatures</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">3D Modeling</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIFS.2018.2846617" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Leopard_TIFS.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="76">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="76">
            <h2>A Leopard Cannot Change Its Spots: Improving Face Recognition Using 3D-based Caricatures</h2>
            <p class="authors">João C. Neves, Hugo Proença</p>
            <p class="venue">IEEE Transactions on Information Forensics and Security (2018)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Caricatures refer to a representation of a person in which the distinctive features are deliberately exaggerated, with several studies showing that humans perform better at recognizing people from caricatures than using original images. Inspired by this observation, this paper introduces the first fully automated caricature-based face recognition approach capable of working with data acquired in the wild. Our approach leverages the 3D face structure from a single 2D image and compares it to a reference model for obtaining a compact representation of face features deviations. This descriptor is subsequently deformed using a 'measure locally, weight globally' strategy to resemble the caricature drawing process. The deformed deviations are incorporated in the 3D model using the Laplacian mesh deformation algorithm, and the 2D face caricature image is obtained by projecting the deformed model in the original camera-view. To demonstrate the advantages of caricature-based face recognition, we train the VGG-Face network from scratch using either original face images (baseline) or caricatured images, and use these models for extracting face descriptors from the LFW, IJB-A and MegaFace datasets. The experiments show an increase in the recognition accuracy when using caricatures rather than original images. Moreover, our approach achieves competitive results with state-of-the-art face recognition methods, even without explicitly tuning the network for any of the evaluation sets.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIFS.2018.2846617" target="_blank">10.1109/TIFS.2018.2846617</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Leopard_TIFS.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2018" 
         data-type="conference"
         data-title="biometric recognition in surveillance environments using master-slave architectures"
         data-authors="hugo proença, joão c. neves"
         data-venue="proceedings of the 31st conference on graphics, patterns and images- sibgrapi 2018"
         data-tags="biometric recognition,surveillance,master-slave architecture,ptz camera"
         data-id="77">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/ieeexplore.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Biometric Recognition in Surveillance Environments Using Master-Slave Architectures</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, João C. Neves</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the 31st Conference on Graphics, Patterns and Images- SIBGRAPI 2018</span> (2018)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Surveillance</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Master-Slave Architecture</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">PTZ Camera</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/SIBGRAPI.2018.00068" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/SIBGRAPI2018.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="77">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="77">
            <h2>Biometric Recognition in Surveillance Environments Using Master-Slave Architectures</h2>
            <p class="authors">Hugo Proença, João C. Neves</p>
            <p class="venue">Proceedings of the 31st Conference on Graphics, Patterns and Images- SIBGRAPI 2018 (2018)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The number of visual surveillance systems deployed worldwide has been growing astoundingly. As a result, attempts have been made to increase the levels of automated analysis of such systems, towards the reliable recognition of human beings in fully covert conditions. Among other possibilities, master-slave architectures can be used to acquire high resolution data of subjects heads from large distances, with enough resolution to perform face recognition. This paper/tutorial provides a comprehensive overview of the major phases behind the development of a recognition system working in outdoor surveillance scenarios, describing frameworks and methods to: 1) use coupled wide view and Pan-Tilt-Zoom (PTZ) imaging devices in surveillance settings, with a wide-view camera covering the whole scene, while a synchronized PTZ device collects high-resolution data from the head region; 2) use soft biometric information (e.g., body metrology and gait) for pruning the set of potential identities for each query; and 3) faithfully balance ethics/privacy and safety/security issues in this kind of systems.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/SIBGRAPI.2018.00068" target="_blank">10.1109/SIBGRAPI.2018.00068</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/SIBGRAPI2018.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2018" 
         data-type="special issue"
         data-title="visual surveillance and biometrics: practices, challenges, and possibilities"
         data-authors="sambit bakshi, guodong guo, hugo proença, massimo tistarelli"
         data-venue="ieee access"
         data-tags="visual surveillance,biometrics,special issue,challenges"
         data-id="78">
        <div class="flex items-start">
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Visual Surveillance and Biometrics: Practices, Challenges, and Possibilities</h3>
                <p class="text-gray-600 mb-2">Sambit Bakshi, Guodong Guo, Hugo Proença, Massimo Tistarelli</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Access</span> (2018)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge special issue">Special issue</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visual Surveillance</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Special Issue</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Challenges</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="78">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="78">
            <h2>Visual Surveillance and Biometrics: Practices, Challenges, and Possibilities</h2>
            <p class="authors">Sambit Bakshi, Guodong Guo, Hugo Proença, Massimo Tistarelli</p>
            <p class="venue">IEEE Access (2018)</p>
            
            
            <div class="links">
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2018" 
         data-type="journal"
         data-title=""a leopard cannot change its spots": improving face recognition using 3d-based caricatures"
         data-authors="joão c. neves, hugo proença"
         data-venue="ieee transactions on information forensics and security"
         data-tags="face recognition,3d caricatures,deep learning,biometrics"
         data-id="79">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/tifs.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">"A Leopard Cannot Change Its Spots": Improving Face Recognition Using 3D-based Caricatures</h3>
                <p class="text-gray-600 mb-2">João C. Neves, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Information Forensics and Security</span> (2018)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Face Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">3D Caricatures</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Deep Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIFS.2018.2846617" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Leopard_TIFS.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="79">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="79">
            <h2>"A Leopard Cannot Change Its Spots": Improving Face Recognition Using 3D-based Caricatures</h2>
            <p class="authors">João C. Neves, Hugo Proença</p>
            <p class="venue">IEEE Transactions on Information Forensics and Security (2018)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Caricatures refer to a representation of a person in which the distinctive features are deliberately exaggerated, with several studies showing that humans perform better at recognizing people from caricatures than using original images. Inspired by this observation, this paper introduces the first fully automated caricature-based face recognition approach capable of working with data acquired in the wild. Our approach leverages the 3D face structure from a single 2D image and compares it to a reference model for obtaining a compact representation of face features deviations. This descriptor is subsequently deformed using a 'measure locally, weight globally' strategy to resemble the caricature drawing process. The deformed deviations are incorporated in the 3D model using the Laplacian mesh deformation algorithm, and the 2D face caricature image is obtained by projecting the deformed model in the original camera-view. To demonstrate the advantages of caricature-based face recognition, we train the VGG-Face network from scratch using either original face images (baseline) or caricatured images, and use these models for extracting face descriptors from the LFW, IJB-A and MegaFace datasets. The experiments show an increase in the recognition accuracy when using caricatures rather than original images. Moreover, our approach achieves competitive results with state-of-the-art face recognition methods, even without explicitly tuning the network for any of the evaluation sets.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIFS.2018.2846617" target="_blank">10.1109/TIFS.2018.2846617</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Leopard_TIFS.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2018" 
         data-type="journal"
         data-title="experiments with ocular biometric datasets: a practitioner's guideline"
         data-authors="zahid akhtar, gautam kumar, sambit bakshi, hugo proença"
         data-venue="it professional"
         data-tags="ocular biometrics,databases,guidelines,experimental protocols"
         data-id="80">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/itprofessional.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Experiments with Ocular Biometric Datasets: A Practitioner's Guideline</h3>
                <p class="text-gray-600 mb-2">Zahid Akhtar, Gautam Kumar, Sambit Bakshi, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IT Professional</span> (2018)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Ocular Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Databases</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Guidelines</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Experimental Protocols</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/MITP.2018.032501748" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ITProfessional_Akhtar.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="80">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="80">
            <h2>Experiments with Ocular Biometric Datasets: A Practitioner's Guideline</h2>
            <p class="authors">Zahid Akhtar, Gautam Kumar, Sambit Bakshi, Hugo Proença</p>
            <p class="venue">IT Professional (2018)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Ocular biometrics is the imaging and use of features extracted from the eyes regions for personal recognition. Ocular biometrics is a promising research field owing to factors such as recognition at a distance and suitability for recognition with regular RGB cameras, especially in visible spectrum on mobile devices. To ensure that ocular biometric academic researches have a positive impact on future technological developments, this paper provides a review of ocular databases available in literature, diversities among these databases, design and parameters consideration issues during acquisition of database and selection of appropriate database for experimentation. Open issues and future research directions are also discussed to identify the path forward.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/MITP.2018.032501748" target="_blank">10.1109/MITP.2018.032501748</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ITProfessional_Akhtar.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2018" 
         data-type="journal"
         data-title="ieee intelligent systems: trends and controversies"
         data-authors="hugo proença, mark nixon, michele nappi"
         data-venue="ieee intelligent systems"
         data-tags="biometric recognition,surveillance,trends and controversies,special issue"
         data-id="81">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/is.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">IEEE Intelligent Systems: Trends and Controversies</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Mark Nixon, Michele Nappi</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Intelligent Systems</span> (2018)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Surveillance</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Trends and Controversies</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Special Issue</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/MIS.2018.033001416" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/mex201803.issue.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="81">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="81">
            <h2>IEEE Intelligent Systems: Trends and Controversies</h2>
            <p class="authors">Hugo Proença, Mark Nixon, Michele Nappi</p>
            <p class="venue">IEEE Intelligent Systems (2018)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Performing covert biometric recognition in surveillance environments has been regarded as a 'grand' challenge, considering the adversity of the conditions where recognition should be carried out (e.g., poor resolution, bad lighting, off-pose and partially occluded data). This special issue compiles a group of approaches to this problem.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/MIS.2018.033001416" target="_blank">10.1109/MIS.2018.033001416</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/mex201803.issue.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2018" 
         data-type="journal"
         data-title="deep-prwis: periocular recognition without the iris and sclera using deep learning frameworks"
         data-authors="hugo proença, joão c. neves"
         data-venue="ieee transactions on information forensics and security"
         data-tags="periocular recognition,deep learning,data augmentation,biometrics"
         data-id="82">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/tifs.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Deep-PRWIS: Periocular Recognition Without the Iris and Sclera Using Deep Learning Frameworks</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, João C. Neves</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Information Forensics and Security</span> (2018)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Deep Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Data Augmentation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIFS.2017.2771230" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Deep-PRWIS.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="82">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="82">
            <h2>Deep-PRWIS: Periocular Recognition Without the Iris and Sclera Using Deep Learning Frameworks</h2>
            <p class="authors">Hugo Proença, João C. Neves</p>
            <p class="venue">IEEE Transactions on Information Forensics and Security (2018)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>This work is based on a disruptive hypothesis for periocular biometrics: in visible-light data, the recognition performance is optimized when the components inside the ocular globe (the iris and the sclera) are simply discarded, and the recogniser's response is exclusively based in information from the surroundings of the eye. As major novelty, we describe a processing chain based on convolution neural networks (CNNs) that defines the regions-of-interest in the input data that should be privileged in an implicit way, i.e., without masking out any areas in the learning/test samples. By using an ocular segmentation algorithm exclusively in the learning data, we separate the ocular from the periocular parts. Then, we produce a large set of 'multi-class' artificial samples, by interchanging the periocular and ocular parts from different subjects. These samples are used for data augmentation purposes and feed the learning phase of the CNN, always considering as label the ID of the periocular part. This way, for every periocular region, the CNN receives multiple samples of different ocular classes, forcing it to conclude that such regions should not be considered in its response. During the test phase, samples are provided without any segmentation mask and the network naturally disregards the ocular components, which contributes for improvements in performance. Our experiments were carried out in full versions of two widely known data sets (UBIRIS.v2 and FRGC) and show that the proposed method consistently advances the state-of-the-art performance in the closed-world setting, reducing the EERs in about 82% (UBIRIS.v2) and 85% (FRGC) and improving the Rank-1 over 41% (UBIRIS.v2) and 12% (FRGC).</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIFS.2017.2771230" target="_blank">10.1109/TIFS.2017.2771230</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Deep-PRWIS.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2018" 
         data-type="journal"
         data-title="deep-prwis: periocular recognition without the iris and sclera using deep learning frameworks"
         data-authors="hugo proença, joão c. neves"
         data-venue="ieee transactions on information forensics and security"
         data-tags="periocular recognition,deep learning,biometrics,ocular recognition,data augmentation"
         data-id="83">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/tifs.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Deep-PRWIS: Periocular Recognition Without the Iris and Sclera Using Deep Learning Frameworks</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, João C. Neves</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Information Forensics and Security</span> (2018)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Deep Learning</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Ocular Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Data Augmentation</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIFS.2017.2771230" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Deep-PRWIS.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="83">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="83">
            <h2>Deep-PRWIS: Periocular Recognition Without the Iris and Sclera Using Deep Learning Frameworks</h2>
            <p class="authors">Hugo Proença, João C. Neves</p>
            <p class="venue">IEEE Transactions on Information Forensics and Security (2018)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>This work is based on a disruptive hypothesis for periocular biometrics: in visible-light data, the recognition performance is optimized when the components inside the ocular globe (the iris and the sclera) are simply discarded, and the recogniser's response is exclusively based in information from the surroundings of the eye. As major novelty, we describe a processing chain based on convolution neural networks (CNNs) that defines the regions-of-interest in the input data that should be privileged in an implicit way, i.e., without masking out any areas in the learning/test samples. By using an ocular segmentation algorithm exclusively in the learning data, we separate the ocular from the periocular parts. Then, we produce a large set of 'multi-class' artificial samples, by interchanging the periocular and ocular parts from different subjects. These samples are used for data augmentation purposes and feed the learning phase of the CNN, always considering as label the ID of the periocular part. This way, for every periocular region, the CNN receives multiple samples of different ocular classes, forcing it to conclude that such regions should not be considered in its response. During the test phase, samples are provided without any segmentation mask and the network naturally disregards the ocular components, which contributes for improvements in performance. Our experiments were carried out in full versions of two widely known data sets (UBIRIS.v2 and FRGC) and show that the proposed method consistently advances the state-of-the-art performance in the closed-world setting, reducing the EERs in about 82% (UBIRIS.v2) and 85% (FRGC) and improving the Rank-1 over 41% (UBIRIS.v2) and 12% (FRGC).</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIFS.2017.2771230" target="_blank">10.1109/TIFS.2017.2771230</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Deep-PRWIS.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2017" 
         data-type="journal"
         data-title="results from miche ii - mobile iris challenge evaluation ii."
         data-authors="maria de marsico, michele nappi, hugo proença"
         data-venue="elsevier pattern recognition letters"
         data-tags="mobile biometrics,iris recognition,miche"
         data-id="84">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-90.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Results from MICHE II - Mobile Iris CHallenge Evaluation II.</h3>
                <p class="text-gray-600 mb-2">Maria De Marsico, Michele Nappi, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Pattern Recognition Letters</span> (2017)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Mobile Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">MICHE</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.patrec.2016.12.013" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ResultsMICHE2.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="84">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="84">
            <h2>Results from MICHE II - Mobile Iris CHallenge Evaluation II.</h2>
            <p class="authors">Maria De Marsico, Michele Nappi, Hugo Proença</p>
            <p class="venue">Elsevier Pattern Recognition Letters (2017)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Mobile biometrics represent the new frontier of authentication. The most appealing feature of mobile devices is the wide availability and the presence of more and more reliable sensors for capturing bio- metric traits, e.g., cameras and accelerometers. Moreover, they more and more often store personal and sensitive data, that need to be protected. Doing this on the same device using biometrics to enforce secu- rity seems a natural solution. This makes this research topic attracting and generally promising. However, the growing interest for related applications is counterbalanced by still present limitations, especially for some traits. Acquisition and computation resources are nowadays widely available, but they are not al- ways sufficient to allow a reliable recognition result. Most of all, the way capture is expected to be carried out, i.e., by the user him/herself in uncontrolled conditions and without an expert assistance, can heavily affect the quality of samples and, as a consequence, the accuracy of recognition. Among the biometric traits raising the interest of researchers, iris plays an important role. Mobile Iris CHallenge Evaluation II (MICHE II) competition provided a testbed to assess the progress of mobile iris recognition, as well as its limitations still to overcome. This paper presents the results of the competition and the analysis of achieved performance, that takes into account both proposals submitted for the competition section launched at the 2016 edition of the International Conference on Pattern Recognition (ICPR), as well as proposals submitted for this special issue.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.patrec.2016.12.013" target="_blank">10.1016/j.patrec.2016.12.013</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ResultsMICHE2.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2017" 
         data-type="conference"
         data-title="irina: iris recognition (even) in innacurately segmented data"
         data-authors="hugo proença, joão c. neves"
         data-venue="proceedings of the ieee conference on computer vision and pattern recognition (cvpr)"
         data-tags="iris recognition,biometrics,segmentation"
         data-id="85">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-44.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">IRINA: Iris Recognition (even) in Innacurately Segmented Data</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, João C. Neves</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</span> (2017)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Segmentation</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/CVPR.2017.714" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/CVPR2017.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="85">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="85">
            <h2>IRINA: Iris Recognition (even) in Innacurately Segmented Data</h2>
            <p class="authors">Hugo Proença, João C. Neves</p>
            <p class="venue">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The effectiveness of current iris recognition systems depends on the accurate segmentation and parameterisation of the iris boundaries, as failures at this point misalign the coefficients of the biometric signatures. This paper describes IRINA, an algorithm for Iris Recognition that is robust against INAccurately segmented samples, which makes it a good candidate to work in poor-quality data. The process is based in the concept of 'corresponding' patch between pairs of images, that is used to estimate the posterior probabilities that patches regard the same biological region, even in case of segmentation errors and non-linear texture deformations. Such information enables to infer a free-form deformation field (2D registration vectors) between images, whose first and second-order statistics provide effective biometric discriminating power. Extensive experiments were carried out in four datasets (CASIA-IrisV3-Lamp, CASIA-IrisV4-Lamp, CASIA-IrisV4-Thousand and WVU) and show that IRINA not only achieves state-of-the-art performance in good quality data, but also handles effectively severe segmentation errors and large differences in pupillary dilation / constriction.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/CVPR.2017.714" target="_blank">10.1109/CVPR.2017.714</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/CVPR2017.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2017" 
         data-type="journal"
         data-title="soft biometrics: globally coherent solutions for hair segmentation and style recognition based on hierarchical mrfs"
         data-authors="hugo proença, joão c. neves"
         data-venue="ieee transactions on information forensics and security"
         data-tags="soft biometrics,hair segmentation,hair style recognition,markov random fields"
         data-id="86">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/tifs.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Soft Biometrics: Globally Coherent Solutions for Hair Segmentation and Style Recognition based on Hierarchical MRFs</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, João C. Neves</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Information Forensics and Security</span> (2017)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Soft Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Hair Segmentation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Hair Style Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Markov Random Fields</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIFS.2017.2680246" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/HairAnalysis_TIFS.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="86">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="86">
            <h2>Soft Biometrics: Globally Coherent Solutions for Hair Segmentation and Style Recognition based on Hierarchical MRFs</h2>
            <p class="authors">Hugo Proença, João C. Neves</p>
            <p class="venue">IEEE Transactions on Information Forensics and Security (2017)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Markov Random Fields (MRFs) are a popular tool in many computer vision problems and faithfully model a broad range of local dependencies. However, rooted in the Hammersley-Clifford theorem, they face serious difficulties in enforcing the global coherence of the solutions without using too high order cliques that reduce the computational effectiveness of the inference phase. Having this problem in mind, we describe a multi-layered (hierarchical) architecture for MRFs that is based exclusively in pairwise connections and typically produces globally coherent solutions, with 1) one layer working at the local (pixel) level, modelling the interactions between adjacent image patches; and 2) a complementary layer working at the object (hypothesis) level pushing toward globally consistent solutions. During optimization, both layers interact into an equilibrium state, that not only segments the data, but also classifies it. The proposed MRF architecture is particularly suitable for problems that deal with biological data (e.g., biometrics), where the reasonability of the solutions can be objectively measured. As test case, we considered the problem of hair / facial hair segmentation and labelling, which are soft biometric labels useful for human recognition in-the-wild. We observed performance levels close to the state-of-the-art at a much lower computational cost, both in the segmentation and classification (labelling) tasks.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIFS.2017.2680246" target="_blank">10.1109/TIFS.2017.2680246</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/HairAnalysis_TIFS.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2017" 
         data-type="journal"
         data-title="insights into the results of miche i - mobile iris challenge evaluation"
         data-authors="maria de marsico, michele nappi, fabio narducci, hugo proença"
         data-venue="elsevier pattern recognition"
         data-tags="iris recognition,mobile biometrics,challenge evaluation,performance analysis"
         data-id="87">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/prl.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Insights into the results of MICHE I - Mobile Iris CHallenge Evaluation</h3>
                <p class="text-gray-600 mb-2">Maria De Marsico, Michele Nappi, Fabio Narducci, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Pattern Recognition</span> (2017)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Mobile Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Challenge Evaluation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Performance Analysis</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.patcog.2017.08.028" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/MICHEInsights.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="87">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="87">
            <h2>Insights into the results of MICHE I - Mobile Iris CHallenge Evaluation</h2>
            <p class="authors">Maria De Marsico, Michele Nappi, Fabio Narducci, Hugo Proença</p>
            <p class="venue">Elsevier Pattern Recognition (2017)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Mobile biometrics technologies are nowadays the new frontier for secure use of data and services, and are considered particularly important due to the massive use of handheld devices in the entire world. Among the biometric traits with potential to be used in mobile settings, the iris/ocular region is a natural candidate, even considering that further advances in the technology are required to meet the operational requirements of such ambitious environments. Aiming at promoting these advances, we organized the Mobile Iris Challenge Evaluation (MICHE)-I contest. This paper presents a comparison of the performance of the participant methods by various Figures of Merit (FoMs). A particular attention is devoted to the identification of the image covariates that are likely to cause a decrease in the performance levels of the compared algorithms. Among these factors, interoperability among different devices plays an important role. The methods (or parts of them) implemented by the analyzed approaches are classified into segmentation (S), which was the main target of MICHE-I, and recognition (R). The paper reports both the results observed for either S or R, and also for different recombinations (S+R) of such methods. Last but not least, we also present the results obtained by multi-classifier strategies.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.patcog.2017.08.028" target="_blank">10.1016/j.patcog.2017.08.028</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/MICHEInsights.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2017" 
         data-type="journal"
         data-title="soft biometrics: globally coherent solutions for hair segmentation and style recognition based on hierarchical mrfs"
         data-authors="hugo proença, joão c. neves"
         data-venue="ieee transactions on information forensics and security"
         data-tags="soft biometrics,hair analysis,markov random fields,segmentation"
         data-id="88">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-91.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Soft Biometrics: Globally Coherent Solutions for Hair Segmentation and Style Recognition based on Hierarchical MRFs</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, João C. Neves</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Information Forensics and Security</span> (2017)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Soft Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Hair Analysis</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Markov Random Fields</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Segmentation</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIFS.2017.2680246" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/HairAnalysis_TIFS.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="88">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="88">
            <h2>Soft Biometrics: Globally Coherent Solutions for Hair Segmentation and Style Recognition based on Hierarchical MRFs</h2>
            <p class="authors">Hugo Proença, João C. Neves</p>
            <p class="venue">IEEE Transactions on Information Forensics and Security (2017)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Markov Random Fields (MRFs) are a popular tool in many computer vision problems and faithfully model a broad range of local dependencies. However, rooted in the Hammersley-Clifford theorem, they face serious difficulties in enforcing the global coherence of the solutions without using too high order cliques that reduce the computational effectiveness of the inference phase. Having this problem in mind, we describe a multi-layered (hierarchical) architecture for MRFs that is based exclusively in pairwise connections and typically produces globally coherent solutions, with 1) one layer working at the local (pixel) level, modelling the interactions between adjacent image patches; and 2) a complementary layer working at the object (hypothesis) level pushing toward globally consistent solutions. During optimization, both layers interact into an equilibrium state, that not only segments the data, but also classifies it. The proposed MRF architecture is particularly suitable for problems that deal with biological data (e.g., biometrics), where the reasonability of the solutions can be objectively measured. As test case, we considered the problem of hair / facial hair segmentation and labelling, which are soft biometric labels useful for human recognition in-the-wild. We observed performance levels close to the state-of-the-art at a much lower computational cost, both in the segmentation and classification (labelling) tasks.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIFS.2017.2680246" target="_blank">10.1109/TIFS.2017.2680246</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/HairAnalysis_TIFS.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2017" 
         data-type="conference"
         data-title="irina: iris recognition (even) in innacurately segmented data"
         data-authors="hugo proença, joão c. neves"
         data-venue="proceedings of the ieee conference on computer vision and pattern recognition - cvpr 2017"
         data-tags="iris recognition,segmentation robustness,biometrics,image registration"
         data-id="89">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/cvf.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">IRINA: Iris Recognition (even) in Innacurately Segmented Data</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, João C. Neves</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition - CVPR 2017</span> (2017)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Segmentation Robustness</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Registration</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/CVPR.2017.714" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/CVPR2017.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="89">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="89">
            <h2>IRINA: Iris Recognition (even) in Innacurately Segmented Data</h2>
            <p class="authors">Hugo Proença, João C. Neves</p>
            <p class="venue">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition - CVPR 2017 (2017)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The effectiveness of current iris recognition systems depends on the accurate segmentation and parameterisation of the iris boundaries, as failures at this point misalign the coefficients of the biometric signatures. This paper describes IRINA, an algorithm for Iris Recognition that is robust against INAccurately segmented samples, which makes it a good candidate to work in poor-quality data. The process is based in the concept of 'corresponding' patch between pairs of images, that is used to estimate the posterior probabilities that patches regard the same biological region, even in case of segmentation errors and non-linear texture deformations. Such information enables to infer a free-form deformation field (2D registration vectors) between images, whose first and second-order statistics provide effective biometric discriminating power. Extensive experiments were carried out in four datasets (CASIA-IrisV3-Lamp, CASIA-IrisV4-Lamp, CASIA-IrisV4-Thousand and WVU) and show that IRINA not only achieves state-of-the-art performance in good quality data, but also handles effectively severe segmentation errors and large differences in pupillary dilation / constriction.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/CVPR.2017.714" target="_blank">10.1109/CVPR.2017.714</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/CVPR2017.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2017" 
         data-type="journal"
         data-title="quis-campi: an annotated multi-biometrics data feed from surveillance scenarios"
         data-authors="joão c. neves, juan c. moreno, hugo proença"
         data-venue="iet biometrics"
         data-tags="multi-biometrics,surveillance,data feed,outdoor recognition"
         data-id="90">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/ietbiometrics.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">QUIS-CAMPI: An Annotated Multi-biometrics Data Feed From Surveillance Scenarios</h3>
                <p class="text-gray-600 mb-2">João C. Neves, Juan C. Moreno, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IET Biometrics</span> (2017)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Multi-biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Surveillance</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Data Feed</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Outdoor Recognition</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1049/iet-bmt.2016.0178" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/QUISCAMPI_IET.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="90">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="90">
            <h2>QUIS-CAMPI: An Annotated Multi-biometrics Data Feed From Surveillance Scenarios</h2>
            <p class="authors">João C. Neves, Juan C. Moreno, Hugo Proença</p>
            <p class="venue">IET Biometrics (2017)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The accuracy of biometric recognition in unconstrained scenarios has been a major concern for a large number of researchers. Despite such efforts, no system can recognize in a fully automated manner human beings in totally wild conditions, such as in surveillance environments. In this context, several sets of degraded data have been made available to the research community, where the reported performance by state-of-the-art algorithms is already saturated, suggesting that these sets do not reflect faithfully the conditions in such hard settings. To this end, we introduce the QUIS-CAMPI data feed, comprising samples automatically acquired by an outdoor visual surveillance system, with subjects on-the-move and at-a-distance (up to 50 m). Also, we supply a high-quality set of enrollment data. When compared to similar data sources, the major novelties of QUIS-CAMPI are: 1) biometric samples are acquired in a fully automatic way; 2) it is an open dataset, i.e., the number of probe images and enrolled subjects grow on a daily basis; and 3) it contains multi-biometric traits. The ensemble properties of QUIS-CAMPI ensure that the data span a representative set of covariate factors of real-world scenarios, making it a valuable tool for developing and benchmarking biometric recognition algorithms capable of working in unconstrained scenarios.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1049/iet-bmt.2016.0178" target="_blank">10.1049/iet-bmt.2016.0178</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/QUISCAMPI_IET.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2017" 
         data-type="conference"
         data-title="exploiting data redundancy for error detection in degraded biometric signatures resulting from in the wild environments"
         data-authors="joão c. neves, hugo proença"
         data-venue="proceedings of the 2nd international workshop on biometrics in the wild, 12th ieee conference on automatic face and gesture recognition – fg 2017"
         data-tags="error detection,biometric signatures,markov random field,feature correlation"
         data-id="91">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/ieeexplore.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Exploiting Data Redundancy for Error Detection in Degraded Biometric Signatures Resulting From in the Wild Environments</h3>
                <p class="text-gray-600 mb-2">João C. Neves, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the 2nd International Workshop on Biometrics in the Wild, 12th IEEE Conference on Automatic Face and Gesture Recognition – FG 2017</span> (2017)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Error Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Signatures</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Markov Random Field</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Feature Correlation</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/FG.2017.122" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/fg2017.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="91">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="91">
            <h2>Exploiting Data Redundancy for Error Detection in Degraded Biometric Signatures Resulting From in the Wild Environments</h2>
            <p class="authors">João C. Neves, Hugo Proença</p>
            <p class="venue">Proceedings of the 2nd International Workshop on Biometrics in the Wild, 12th IEEE Conference on Automatic Face and Gesture Recognition – FG 2017 (2017)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>An error-correcting code (ECC) is a process of adding redundant data to a message, such that it can be recovered by a receiver even if a number of errors are introduced in transmission. Inspired by the principles of ECC, we introduce a method capable of detecting degraded features in biometric signatures by exploiting feature correlation. The main novelty is that, unlike existing biometric cryptosystems, the proposed method works directly on the biometric signature. Our approach performs a redundancy analysis of non-degraded data to build an undirected graphical model (Markov Random Field), whose energy minimization determines the sequence of degraded components of the biometric sample. Experiments carried out in different biometric traits ascertain the improvements attained when disregarding degraded features during the matching phase. Also, we stress that the proposed method is general enough to work in different classification methods, such as CNNs.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/FG.2017.122" target="_blank">10.1109/FG.2017.122</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/fg2017.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2017" 
         data-type="journal"
         data-title="fusing vantage point trees and linear discriminants for fast feature classification"
         data-authors="hugo proença, joão c. neves"
         data-venue="springer journal of classification"
         data-tags="classification,nearest neighbor,linear discriminant,vantage-point trees"
         data-id="92">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-84.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Fusing Vantage Point Trees and Linear Discriminants for Fast Feature Classification</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, João C. Neves</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Springer Journal of Classification</span> (2017)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Classification</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Nearest Neighbor</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Linear Discriminant</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Vantage-Point trees</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1007/s00357-017-9223-0" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/VPC.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="92">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="92">
            <h2>Fusing Vantage Point Trees and Linear Discriminants for Fast Feature Classification</h2>
            <p class="authors">Hugo Proença, João C. Neves</p>
            <p class="venue">Springer Journal of Classification (2017)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>This paper describes a classification strategy that can be regarded as a more general form of nearest-neighbor classification. It fuses the concepts of nearest neighbor, linear discriminant and Vantage-Point trees, yielding an efficient indexing data structure and classification algorithm. In the learning phase, we define a set of disjoint subspaces of reduced complexity that can be separated by linear discriminants, ending up with an ensemble of simple (weak) classifiers that work locally. In classification, the closest centroids to the query determine the set of classifiers considered, which responses are weighted. The algorithm was experimentally validated in datasets widely used in the field, attaining error rates that are favourably compara- ble to the state-of-the-art classification techniques. Lastly, the proposed solution has a set of interesting properties for a broad range of applications: 1) it is deterministic; 2) it classifies in time approximately logarithmic with respect to the size of the learning set, being far more efficient than nearest neighbor classification in terms of computational cost; and 3) it keeps the generalization ability of simple models.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1007/s00357-017-9223-0" target="_blank">10.1007/s00357-017-9223-0</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/VPC.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2016" 
         data-type="journal"
         data-title="joint head pose/soft label estimation for human recognition in-the-wild"
         data-authors="hugo proença, joão c. neves, silvio barra, tiago marques, juan c. moreno"
         data-venue="ieee transactions on pattern analysis and machine intelligence"
         data-tags="soft biometrics,head pose estimation,human recognition,3d morphology"
         data-id="93">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-86.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Joint Head Pose/Soft Label Estimation for Human Recognition In-The-Wild</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, João C. Neves, Silvio Barra, Tiago Marques, Juan C. Moreno</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Pattern Analysis and Machine Intelligence</span> (2016)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Soft Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Head Pose Estimation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Human Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">3D Morphology</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TPAMI.2016.2522441" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/SoftBiometrics.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="93">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="93">
            <h2>Joint Head Pose/Soft Label Estimation for Human Recognition In-The-Wild</h2>
            <p class="authors">Hugo Proença, João C. Neves, Silvio Barra, Tiago Marques, Juan C. Moreno</p>
            <p class="venue">IEEE Transactions on Pattern Analysis and Machine Intelligence (2016)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Soft biometrics have been emerging to complement other traits and are particularly useful for poor quality data. In this paper, we propose an efficient algorithm to estimate human head poses and to infer soft biometric labels based on the 3D morphology of the human head. Starting by considering a set of pose hypotheses, we use a learning set of head shapes synthesized from anthropometric surveys to derive a set of 3D head centroids that constitutes a metric space. Next, representing queries by sets of 2D head landmarks, we use projective geometry techniques to rank efficiently the joint 3D head centroids / pose hypotheses according to their likelihood of matching each query. The rationale is that the most likely hypotheses are sufficiently close to the query, so a good solution can be found by convex energy minimization techniques. Once a solution has been found, the 3D head centroid and the query are assumed to have similar morphology, yielding the soft label. Our experiments point toward the usefulness of the proposed solution, which can improve the effectiveness of face recognizers and can also be used as a privacy-preserving solution for biometric recognition in public environments.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TPAMI.2016.2522441" target="_blank">10.1109/TPAMI.2016.2522441</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/SoftBiometrics.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2016" 
         data-type="book"
         data-title="human recognition in unconstrained environments using computer vision, pattern recognition and machine learning methods for biometrics"
         data-authors="maria de marsico, michele nappi, hugo proença"
         data-venue="springer-verlag book series, communications engineering/ computer vision"
         data-tags="human recognition,unconstrained environments,biometrics,computer vision"
         data-id="94">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-128.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Human Recognition in Unconstrained Environments Using Computer Vision, Pattern Recognition and Machine Learning Methods for Biometrics</h3>
                <p class="text-gray-600 mb-2">Maria De Marsico, Michele Nappi, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Springer-Verlag book series, Communications Engineering/ Computer Vision</span> (2016)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge book">Book</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Human Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Unconstrained Environments</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Computer Vision</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="94">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="94">
            <h2>Human Recognition in Unconstrained Environments Using Computer Vision, Pattern Recognition and Machine Learning Methods for Biometrics</h2>
            <p class="authors">Maria De Marsico, Michele Nappi, Hugo Proença</p>
            <p class="venue">Springer-Verlag book series, Communications Engineering/ Computer Vision (2016)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p></p>
            </div>
            
            
            <div class="links">
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2016" 
         data-type="journal"
         data-title="biometric recognition in surveillance scenarios: a survey"
         data-authors="joão c. neves, hugo proença"
         data-venue="springer artificial intelligence review"
         data-tags="biometric recognition,surveillance,survey,unconstrained environments"
         data-id="95">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-87.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Biometric Recognition in Surveillance Scenarios: A Survey</h3>
                <p class="text-gray-600 mb-2">João C. Neves, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Springer Artificial Intelligence Review</span> (2016)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Surveillance</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Survey</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Unconstrained Environments</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1007/s10462-016-9474-x" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/BiometricsSurveillance_Survey.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="95">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="95">
            <h2>Biometric Recognition in Surveillance Scenarios: A Survey</h2>
            <p class="authors">João C. Neves, Hugo Proença</p>
            <p class="venue">Springer Artificial Intelligence Review (2016)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Interest in the security of individuals has increased in recent years. This increase has in turn led to much wider deployment of surveillance cameras worldwide, and consequently, automated surveillance systems research has received more attention from the scientific community than before. Concurrently, biometrics research has become more popular as well, and it is supported by the increasing number of approaches devised to address specific degradation factors of unconstrained environments. Despite these recent efforts, no automated surveillance system that performs reliable biometric recognition in such an environment has become available. Nevertheless, recent developments in human motion analysis and biometric recognition suggest that both can be combined to develop a fully automated system. As such, this paper reviews recent advances in both areas, with a special focus on surveillance scenarios. When compared to previous studies, we highlight two distinct features, i.e., (1) our emphasis is on approaches that are devised to work in unconstrained environments and surveillance scenarios; and (2) biometric recognition is the final goal of the surveillance system, as opposed to behavior analysis, anomaly detection or action recognition.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1007/s10462-016-9474-x" target="_blank">10.1007/s10462-016-9474-x</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/BiometricsSurveillance_Survey.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2016" 
         data-type="book_chapter"
         data-title="iris recognition in visible wavelengths and unconstrained conditions."
         data-authors="hugo proença"
         data-venue="handbook of iris recognition (2nd edition)"
         data-tags="iris recognition,visible wavelength,unconstrained conditions,periocular recognition"
         data-id="96">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-5.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Iris Recognition in Visible Wavelengths and Unconstrained Conditions.</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Handbook of Iris Recognition (2nd edition)</span> (2016)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge book_chapter">Book chapter</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visible Wavelength</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Unconstrained Conditions</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Recognition</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Hand_1.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="96">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="96">
            <h2>Iris Recognition in Visible Wavelengths and Unconstrained Conditions.</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">Handbook of Iris Recognition (2nd edition) (2016)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>One of the most challenging goals in biometrics research is the development of recognition systems to work in unconstrained environments and without assuming the subjects' willingness to be recognised. This has led to the concept of non-cooperative recognition, which broaden the application of biometrics to forensics / criminal seek domains. In this scope, one active research topic seeks to use as main trait the ocular region acquired at visible wavelengths, from moving targets and large distances. Under these conditions, performing reliable recognition is extremely difficult, because such real-world data have features that are notoriously different from those obtained in the classical constrained setups of currently deployed recognition systems. This chapter discusses the feasibility of iris / ocular biometric recognition: it starts by comparing the main properties of near-infrared and visible wavelength ocular data, and stresses the main difficulties behind the ac- curate segmentation of all components in the eye vicinity. Next, it summarises the most relevant research conducted in the scope of visible wavelength iris recognition and relates it to the concept of periocular recognition, which is an attempt to augment classes separability by using - apart from the iris - information from the surroundings of the eye. Finally, the current challenges in this topic and some directions for further research are discussed.</p>
            </div>
            
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Hand_1.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2016" 
         data-type="book_chapter"
         data-title="iris biometric indexing"
         data-authors="hugo proença, joão c. neves"
         data-venue="iris and periocular biometric recognition"
         data-tags="iris recognition,biometric indexing,retrieval"
         data-id="97">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-6.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Iris Biometric Indexing</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, João C. Neves</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Iris and Periocular Biometric Recognition</span> (2016)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge book_chapter">Book chapter</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Indexing</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Retrieval</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/BC_II.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="97">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="97">
            <h2>Iris Biometric Indexing</h2>
            <p class="authors">Hugo Proença, João C. Neves</p>
            <p class="venue">Iris and Periocular Biometric Recognition (2016)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Indexing / retrieving sets of iris biometric signatures has been a topic of increasing popularity, mostly due to the deployment of iris recognition systems in nationwide scale scenarios. In these conditions, for each identification attempt, there might exist hundreds of millions of enrolled identities and is unrealistic to match the probe against all gallery elements in a reasonable amount of time. Hence, the idea of indexing / retrieval is - upon receiving one sample - to find in a quick way a sub-set of elements in the database that most probably contains the identity of interest, i.e., the one corresponding to the probe. Most of the state-of-the-art strategies to index iris biometric signatures were devised to decision environments with a clear separation between genuine and impostor matching scores. However, if iris recognition systems work in low quality data, the resulting decision environments are poorly separable, with a significant overlap between the distributions of both matching scores. This chapter summarises the state-of-the-art in terms of iris bio- metric indexing / retrieval and focuses in an indexing / retrieval method for such low quality data and operates at the code level, i.e., after the signature encoding process. Gallery codes are decomposed at multiple scales, and using the most reliable components of each scale, their position in a n-ary tree is determined. During retrieval, the probe is decomposed similarly, and the distances to multi-scale centroids are used to penalize paths in the tree. At the end, only a subset of branches is traversed up to the last level.</p>
            </div>
            
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/BC_II.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2016" 
         data-type="journal"
         data-title="visible-wavelength iris/periocular imaging and recognition in surveillance environments"
         data-authors="hugo proença, joão c. neves"
         data-venue="elsevier image and vision computing"
         data-tags="visual surveillance,iris recognition,periocular recognition,visible wavelength"
         data-id="98">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-88.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Visible-wavelength Iris/Periocular Imaging and Recognition in Surveillance Environments</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, João C. Neves</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Image and Vision Computing</span> (2016)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visual Surveillance</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visible Wavelength</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.imavis.2016.03.015" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/PeriocularImagingSurveillance.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="98">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="98">
            <h2>Visible-wavelength Iris/Periocular Imaging and Recognition in Surveillance Environments</h2>
            <p class="authors">Hugo Proença, João C. Neves</p>
            <p class="venue">Elsevier Image and Vision Computing (2016)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Visual surveillance cameras have been massively deployed in public urban environments over the recent years, as a crime prevention and law enforcement solution. This fact raised the interest in developing automata to infer useful information from such crowded scenes (from abnormal behavior detection to human identification). In order to cover wide outdoor areas, one interesting possibility is to combine wide- angle and pan–tilt–zoom (PTZ) cameras in a master–slave configuration. The use of fish-eye lenses allows the master camera to maximize the coverage area while the PTZ acts as a foveal sensor, providing high- resolution images of the interest regions. This paper addresses the feasibility of using this type of data acquisition paradigm for imaging iris/periocular data with enough discriminating power to be used for biometric recognition purposes.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.imavis.2016.03.015" target="_blank">10.1016/j.imavis.2016.03.015</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/PeriocularImagingSurveillance.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2016" 
         data-type="special_issue"
         data-title="biometric recognition in-the-wild"
         data-authors="hugo proença, mark nixon, michele nappi"
         data-venue="ieee intelligent systems, trends and controversies"
         data-tags="biometric recognition,in-the-wild,unconstrained environments"
         data-id="99">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-121.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Biometric Recognition in-the-Wild</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Mark Nixon, Michele Nappi</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Intelligent Systems, Trends and Controversies</span> (2016)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge special_issue">Special issue</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">In-the-Wild</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Unconstrained Environments</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="99">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="99">
            <h2>Biometric Recognition in-the-Wild</h2>
            <p class="authors">Hugo Proença, Mark Nixon, Michele Nappi</p>
            <p class="venue">IEEE Intelligent Systems, Trends and Controversies (2016)</p>
            
            
            <div class="links">
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2016" 
         data-type="conference"
         data-title="mobile iris challenge evaluation ii: results from the icpr competition"
         data-authors="modesto castrillon, maria de marsico, michele nappi, fabio narducci, hugo proença"
         data-venue="proceedings of the international conference on pattern recognition – icpr 2016"
         data-tags="mobile biometrics,iris recognition,miche"
         data-id="100">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-43.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Mobile Iris CHallenge Evaluation II: results from the ICPR competition</h3>
                <p class="text-gray-600 mb-2">Modesto Castrillon, Maria De Marsico, Michele Nappi, Fabio Narducci, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the International Conference on Pattern Recognition – ICPR 2016</span> (2016)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Mobile Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">MICHE</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/ICPR.2016.7899624" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/icpr2016.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="100">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="100">
            <h2>Mobile Iris CHallenge Evaluation II: results from the ICPR competition</h2>
            <p class="authors">Modesto Castrillon, Maria De Marsico, Michele Nappi, Fabio Narducci, Hugo Proença</p>
            <p class="venue">Proceedings of the International Conference on Pattern Recognition – ICPR 2016 (2016)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The growing interest for mobile biometrics stems from the increasing need to secure personal data and services, which are often stored or accessed from there. Modern user mobile devices, with acquisition and computation resources to support related operations, are nowadays widely available. This makes this research topic very attracting and promising. Iris recognition plays a major role in this scenario. However, mo- bile biometrics still suffer from some hindering fac- tors. The resolution of captured images and the computational power are not comparable to desktop systems yet. Furthermore, the acquisition setting is generally uncontrolled, with users who are not that expert to autonomously generate biometric samples of sufficient quality. Mobile Iris CHallenge Evaluation aims at providing a testbed to assess the progress of mobile iris recognition, and to evaluate the extent of its present limitations. This paper presents the results of the competition launched at the 2016 edition of the International Conference on Pattern Recognition (ICPR).</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/ICPR.2016.7899624" target="_blank">10.1109/ICPR.2016.7899624</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/icpr2016.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2016" 
         data-type="conference"
         data-title="icb-rw 2016: international challenge on biometric recognition in the wild"
         data-authors="joão c. neves, hugo proença"
         data-venue="proceedings of the 9th iapr international conference on biometrics - icb 2016"
         data-tags="biometric recognition,wild conditions,surveillance"
         data-id="101">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-42.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">ICB-RW 2016: International Challenge on Biometric Recognition in the Wild</h3>
                <p class="text-gray-600 mb-2">João C. Neves, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the 9th IAPR International Conference on Biometrics - ICB 2016</span> (2016)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Wild Conditions</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Surveillance</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/ICB.2016.7550066" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ICB16.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="101">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="101">
            <h2>ICB-RW 2016: International Challenge on Biometric Recognition in the Wild</h2>
            <p class="authors">João C. Neves, Hugo Proença</p>
            <p class="venue">Proceedings of the 9th IAPR International Conference on Biometrics - ICB 2016 (2016)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Biometric recognition in totally wild conditions, such as the observed in visual surveillance scenarios has not been achieved yet. The ICB-RW competition was promoted to support this endeavor, being the first biometric challenge carried out in data that realistically result from surveillance scenarios. The competition relied on an innovative master- slave surveillance system for the acquisition of face imagery at-a-distance and on-the-move. This paper describes the competition details and reports the performance achieved by the participants algorithms.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/ICB.2016.7550066" target="_blank">10.1109/ICB.2016.7550066</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ICB16.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2016" 
         data-type="journal"
         data-title="joint head pose/soft label estimation for human recognition in-the-wild"
         data-authors="hugo proença, joão c. neves, silvio barra, tiago marques, juan c. moreno"
         data-venue="ieee transactions on pattern analysis and machine intelligence"
         data-tags="soft biometrics,head pose estimation,human recognition,in-the-wild"
         data-id="102">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-86.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Joint Head Pose/Soft Label Estimation for Human Recognition In-The-Wild</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, João C. Neves, Silvio Barra, Tiago Marques, Juan C. Moreno</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Pattern Analysis and Machine Intelligence</span> (2016)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Soft Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Head Pose Estimation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Human Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">In-the-Wild</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TPAMI.2016.2522441" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/SoftBiometrics.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="102">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="102">
            <h2>Joint Head Pose/Soft Label Estimation for Human Recognition In-The-Wild</h2>
            <p class="authors">Hugo Proença, João C. Neves, Silvio Barra, Tiago Marques, Juan C. Moreno</p>
            <p class="venue">IEEE Transactions on Pattern Analysis and Machine Intelligence (2016)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Soft biometrics have been emerging to complement other traits and are particularly useful for poor quality data. In this paper, we propose an efficient algorithm to estimate human head poses and to infer soft biometric labels based on the 3D morphology of the human head. Starting by considering a set of pose hypotheses, we use a learning set of head shapes synthesized from anthropometric surveys to derive a set of 3D head centroids that constitutes a metric space. Next, representing queries by sets of 2D head landmarks, we use projective geometry techniques to rank efficiently the joint 3D head centroids / pose hypotheses according to their likelihood of matching each query. The rationale is that the most likely hypotheses are sufficiently close to the query, so a good solution can be found by convex energy minimization techniques. Once a solution has been found, the 3D head centroid and the query are assumed to have similar morphology, yielding the soft label. Our experiments point toward the usefulness of the proposed solution, which can improve the effectiveness of face recognizers and can also be used as a privacy-preserving solution for biometric recognition in public environments.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TPAMI.2016.2522441" target="_blank">10.1109/TPAMI.2016.2522441</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/SoftBiometrics.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2015" 
         data-type="conference"
         data-title="a calibration algorithm for multi-camera visual surveillance systems based on single-view metrology"
         data-authors="joão c. neves, juan c. moreno, silvio barra, hugo proença"
         data-venue="proceedings of the 7th iberian conference on pattern recognition and image analysis - ibpria 2015"
         data-tags="camera calibration,visual surveillance,single-view metrology,master-slave system"
         data-id="103">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-37.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">A Calibration Algorithm for Multi-camera Visual Surveillance Systems Based on Single-View Metrology</h3>
                <p class="text-gray-600 mb-2">João C. Neves, Juan C. Moreno, Silvio Barra, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the 7th Iberian Conference on Pattern Recognition and Image Analysis - IbPRIA 2015</span> (2015)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Camera Calibration</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visual Surveillance</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Single-View Metrology</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Master-slave System</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1007/978-3-319-19390-8_62" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/ibpria15.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="103">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="103">
            <h2>A Calibration Algorithm for Multi-camera Visual Surveillance Systems Based on Single-View Metrology</h2>
            <p class="authors">João C. Neves, Juan C. Moreno, Silvio Barra, Hugo Proença</p>
            <p class="venue">Proceedings of the 7th Iberian Conference on Pattern Recognition and Image Analysis - IbPRIA 2015 (2015)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The growing concerns about persons security and the increasing popularity of pan-tilt-zoom (PTZ) cameras, have been raising the interest on automated master-slave surveillance systems. Such systems are typically composed by (1) a fixed wide-angle camera that covers a large area, detects and tracks moving objects in the scene; and (2) a PTZ camera, that provides a close-up view of an object of interest. Previously published approaches attempted to establish 2D correspondences between the video streams of both cameras, which is a ill-posed formulation due to the absence of depth information. On the other side, 3D-based approaches are more accurate but require more than one fixed camera to estimate depth information. In this paper, we describe a novel method for easy and precise calibration of a master-slave surveillance sys- tem, composed by a single fixed wide-angle camera. Our method exploits single view metrology to infer 3D data of the tracked humans and to self- perform the transformation between camera views. Experimental results in both simulated and realistic scenes point for the effectiveness of the proposed model in comparison with the state-of-the-art.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1007/978-3-319-19390-8_62" target="_blank">10.1007/978-3-319-19390-8_62</a></p>
            
            <div class="links">
                
                <a href="http://di.ubi.pt/~hugomcp/doc/ibpria15.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2015" 
         data-type="journal"
         data-title="a master-slave calibration algorithm with fish-eye correction"
         data-authors="joão c. neves, juan c. moreno, hugo proença"
         data-venue="hindawi mathematical problems in engineering"
         data-tags="master-slave system,calibration,fish-eye correction,surveillance"
         data-id="104">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-85.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">A Master-slave Calibration Algorithm with Fish-eye Correction</h3>
                <p class="text-gray-600 mb-2">João C. Neves, Juan C. Moreno, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Hindawi Mathematical Problems in Engineering</span> (2015)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Master-slave System</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Calibration</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Fish-eye Correction</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Surveillance</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1155/2015/427270" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/MasterSlaveCalibration_MPE.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="104">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="104">
            <h2>A Master-slave Calibration Algorithm with Fish-eye Correction</h2>
            <p class="authors">João C. Neves, Juan C. Moreno, Hugo Proença</p>
            <p class="venue">Hindawi Mathematical Problems in Engineering (2015)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Surveillance systems capable of autonomously monitoring vast areas are an emerging trend, particularly when wide-angle cameras are combined with pan-tilt-zoom (PTZ) cameras in a master-slave configuration. The use of fish-eye lenses allows the master camera to maximize the coverage area while the PTZ acts as a foveal sensor, providing high-resolution images of regions of interest. Despite the advantages of this architecture, the mapping between image coordinates and pan-tilt values is the major bottleneck in such systems, since it depends on depth information and fish-eye effect correction. In this paper, we address these problems by exploiting geometric cues to perform height estimation. This information is used both for inferring 3D information from a single static camera deployed on an arbitrary position and for determining lens parameters to remove fish-eye distortion. When compared with the previous approaches, our method has the following advantages: (1) fish-eye distortion is corrected without relying on calibration patterns; (2) 3D information is inferred from a single static camera disposed on an arbitrary location of the scene.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1155/2015/427270" target="_blank">10.1155/2015/427270</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/MasterSlaveCalibration_MPE.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2015" 
         data-type="journal"
         data-title="periocular recognition: how much facial expressions affect performance?"
         data-authors="elisa barroso, gil santos, luís cardoso, chandrashekhar padole, hugo proença"
         data-venue="springer pattern analysis and applications"
         data-tags="periocular recognition,facial expressions,biometric performance"
         data-id="105">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-83.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Periocular Recognition: How Much Facial Expressions Affect Performance?</h3>
                <p class="text-gray-600 mb-2">Elisa Barroso, Gil Santos, Luís Cardoso, Chandrashekhar Padole, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Springer Pattern Analysis and Applications</span> (2015)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Facial Expressions</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Performance</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1007/s10044-015-0493-z" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/PeriocularExpressions_PAA.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="105">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="105">
            <h2>Periocular Recognition: How Much Facial Expressions Affect Performance?</h2>
            <p class="authors">Elisa Barroso, Gil Santos, Luís Cardoso, Chandrashekhar Padole, Hugo Proença</p>
            <p class="venue">Springer Pattern Analysis and Applications (2015)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Using information near the human eye to per- form biometric recognition has been gaining popularity. Previous works in this area, designated periocular recognition, show remarkably low error rates and particularly high robustness when data are acquired under less con- trolled conditions. In this field, one factor that remains to be studied is the effect of facial expressions on recognition performance, as expressions change the textural/shape information inside the periocular region. We have collected a multisession dataset whose single variation is the subjects' facial expressions and analyzed the corresponding variations in performance, using the state-of-the-art peri- ocular recognition strategy. The effectiveness attained by different strategies to handle the effects of facial expressions was compared: (1) single-sample enrolment; (2) multisample enrolment, and (3) multisample enrolment with facial expression recognition, with results also vali- dated in the well-known Cohn–Kanade AU-Coded Expression dataset. Finally, the role of each type of facial expression in the biometrics menagerie effect is discussed.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1007/s10044-015-0493-z" target="_blank">10.1007/s10044-015-0493-z</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/PeriocularExpressions_PAA.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2015" 
         data-type="journal"
         data-title="aperiodic feature representation for gait recognition in cross-view scenarios for unconstrained biometrics"
         data-authors="chandrashekhar padole, hugo proença"
         data-venue="springer pattern analysis and applications"
         data-tags="gait recognition,feature representation,cross-view scenarios,unconstrained biometrics"
         data-id="106">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-81.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Aperiodic Feature Representation for Gait Recognition in Cross-view Scenarios for Unconstrained Biometrics</h3>
                <p class="text-gray-600 mb-2">Chandrashekhar Padole, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Springer Pattern Analysis and Applications</span> (2015)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Gait Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Feature Representation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Cross-view Scenarios</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Unconstrained Biometrics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1007/s10044-015-0468-0" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Gait_PAA.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="106">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="106">
            <h2>Aperiodic Feature Representation for Gait Recognition in Cross-view Scenarios for Unconstrained Biometrics</h2>
            <p class="authors">Chandrashekhar Padole, Hugo Proença</p>
            <p class="venue">Springer Pattern Analysis and Applications (2015)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The state-of-the-art gait recognition algorithms require a gait cycle estimation before the feature extraction and are classified as periodic algorithms. Their effective- ness substantially decreases due to errors in detecting gait cycles, which are likely to occur in data acquired in non- controlled conditions. Hence, the main contributions of this paper are: (1) propose an aperiodic gait recognition strategy, where features are extracted without the concept of gait cycle, in case of multi-view scenario; (2) propose the fusion of the different feature subspaces of aperiodic feature representations at score level in cross-view scenarios. The experiments were performed with widely known CASIA Gait database B, which enabled us to draw the following major conclusions, (1) for multi-view scenarios, features extracted from gait sequences of varying length have as much discriminating power as traditional periodic features; (2) for cross-view scenarios, we observed an average improvement of 22 % over the error rates of state-of- the-art algorithms, due to the proposed fusion scheme.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1007/s10044-015-0468-0" target="_blank">10.1007/s10044-015-0468-0</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Gait_PAA.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2015" 
         data-type="journal"
         data-title="robust periocular recognition by fusing sparse representations of color and geometry information"
         data-authors="juan c. moreno, v. b. surya prasath, gil santos, hugo proença"
         data-venue="springer journal of signal processing systems"
         data-tags="periocular recognition,sparse representation,color,geometry"
         data-id="107">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-82.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Robust periocular recognition by fusing sparse representations of color and geometry information</h3>
                <p class="text-gray-600 mb-2">Juan C. Moreno, V. B. Surya Prasath, Gil Santos, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Springer Journal of Signal Processing Systems</span> (2015)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Sparse Representation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Color</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Geometry</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1007/s11265-015-1023-3" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/SparseFusion_JSPS.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="107">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="107">
            <h2>Robust periocular recognition by fusing sparse representations of color and geometry information</h2>
            <p class="authors">Juan C. Moreno, V. B. Surya Prasath, Gil Santos, Hugo Proença</p>
            <p class="venue">Springer Journal of Signal Processing Systems (2015)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>In this paper, we propose a re-weighted elastic net (REN) model for biometric recognition. The new model is applied to data separated into geometric and color spatial components. The geometric information is extracted using a fast cartoon - texture decomposition model based on a dual formulation of the total variation norm allowing us to carry information about the overall geometry of images. Color components are defined using linear and nonlinear color spaces, namely the red-green-blue (RGB), chromaticity- brightness (CB) and hue-saturation-value (HSV). Next, according to a Bayesian fusion-scheme, sparse representations for classification purposes are obtained. The scheme is numerically solved using a gradient projection (GP) algo- rithm. In the empirical validation of the proposed model, we have chosen the periocular region, which is an emerging trait known for its robustness against low quality data. Our results were obtained in the publicly available FRGC and UBIRIS.v2 data sets and show consistent improvements in recognition effectiveness when compared to related state- of-the-art techniques.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1007/s11265-015-1023-3" target="_blank">10.1007/s11265-015-1023-3</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/SparseFusion_JSPS.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2015" 
         data-type="conference"
         data-title="evaluation of background subtraction algorithms for human visual surveillance"
         data-authors="joão c. neves, kamila wysoczanska, hugo proença"
         data-venue="proceedings of the international conference on signal and image processing applications – icsipa 2015"
         data-tags="background subtraction,visual surveillance,human detection"
         data-id="108">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-41.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Evaluation of Background Subtraction Algorithms for Human Visual Surveillance</h3>
                <p class="text-gray-600 mb-2">João C. Neves, Kamila Wysoczanska, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the International Conference on Signal and Image Processing Applications – ICSIPA 2015</span> (2015)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Background Subtraction</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visual Surveillance</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Human Detection</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ICSIPA15.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="108">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="108">
            <h2>Evaluation of Background Subtraction Algorithms for Human Visual Surveillance</h2>
            <p class="authors">João C. Neves, Kamila Wysoczanska, Hugo Proença</p>
            <p class="venue">Proceedings of the International Conference on Signal and Image Processing Applications – ICSIPA 2015 (2015)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The fully automated surveillance of human beings remains an open problem, particularly for in-the-wild scenarios, i.e., for complex backgrounds and under uncontrolled lighting conditions. Background Subtraction (BGS) is typically the first phase of the processing chain of such type of systems and holds the feasibility of all the subsequent phases. Hence, it is particularly important to perceive the relative effectiveness of BGS, with respect to the kind of environment. This paper gives an objective evaluation of the state-of-the-art BGS algorithms on unconstrained outdoor environments. When compared to similar published works, the major novelties are two-fold: 1) the focus is put on scenes populated by human beings; and 2) an objective measure of the wildness of environments is proposed, that strongly correlates to BGS performance, and enables to perceive the algorithms' robustness with respect to the environment complexity. As main conclusions, we observed that the SOBS algorithm outperforms the remaining methods. Nevertheless, its performance leads to conclude that BGS in unconstrained environments is still an open problem.</p>
            </div>
            
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ICSIPA15.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2015" 
         data-type="journal"
         data-title="iris recognition: what's beyond bit fragility?"
         data-authors="hugo proença"
         data-venue="ieee transactions on information forensics and security"
         data-tags="iris recognition,bit fragility,bit discriminability,multi-spectral data"
         data-id="109">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-80.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Iris Recognition: What's Beyond Bit Fragility?</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Information Forensics and Security</span> (2015)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Bit Fragility</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Bit Discriminability</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Multi-spectral Data</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIFS.2014.2371691" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/BitFragility.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="109">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="109">
            <h2>Iris Recognition: What's Beyond Bit Fragility?</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">IEEE Transactions on Information Forensics and Security (2015)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The concept of fragility of some bits in the iris codes regards exclusively their within-class variation, i.e., the probability that they take different values in templates computed from different images of the same iris. This paper extends that concept, by noticing that a similar phenomenon occurs for the between-classes comparisons, i.e., some bits have higher probability than others of assuming a predominant value, which was observed for near-infrared and (in a more evident way) for visible wavelength data. Accordingly, we propose a new measure (bit discriminability) that takes into account both the within-class and between-classes variabilities, and has roots in the Fisher discriminant. Based on the bit discriminability, we compare the usefulness of the different regions of the iris for biometric recognition, with respect to multi-spectral data and to different filters parameterizations. Finally, we measure the amount of information lost in codes quantization, which gives insight to further research on iris matching strategies that consider both phase and magnitude. Albeit augmenting the computational burden of recognition, such kind of strategies will consistently improve performance, particularly in poor-quality data.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIFS.2014.2371691" target="_blank">10.1109/TIFS.2014.2371691</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/BitFragility.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2015" 
         data-type="conference"
         data-title="acquiring high-resolution face images in outdoor environments: a master-slave calibration algorithm"
         data-authors="joão c. neves, juan c. moreno, silvio barra, hugo proença"
         data-venue="proceedings of the ieee seventh international conference on biometrics: theory, applications and systems – btas 2015"
         data-tags="face recognition,surveillance,ptz camera,master-slave calibration"
         data-id="110">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-39.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Acquiring High-resolution Face Images in Outdoor Environments: A master-slave Calibration Algorithm</h3>
                <p class="text-gray-600 mb-2">João C. Neves, Juan C. Moreno, Silvio Barra, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the IEEE Seventh International Conference on Biometrics: Theory, Applications and Systems – BTAS 2015</span> (2015)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Face Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Surveillance</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">PTZ Camera</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Master-slave Calibration</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/BTAS.2015.7358744" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="110">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="110">
            <h2>Acquiring High-resolution Face Images in Outdoor Environments: A master-slave Calibration Algorithm</h2>
            <p class="authors">João C. Neves, Juan C. Moreno, Silvio Barra, Hugo Proença</p>
            <p class="venue">Proceedings of the IEEE Seventh International Conference on Biometrics: Theory, Applications and Systems – BTAS 2015 (2015)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Facial recognition at-a-distance in surveillance scenarios remains an open problem, particularly due to the small number of pixels representing the facial region. The use of pan-tilt-zoom (PTZ) cameras has been advocated to solve this problem, however, the existing approaches either rely on rough approximations or additional constraints to estimate the mapping between image coordinates and pan-tilt parameters. In this paper, we aim at extending PTZ-assisted facial recognition to surveillance scenarios by proposing a master-slave calibration algorithm capable of accurately estimating pan-tilt parameters without depending on additional constraints. Our approach exploits geometric cues to automatically estimate subjects height and thus determine their 3D position. Experimental results show that the presented algorithm is able to acquire high-resolution face im- ages at a distance ranging from 5 to 40 meters with high success rate. Additionally, we certify the applicability of the aforementioned algorithm to biometric recognition through a face recognition test, comprising 20 probe subjects and 13,020 gallery subjects.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/BTAS.2015.7358744" target="_blank">10.1109/BTAS.2015.7358744</a></p>
            
            <div class="links">
                
                <a href="" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2015" 
         data-type="conference"
         data-title="quis-campi: extending in the wild biometric recognition to surveillance environments"
         data-authors="gil santos, joão c. neves, sílvio filipe, emanuel grancho, silvio barra, fabio narducci, hugo proença"
         data-venue="proceedings of the 18th international conference on image analysis and processing - iciap 2015"
         data-tags="biometric recognition,surveillance,in-the-wild,human detection,tracking"
         data-id="111">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-40.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Quis-Campi: Extending In The Wild Biometric Recognition to Surveillance Environments</h3>
                <p class="text-gray-600 mb-2">Gil Santos, João C. Neves, Sílvio Filipe, Emanuel Grancho, Silvio Barra, Fabio Narducci, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the 18th International Conference on Image Analysis and Processing - ICIAP 2015</span> (2015)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Surveillance</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">In-the-Wild</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Human Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Tracking</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1007/978-3-319-23222-5_8" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/ICIAP15.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="111">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="111">
            <h2>Quis-Campi: Extending In The Wild Biometric Recognition to Surveillance Environments</h2>
            <p class="authors">Gil Santos, João C. Neves, Sílvio Filipe, Emanuel Grancho, Silvio Barra, Fabio Narducci, Hugo Proença</p>
            <p class="venue">Proceedings of the 18th International Conference on Image Analysis and Processing - ICIAP 2015 (2015)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Efforts in biometrics are being held into extending robust recognition techniques to in the wild scenarios. Nonetheless, and despite being a very attractive goal, human identification in the surveillance con- text remains an open problem. In this paper, we introduce a novel bio- metric system – Quis-Campi – that effectively bridges the gap between surveillance and biometric recognition while having a minimum amount of operational restrictions. We propose a fully automated surveillance sys- tem for human recognition purposes, attained by combining human detection and tracking, further enhanced by a PTZ camera that delivers data with enough quality to perform biometric recognition. Along with the system concept, implementation details for both hardware and software modules are provided, as well as preliminary results over a real scenario.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1007/978-3-319-23222-5_8" target="_blank">10.1007/978-3-319-23222-5_8</a></p>
            
            <div class="links">
                
                <a href="http://di.ubi.pt/~hugomcp/doc/ICIAP15.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2015" 
         data-type="journal"
         data-title="iris recognition: what's beyond bit fragility?"
         data-authors="hugo proença"
         data-venue="ieee transactions on information forensics and security"
         data-tags="iris recognition,bit fragility,biometrics"
         data-id="112">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-80.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Iris Recognition: What's Beyond Bit Fragility?</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Information Forensics and Security</span> (2015)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Bit Fragility</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIFS.2014.2371691" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/BitFragility.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="112">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="112">
            <h2>Iris Recognition: What's Beyond Bit Fragility?</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">IEEE Transactions on Information Forensics and Security (2015)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The concept of fragility of some bits in the iris codes regards exclusively their within-class variation, i.e., the probability that they take different values in templates computed from different images of the same iris. This paper extends that concept, by noticing that a similar phenomenon occurs for the between-classes comparisons, i.e., some bits have higher probability than others of assuming a predominant value, which was observed for near-infrared and (in a more evident way) for visible wavelength data. Accordingly, we propose a new measure (bit discriminability) that takes into account both the within-class and between-classes variabilities, and has roots in the Fisher discriminant. Based on the bit discriminability, we compare the usefulness of the different regions of the iris for biometric recognition, with respect to multi-spectral data and to different filters parameterizations. Finally, we measure the amount of information lost in codes quantization, which gives insight to further research on iris matching strategies that consider both phase and magnitude. Albeit augmenting the computational burden of recognition, such kind of strategies will consistently improve performance, particularly in poor-quality data.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIFS.2014.2371691" target="_blank">10.1109/TIFS.2014.2371691</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/BitFragility.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2015" 
         data-type="journal"
         data-title="biohdd: a dataset for studying biometric identification on heavily degraded data"
         data-authors="gil santos, paulo fiadeiro, hugo proença"
         data-venue="iet biometrics"
         data-tags="biometric identification,degraded data,dataset,human recognition"
         data-id="113">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-79.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">BioHDD: A Dataset for Studying Biometric Identification on Heavily Degraded Data</h3>
                <p class="text-gray-600 mb-2">Gil Santos, Paulo Fiadeiro, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IET Biometrics</span> (2015)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Identification</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Degraded Data</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Dataset</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Human Recognition</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1049/iet-bmt.2014.0045" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/BioHDD_IET-Biom.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="113">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="113">
            <h2>BioHDD: A Dataset for Studying Biometric Identification on Heavily Degraded Data</h2>
            <p class="authors">Gil Santos, Paulo Fiadeiro, Hugo Proença</p>
            <p class="venue">IET Biometrics (2015)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Substantial efforts have been put into bridging the gap between biometrics and visual surveillance, in order to developautomata able to recognise human beings 'in the wild'. This study focuses on biometric recognition in extremely degraded data, and its main contributions are three-fold: (1) announce the availability of an annotated dataset that contains high quality mugshots of 101 subjects, and large sets of probes degraded extremely by 10 different noise factors; (2) report the results of a mimicked watchlist identification scheme: an online survey was conducted, where participants were asked to perform positive and negative identification of probes against the enrolled identities. Along with their answers, volunteers had to provide the major reasons that sustained their responses, which enabled the authors to perceive the kind of features that are most frequently associated with 30 successful/failed human identification processes. As main conclusions, the authors observed that humans rely greatly on shape information and holistic features. Otherwise, colour and texture-based features are almost disregarded by humans; (3) finally, the authors give evidence that the positive human identification on such extremely degraded data might be unreliable, whereas negative identification might constitute an interesting alternative for such cases.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1049/iet-bmt.2014.0045" target="_blank">10.1049/iet-bmt.2014.0045</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/BioHDD_IET-Biom.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2015" 
         data-type="conference"
         data-title="dynamic camera scheduling for visual surveillance in crowded scenes using markov random fields"
         data-authors="joão c. neves, hugo proença"
         data-venue="proceedings of the 12th ieee international conference on advanced video and signal based surveillance - avss 2015"
         data-tags="camera scheduling,ptz camera,visual surveillance,markov random fields"
         data-id="114">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-38.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Dynamic Camera Scheduling for Visual Surveillance in Crowded Scenes using Markov Random Fields</h3>
                <p class="text-gray-600 mb-2">João C. Neves, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the 12th IEEE International Conference on Advanced Video and Signal based Surveillance - AVSS 2015</span> (2015)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Camera Scheduling</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">PTZ Camera</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visual Surveillance</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Markov Random Fields</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/AVSS.2015.7301790" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="114">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="114">
            <h2>Dynamic Camera Scheduling for Visual Surveillance in Crowded Scenes using Markov Random Fields</h2>
            <p class="authors">João C. Neves, Hugo Proença</p>
            <p class="venue">Proceedings of the 12th IEEE International Conference on Advanced Video and Signal based Surveillance - AVSS 2015 (2015)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The use of pan-tilt-zoom (PTZ) cameras for capturing high-resolution data of human-beings is an emerging trend in surveillance systems. However, this new paradigm en- tails additional challenges, such as camera scheduling, that can dramatically affect the performance of the system. In this paper, we present a camera scheduling approach capable of determining - in real-time - the sequence of acquisitions that maximizes the number of different targets obtained, while minimizing the cumulative transition time. Our approach models the problem as an undirected graphical model (Markov random field, MRF), which energy minimization can approximate the shortest tour to visit the maximum number of targets. A comparative analysis with the state-of-the-art camera scheduling methods evidences that our approach is able to improve the observation rate while maintaining a competitive tour time.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/AVSS.2015.7301790" target="_blank">10.1109/AVSS.2015.7301790</a></p>
            
            <div class="links">
                
                <a href="" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2014" 
         data-type="conference"
         data-title="automatic face recognition in hdr imaging"
         data-authors="manuela pereira, juan c. moreno, hugo proença, antónio pinheiro"
         data-venue="proceedings of the spie photonics europe conference"
         data-tags="face recognition,hdr imaging,tone mapping,privacy"
         data-id="115">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-131.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Automatic face recognition in HDR imaging</h3>
                <p class="text-gray-600 mb-2">Manuela Pereira, Juan C. Moreno, Hugo Proença, António Pinheiro</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the SPIE Photonics Europe Conference</span> (2014)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Face Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">HDR Imaging</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Tone Mapping</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Privacy</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1117/12.2054539" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/9138/913804/Automatic-face-recognition-in-HDR-imaging/10.1117/12.2054539.short" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="115">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="115">
            <h2>Automatic face recognition in HDR imaging</h2>
            <p class="authors">Manuela Pereira, Juan C. Moreno, Hugo Proença, António Pinheiro</p>
            <p class="venue">Proceedings of the SPIE Photonics Europe Conference (2014)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The gaining popularity of the new High Dynamic Range (HDR) imaging systems is raising new privacy issues caused by the methods used for visualization. HDR images require tone mapping methods for an appropriate visualization on conventional and non-expensive LDR displays. These visualization methods might result in completely different visualization raising several issues on privacy intrusion. In fact, some visualization methods result in a perceptual recognition of the individuals, while others do not even show any identity. Although perceptual recognition might be possible, a natural question that can rise is how computer based recognition will perform using tone mapping generated images? In this paper, a study where automatic face recognition using sparse representation is tested with images that result from common tone mapping operators applied to HDR images. Its ability for the face identity recognition is described. Furthermore, typical LDR images are used for the face recognition training.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1117/12.2054539" target="_blank">10.1117/12.2054539</a></p>
            
            <div class="links">
                
                <a href="https://www.spiedigitallibrary.org/conference-proceedings-of-spie/9138/913804/Automatic-face-recognition-in-HDR-imaging/10.1117/12.2054539.short" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2014" 
         data-type="journal"
         data-title="detection and separation of overlapping cells based on contour concavity for leishmania images"
         data-authors="joão c. neves, helena castro, ana tomás, miguel coimbra, hugo proença"
         data-venue="wiley cytometry: part a"
         data-tags="cell detection,image processing,contour concavity,leishmania"
         data-id="116">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-75.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Detection and Separation of Overlapping Cells Based on Contour Concavity for Leishmania images</h3>
                <p class="text-gray-600 mb-2">João C. Neves, Helena Castro, Ana Tomás, Miguel Coimbra, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Wiley Cytometry: Part A</span> (2014)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Cell Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Processing</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Contour Concavity</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Leishmania</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1002/cyto.a.22465" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Leishmania_CytometryA.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="116">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="116">
            <h2>Detection and Separation of Overlapping Cells Based on Contour Concavity for Leishmania images</h2>
            <p class="authors">João C. Neves, Helena Castro, Ana Tomás, Miguel Coimbra, Hugo Proença</p>
            <p class="venue">Wiley Cytometry: Part A (2014)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Life scientists often must count cells in microscopy images, which is a tedious and time-consuming task. Automatic approaches present a solution to this problem. Several procedures have been devised for this task, but the majority suffer from performance degradation in the case of cell overlap. In this article, we propose a method to deter- mine the positions of macrophages and parasites in fluorescence images of Leishmania- infected macrophages. The proposed strategy is primarily based on blob detection, clustering, and separation using concave regions of the cells' contours. In comparison with the approaches of Nogueira (Master's thesis, Department of University of Porto Computer Science, 2011) and Leal et al. (Proceedings of the 9th international conference on Image Analysis and Recognition, Vol. II, ICIAR'12. Berlin, Heidelberg: Springer-Verlag; 2012. pp. 432–439), which also addressed this type of image, we conclude that the proposed methodology achieves better performance in the automatic annotation of Leishmania infections.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1002/cyto.a.22465" target="_blank">10.1002/cyto.a.22465</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Leishmania_CytometryA.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2014" 
         data-type="journal"
         data-title="ocular biometrics by score-level fusion of disparate experts"
         data-authors="hugo proença"
         data-venue="ieee transactions on image processing"
         data-tags="ocular biometrics,score-level fusion,periocular recognition,iris recognition"
         data-id="117">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-78.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Ocular Biometrics by Score-Level Fusion of Disparate Experts</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Image Processing</span> (2014)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Ocular Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Score-Level Fusion</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIP.2014.2361285" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Atomistic_TIP.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="117">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="117">
            <h2>Ocular Biometrics by Score-Level Fusion of Disparate Experts</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">IEEE Transactions on Image Processing (2014)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The concept of periocular biometrics emerged to improve the robustness of iris recognition to degraded data. Being a relatively recent topic, most of the periocular recognition algorithms work in a holistic way, and apply a feature encoding / matching strategy without considering each biological component in the periocular area. This not only augments the correlation between the components in the resulting biometric signature, but also increases the sensitivity to particular data covariates. The main novelty in this paper is to propose a periocular recognition ensemble made of two disparate components: 1) one expert analyses the iris texture and exhaustively exploits the multi-spectral information in visible-light data; 2) another expert parameterises the shape of eyelids and defines a surrounding dimensionless region-of-interest, from where statistics of the eyelids, eyelashes and skin wrinkles / furrows are encoded. Both experts work on disjoint regions of the periocular area and meet three important properties: 1) they produce practically independent responses, which is behind the better performance of the ensemble when compared to the best individual recogniser; 2) they don't share particularly sensitivity to any image covariate, which accounts for augmenting the robustness against degraded data. Finally, it should be stressed that we disregard information in the periocular region that can be easily forged (e.g., shape of eyebrows), which constitutes an active anti-counterfeit measure. An empirical evaluation was conducted on two public data sets (FRGC and UBIRIS.v2), and points for consistent improvements in performance of the proposed ensemble over the state-of-the-art periocular recognition algorithms.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIP.2014.2361285" target="_blank">10.1109/TIP.2014.2361285</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Atomistic_TIP.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2014" 
         data-type="journal"
         data-title="face recognition: handling data misalignments implicitly by fusion of sparse representations"
         data-authors="hugo proença, joão c. neves, juan c. moreno"
         data-venue="iet computer vision"
         data-tags="face recognition,sparse representations,data misalignments,fusion"
         data-id="118">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-77.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Face Recognition: Handling Data Misalignments Implicitly by Fusion of Sparse Representations</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, João C. Neves, Juan C. Moreno</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IET Computer Vision</span> (2014)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Face Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Sparse Representations</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Data Misalignments</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Fusion</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1049/iet-cvi.2014.0039" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/FaceSparse_IETCV.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="118">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="118">
            <h2>Face Recognition: Handling Data Misalignments Implicitly by Fusion of Sparse Representations</h2>
            <p class="authors">Hugo Proença, João C. Neves, Juan C. Moreno</p>
            <p class="venue">IET Computer Vision (2014)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Sparse representations for classification (SRC) are considered a relevant advance to the biometrics field, but are particularly sensitive to data misalignments. In previous studies, such misalignments were compensated for by finding appropriate geometric transforms between the elements in the dictionary and the query image, which is costly in terms of computational burden. This study describes an algorithm that compensates for data misalignments in SRC in an implicit way, that is, without finding/applying any geometric transform at every recognition attempt. The authors' study is based on three concepts: (i) sparse representations; (ii) projections on orthogonal subspaces; and (iii) discriminant locality preserving with maximum margin projections. When compared with the classical SRC algorithm, apart from providing slightly better performance, the proposed method is much more robust against global/local data misalignments. In addition, it attains performance close to the state-of-the-art algorithms at a much lower computational cost, offering a potential solution for real- time scenarios and large-scale applications.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1049/iet-cvi.2014.0039" target="_blank">10.1049/iet-cvi.2014.0039</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/FaceSparse_IETCV.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2014" 
         data-type="journal"
         data-title="fast and globally convex multiphase active contours for brain mri segmentation"
         data-authors="juan c. moreno, v. b. surya prasath, hugo proença, k. palaniappan"
         data-venue="elsevier computer vision and image understanding"
         data-tags="brain mri segmentation,active contours,image segmentation,medical imaging"
         data-id="119">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-76.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Fast and Globally Convex Multiphase Active Contours for Brain MRI Segmentation</h3>
                <p class="text-gray-600 mb-2">Juan C. Moreno, V. B. Surya Prasath, Hugo Proença, K. Palaniappan</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Computer Vision and Image Understanding</span> (2014)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Brain MRI Segmentation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Active Contours</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Segmentation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Medical Imaging</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.cviu.2014.04.010" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/MRI_Segmentation.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="119">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="119">
            <h2>Fast and Globally Convex Multiphase Active Contours for Brain MRI Segmentation</h2>
            <p class="authors">Juan C. Moreno, V. B. Surya Prasath, Hugo Proença, K. Palaniappan</p>
            <p class="venue">Elsevier Computer Vision and Image Understanding (2014)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Multiphase active contour based models are useful in identifying multiple regions with spatial consistency but varying characteristics such as the mean intensities of regions. Segmenting brain magnetic resonance images (MRIs) using a multiphase approach is useful to differentiate white and gray matter tissue for anatomical, functional and disease studies. Multiphase active contour methods are superior to other approaches due to their topological flexibility, accurate boundaries, robustness to image variations and adaptive energy functionals. Globally convex methods are furthermore initialization independent. We extend the relaxed globally convex Chan and Vese two-phase piecewise constant energy minimization formulation of Chan et al. (2006) [1] to the multiphase domain and prove the existence of a global minimizer in a specific space which is one of the novel contributions of the paper. An efficient dual minimization implementation of our binary partitioning function model accurately describes disjoint regions using stable segmentations by avoiding local minima solutions. Experimental results indicate that the proposed approach provides consistently better accuracy than other related multiphase active contour algorithms using four different error metrics (Dice, Rand Index, Global Consistency Error and Variation of Information) even under severe noise, intensity inhomogeneities, and partial volume effects in MRI imagery.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.cviu.2014.04.010" target="_blank">10.1016/j.cviu.2014.04.010</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/MRI_Segmentation.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2014" 
         data-type="book_chapter"
         data-title="biometric identification from facial sketches of poor fidelity: comparison of human and machine performance"
         data-authors="hugo proença, joão c. neves, joão sequeiros, nuno carapito, nuno c. garcia"
         data-venue="signal and image processing for biometrics : state of the art and recent advances, jacob scharcanski, hugo proença, eliza yingzi du (eds.), springer verlag book series"
         data-tags="facial sketches,biometric identification,human vs machine performance"
         data-id="120">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-4.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Biometric Identification from Facial Sketches of Poor Fidelity: Comparison of Human and Machine Performance</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, João C. Neves, João Sequeiros, Nuno Carapito, Nuno C. Garcia</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Signal and Image Processing for Biometrics : State of the Art and Recent Advances, Jacob Scharcanski, Hugo Proença, Eliza Yingzi Du (Eds.), Springer Verlag book series</span> (2014)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge book_chapter">Book chapter</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Facial Sketches</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Identification</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Human vs Machine Performance</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/BC_Sketches.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="120">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="120">
            <h2>Biometric Identification from Facial Sketches of Poor Fidelity: Comparison of Human and Machine Performance</h2>
            <p class="authors">Hugo Proença, João C. Neves, João Sequeiros, Nuno Carapito, Nuno C. Garcia</p>
            <p class="venue">Signal and Image Processing for Biometrics : State of the Art and Recent Advances, Jacob Scharcanski, Hugo Proença, Eliza Yingzi Du (Eds.), Springer Verlag book series (2014)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Facial sketch recognition refers to the establishment of a link between a drew representation of a human face and an identity, based on information given by a eyewitness of some illegal act. It is a topic of growing interest, and various software frameworks to synthesize sketches are available nowadays. When com- pared to the traditional hand-made sketches, such sketches resemble more closely the appearance of real mugshots, and led to the possibility of using automated face recognition methods in the identification task. However, there are often deficiencies of witnesses in describing the subjects' appearance, which might bias the main features of sketches with respect to the corresponding identity. This chapter com- pares the human and machine performance in the task of sketch identification (rank- 1 identification). One hundred subjects were considered as gallery data, and five images from each stored in a database. Also, one hundred sketches were drew by non-professionals and used as probe data, each of these resembling an identity in the gallery set. Next, a set of volunteers was asked to identify each sketch, and their answers compared to the rank-1 identification responses given by automated face recognition techniques. Three appearance-based face recognition algorithms were used: 1) Gabor-based description, with l2 norm distance ; 2) sparse representation for classification; and 3) eigenfaces. The sparse representation for classification algorithm yielded the best results, whereas the responses given by the Gabor-based description algorithm were the most correlated to human responses.</p>
            </div>
            
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/BC_Sketches.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2014" 
         data-type="book_chapter"
         data-title="using ocular data for unconstrained biometric recognition"
         data-authors="hugo proença, gil santos, joão c. neves"
         data-venue="face recognition in adverse conditions, maria de marsico, michele nappi, massimo tistarelli (eds.), advances in computational intelligence and robotics book series"
         data-tags="ocular biometrics,periocular recognition,unconstrained biometrics"
         data-id="121">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-3.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Using Ocular Data for Unconstrained Biometric Recognition</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Gil Santos, João C. Neves</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Face Recognition in Adverse Conditions, Maria De Marsico, Michele Nappi, Massimo Tistarelli (Eds.), Advances in Computational Intelligence and Robotics Book Series</span> (2014)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge book_chapter">Book chapter</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Ocular Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Unconstrained Biometrics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/BC_UOD.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="121">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="121">
            <h2>Using Ocular Data for Unconstrained Biometric Recognition</h2>
            <p class="authors">Hugo Proença, Gil Santos, João C. Neves</p>
            <p class="venue">Face Recognition in Adverse Conditions, Maria De Marsico, Michele Nappi, Massimo Tistarelli (Eds.), Advances in Computational Intelligence and Robotics Book Series (2014)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>There are several scenarios where a full facial picture cannot be obtained nor the iris properly imaged. For such cases, a good possibility might be to use the ocular region for recognition, which is a relatively new idea and is regarded as a good trade-off between using the whole face or the iris alone. The area in the vicinity of the eyes is designated as periocular and is particularly useful on less constrained conditions, when image acquisition is unreliable, or to avoid iris pattern spoofing. This chapter provides a comprehensive summary of the most relevant research conducted in the scope of ocular (periocular) recognition methods. We compare the main features of the publicly available data sets and summarize the techniques most frequently used in the recognition algorithms. Also, we present the state-of-the-art results in terms of recognition accuracy and discuss the current issues on this topic, together with some directions for further work.</p>
            </div>
            
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/BC_UOD.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2014" 
         data-type="journal"
         data-title="reigsac: fast discrimination of spurious keypoint correspondences on planar surfaces"
         data-authors="hugo proença"
         data-venue="springer machine vision and applications"
         data-tags="keypoint correspondences,computer vision,ransac,planar surfaces"
         data-id="122">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-74.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">ReigSAC: Fast Discrimination of Spurious Keypoint Correspondences on Planar Surfaces</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Springer Machine Vision and Applications</span> (2014)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Keypoint Correspondences</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Computer Vision</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">RANSAC</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Planar Surfaces</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1007/s00138-014-0593-6" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Keypoints_MVA.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="122">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="122">
            <h2>ReigSAC: Fast Discrimination of Spurious Keypoint Correspondences on Planar Surfaces</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">Springer Machine Vision and Applications (2014)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Various methods were proposed to detect/match special interest points (keypoints) in images and some of them (e.g., SIFT and SURF) are among the most cited tech- niques in computer vision research. This paper describes an algorithm to discriminate between genuine and spurious key- point correspondences on planar surfaces. We draw random samples of the set of correspondences, from which homogra- phies are obtained and their principal eigenvectors extracted. Density estimation on that feature space determines the most likely true transform. Such homography feeds a cost func- tion that gives the goodness of each keypoint correspondence. Being similar to the well-known RANSAC strategy, the key finding is that the main eigenvector of the most (genuine) homographies tends to represent a similar direction. Hence, density estimation in the eigenspace dramatically reduces the number of transforms actually evaluated to obtain reli- able estimations. Our experiments were performed on hard image data sets, and pointed that the proposed approach yields effectiveness similar to the RANSAC strategy, at sig- nificantly lower computational burden, in terms of the pro- portion between the number of homographies generated and those that are actually evaluated.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1007/s00138-014-0593-6" target="_blank">10.1007/s00138-014-0593-6</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Keypoints_MVA.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2014" 
         data-type="book"
         data-title="signal and image processing for biometrics: state of the art and recent advances"
         data-authors="jacob scharcanski, hugo proença, eliza yingzi du"
         data-venue="springer-verlag book series, lecture notes on electrical engineering"
         data-tags="signal processing,image processing,biometrics"
         data-id="123">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-129.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Signal and Image Processing for Biometrics: State of the Art and Recent Advances</h3>
                <p class="text-gray-600 mb-2">Jacob Scharcanski, Hugo Proença, Eliza Yingzi Du</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Springer-Verlag book series, Lecture Notes on Electrical Engineering</span> (2014)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge book">Book</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Signal Processing</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Processing</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="123">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="123">
            <h2>Signal and Image Processing for Biometrics: State of the Art and Recent Advances</h2>
            <p class="authors">Jacob Scharcanski, Hugo Proença, Eliza Yingzi Du</p>
            <p class="venue">Springer-Verlag book series, Lecture Notes on Electrical Engineering (2014)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p></p>
            </div>
            
            
            <div class="links">
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2014" 
         data-type="special_issue"
         data-title="mobile iris challenge evaluation"
         data-authors="maria de marsico, michele nappi, hugo proença"
         data-venue="elsevier pattern recognition letters"
         data-tags="iris recognition,mobile biometrics,challenge evaluation"
         data-id="124">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-123.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Mobile Iris Challenge Evaluation</h3>
                <p class="text-gray-600 mb-2">Maria De Marsico, Michele Nappi, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Pattern Recognition Letters</span> (2014)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge special_issue">Special issue</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Mobile Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Challenge Evaluation</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/MICHE_SI.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="124">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="124">
            <h2>Mobile Iris Challenge Evaluation</h2>
            <p class="authors">Maria De Marsico, Michele Nappi, Hugo Proença</p>
            <p class="venue">Elsevier Pattern Recognition Letters (2014)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p></p>
            </div>
            
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/MICHE_SI.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2014" 
         data-type="special_issue"
         data-title="biometric recognition in-the-wild"
         data-authors="hugo proença"
         data-venue="mdpi symmetry"
         data-tags="biometric recognition,in-the-wild,unconstrained environments"
         data-id="125">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-122.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Biometric Recognition In-The-Wild</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">MDPI Symmetry</span> (2014)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge special_issue">Special issue</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">In-The-Wild</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Unconstrained Environments</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Symmetry_SI.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="125">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="125">
            <h2>Biometric Recognition In-The-Wild</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">MDPI Symmetry (2014)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p></p>
            </div>
            
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Symmetry_SI.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2014" 
         data-type="journal"
         data-title="ocular biometrics by score-level fusion of disparate experts"
         data-authors="hugo proença"
         data-venue="ieee transactions on image processing"
         data-tags="ocular biometrics,score-level fusion,periocular recognition"
         data-id="126">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-78.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Ocular Biometrics by Score-Level Fusion of Disparate Experts</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Image Processing</span> (2014)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Ocular Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Score-Level Fusion</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Recognition</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIP.2014.2361285" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Atomistic_TIP.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="126">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="126">
            <h2>Ocular Biometrics by Score-Level Fusion of Disparate Experts</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">IEEE Transactions on Image Processing (2014)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The concept of periocular biometrics emerged to improve the robustness of iris recognition to degraded data. Being a relatively recent topic, most of the periocular recognition algorithms work in a holistic way, and apply a feature encoding / matching strategy without considering each biological component in the periocular area. This not only augments the correlation between the components in the resulting biometric signature, but also increases the sensitivity to particular data covariates. The main novelty in this paper is to propose a periocular recognition ensemble made of two disparate components: 1) one expert analyses the iris texture and exhaustively exploits the multi-spectral information in visible-light data; 2) another expert parameterises the shape of eyelids and defines a surrounding dimensionless region-of-interest, from where statistics of the eyelids, eyelashes and skin wrinkles / furrows are encoded. Both experts work on disjoint regions of the periocular area and meet three important properties: 1) they produce practically independent responses, which is behind the better performance of the ensemble when compared to the best individual recogniser; 2) they don't share particularly sensitivity to any image covariate, which accounts for augmenting the robustness against degraded data. Finally, it should be stressed that we disregard information in the periocular region that can be easily forged (e.g., shape of eyebrows), which constitutes an active anti-counterfeit measure. An empirical evaluation was conducted on two public data sets (FRGC and UBIRIS.v2), and points for consistent improvements in performance of the proposed ensemble over the state-of-the-art periocular recognition algorithms.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIP.2014.2361285" target="_blank">10.1109/TIP.2014.2361285</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Atomistic_TIP.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2014" 
         data-type="conference"
         data-title="segmenting the periocular region using a hierarchical graphical model fed by texture / shape information and geometrical constraints"
         data-authors="hugo proença, joão c. neves, gil santos"
         data-venue="proceedings of the international joint conference on biometrics - ijcb 2014"
         data-tags="periocular region,segmentation,graphical model,markov random field"
         data-id="127">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-36.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Segmenting the Periocular Region using a Hierarchical Graphical Model Fed by Texture / Shape Information and Geometrical Constraints</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, João C. Neves, Gil Santos</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the International Joint Conference on Biometrics - IJCB 2014</span> (2014)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Region</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Segmentation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Graphical Model</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Markov Random Field</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/BTAS.2014.6996228" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/IJCB14.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="127">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="127">
            <h2>Segmenting the Periocular Region using a Hierarchical Graphical Model Fed by Texture / Shape Information and Geometrical Constraints</h2>
            <p class="authors">Hugo Proença, João C. Neves, Gil Santos</p>
            <p class="venue">Proceedings of the International Joint Conference on Biometrics - IJCB 2014 (2014)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Using the periocular region for biometric recognition is an interesting possibility: this area of the human body is highly discriminative among subjects and relatively stable in appearance. In this paper, the main idea is that improved solutions for defining the periocular region-of-interest and better pose / gaze estimates can be obtained by segment- ing (labelling) all the components in the periocular vicinity. Accordingly, we describe an integrated algorithm for labelling the periocular region, that uses a unique model to discriminate between seven components in a single-shot: iris, sclera, eyelashes, eyebrows, hair, skin and glasses. Our solution fuses texture / shape descriptors and geometrical constraints to feed a two-layered graphical model (Markov Random Field), which energy minimization provides a robust solution against uncontrolled lighting conditions and variations in subjects pose and gaze.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/BTAS.2014.6996228" target="_blank">10.1109/BTAS.2014.6996228</a></p>
            
            <div class="links">
                
                <a href="http://di.ubi.pt/~hugomcp/doc/IJCB14.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2013" 
         data-type="journal"
         data-title="iris biometrics: indexing and retrieving heavily degraded data"
         data-authors="hugo proença"
         data-venue="ieee transactions on information forensics and security"
         data-tags="iris recognition,indexing,biometrics,degraded data"
         data-id="128">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-73.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Iris Biometrics: Indexing and Retrieving Heavily Degraded Data</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Information Forensics and Security</span> (2013)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Indexing</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Degraded Data</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIFS.2013.2283458" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/VWII_TIFS.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="128">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="128">
            <h2>Iris Biometrics: Indexing and Retrieving Heavily Degraded Data</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">IEEE Transactions on Information Forensics and Security (2013)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Most of the methods to index iris biometric signatures were designed for decision environments with a clear separation between genuine and impostor matching scores. However, in case of less controlled data acquisition, images will be degraded and the decision environments poorly separated. This paper proposes an indexing / retrieval method for degraded images and operates at the code level, making it compatible with different feature encoding strategies. Gallery codes are decomposed at multiple scales, and according to their most reliable components at each scale, the position in an n-ary tree determined. In retrieval, the probe is decomposed similarly, and the distances to multi-scale centroids are used to penalize paths in the tree. At the end, only a subset of the branches is traversed up to the last level. When compared to related strategies, the proposed method outperforms them on degraded data, particularly in the performance range most important for biometrics (hit rates above 0.95). Finally, according to the computational cost of the retrieval phase, the number of enrolled identities above which indexing is computationally cheaper than an exhaustive search is determined.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIFS.2013.2283458" target="_blank">10.1109/TIFS.2013.2283458</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/VWII_TIFS.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2013" 
         data-type="conference"
         data-title="creating synthetic iriscodes to feed biometrics experiments"
         data-authors="hugo proença, joão c. neves"
         data-venue="proceedings of the 2013 ieee workshop on biometric measurements and systems for security and medical applications - bioms 2013"
         data-tags="iris recognition,synthetic data,iriscodes,biometric experiments"
         data-id="129">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-34.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Creating Synthetic IrisCodes to Feed Biometrics Experiments</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, João C. Neves</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the 2013 IEEE Workshop on Biometric Measurements and Systems for Security and Medical Applications - BioMS 2013</span> (2013)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Synthetic Data</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">IrisCodes</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Experiments</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/BIOMS.2013.6656141" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/BioMS13.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="129">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="129">
            <h2>Creating Synthetic IrisCodes to Feed Biometrics Experiments</h2>
            <p class="authors">Hugo Proença, João C. Neves</p>
            <p class="venue">Proceedings of the 2013 IEEE Workshop on Biometric Measurements and Systems for Security and Medical Applications - BioMS 2013 (2013)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The collection of iris data suitable to be used in experiments is difficult, mainly due to two factors: 1) the time spent by volunteers in the acquisition process; and 2) security / privacy concerns of volunteers. Even though there are methods to create images of artificial irises, there is no method exclusively focused in the synthesis of the iris biometric signatures (IrisCodes). In experiments related with some phases of the biometric recognition process (e.g., indexing / retrieval), a large number of signatures is required for proper evaluation, which, in case of real data, is extremely hard to obtain. Hence, this paper describes a stochastic method to synthesize IrisCodes, based on the notion of data correlation. These artificial signatures can be used to feed experiments on iris recognition, namely on the iris matching, indexing and retrieval phases. We experimentally confirmed that both the genuine and impostor distributions obtained on the artificial data closely resemble the values obtained in data sets of real irises. Finally, another interesting feature is that the method is easily parametrized to mimic IrisCodes extracted from data of varying levels of quality, i.e., ranging from data acquired in high controlled to unconstrained environments.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/BIOMS.2013.6656141" target="_blank">10.1109/BIOMS.2013.6656141</a></p>
            
            <div class="links">
                
                <a href="http://di.ubi.pt/~hugomcp/doc/BioMS13.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2013" 
         data-type="journal"
         data-title="performance evaluation of image local keypoints detection and matching techniques"
         data-authors="hugo proença"
         data-venue="springer signal, image and video processing"
         data-tags="keypoint detection,keypoint matching,image processing,performance evaluation"
         data-id="130">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-71.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Performance Evaluation of Image Local Keypoints Detection and Matching Techniques</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Springer Signal, Image and Video Processing</span> (2013)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Keypoint Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Keypoint Matching</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Processing</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Performance Evaluation</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1007/s11760-013-0535-1" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/keypoints_SIVP.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="130">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="130">
            <h2>Performance Evaluation of Image Local Keypoints Detection and Matching Techniques</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">Springer Signal, Image and Video Processing (2013)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The extraction of local photometric descriptors from images has been extensively reported in the computer vision literature. The main purpose of this paper is to provide an objective comparison between the performance of four of the most popular algorithms of this kind: SIFT, SURF, BRIEF and DAISY. Constraining our analysis to grayscale data, several major points distinguish this work from the previous evaluation initiatives: (1) A large amount of data were used, representing a broad range of real-world scenes; (2) an automated evaluation procedure was devised, in order to minimize subjectivity; and (3) we analyze the reliability of each algorithm not only in terms of the distances between corresponding feature descriptors but also of their order statistics. Also, the public availability of a new annotated data set is reported, which is suitable for the automated and statistically significant evaluation of keypoint detection and match- ing strategies.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1007/s11760-013-0535-1" target="_blank">10.1007/s11760-013-0535-1</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/keypoints_SIVP.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2013" 
         data-type="journal"
         data-title="iris biometrics: synthesis of degraded ocular images"
         data-authors="luís cardoso, andré barbosa, frutuoso g. silva, antónio pinheiro, hugo proença"
         data-venue="ieee transactions on information forensics and security"
         data-tags="iris recognition,synthetic data,degraded images,biometric experiments"
         data-id="131">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-69.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Iris Biometrics: Synthesis of Degraded Ocular Images</h3>
                <p class="text-gray-600 mb-2">Luís Cardoso, André Barbosa, Frutuoso G. Silva, António Pinheiro, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Information Forensics and Security</span> (2013)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Synthetic Data</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Degraded Images</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Experiments</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIFS.2013.2262942" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/NOISYRIS_TIFS.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="131">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="131">
            <h2>Iris Biometrics: Synthesis of Degraded Ocular Images</h2>
            <p class="authors">Luís Cardoso, André Barbosa, Frutuoso G. Silva, António Pinheiro, Hugo Proença</p>
            <p class="venue">IEEE Transactions on Information Forensics and Security (2013)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Iris recognition is a popular technique for recognizing humans. However, as is the case with most biometric traits, it is difficult to collect data that are suitable for use in experiments due to three factors: 1) the substantial amount of data that is required; 2) the time that is spent in the acquisition process; and 3) the security and privacy concerns of potential volunteers. This paper de- scribes a stochastic method for synthesizing ocular data to support experiments on iris recognition. Specifically, synthetic data are in- tended for use in the most important phases of those experiments: segmentation and signature encoding/matching. The resulting data have an important characteristic: they simulate image acquisition under uncontrolled conditions. We have experimentally confirmed that the proposed strategy can mimic the data degradation factors that usually result from such conditions. Finally, we announce the availability of an online platform for generating degraded synthetic ocular data. This platform is freely accessible worldwide.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIFS.2013.2262942" target="_blank">10.1109/TIFS.2013.2262942</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/NOISYRIS_TIFS.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2013" 
         data-type="journal"
         data-title="iris biometrics: indexing and retrieving heavily degraded data"
         data-authors="hugo proença"
         data-venue="ieee transactions on information forensics and security"
         data-tags="iris biometrics,indexing,retrieval,degraded data"
         data-id="132">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-73.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Iris Biometrics: Indexing and Retrieving Heavily Degraded Data</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Information Forensics and Security</span> (2013)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Indexing</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Retrieval</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Degraded Data</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIFS.2013.2283458" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/VWII_TIFS.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="132">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="132">
            <h2>Iris Biometrics: Indexing and Retrieving Heavily Degraded Data</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">IEEE Transactions on Information Forensics and Security (2013)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Most of the methods to index iris biometric signa- tures were designed for decision environments with a clear sepa- ration between genuine and impostor matching scores. However, in case of less controlled data acquisition, images will be degraded and the decision environments poorly separated. This paper proposes an indexing / retrieval method for degraded images and operates at the code level, making it compatible with different feature encoding strategies. Gallery codes are decomposed at multiple scales, and according to their most reliable components at each scale, the position in an n-ary tree determined. In retrieval, the probe is decomposed similarly, and the distances to multi-scale centroids are used to penalize paths in the tree. At the end, only a subset of the branches is traversed up to the last level. When compared to related strategies, the proposed method outperforms them on degraded data, particularly in the performance range most important for biometrics (hit rates above 0.95). Finally, according to the computational cost of the retrieval phase, the number of enrolled identities above which indexing is computationally cheaper than an exhaustive search is determined.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIFS.2013.2283458" target="_blank">10.1109/TIFS.2013.2283458</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/VWII_TIFS.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2013" 
         data-type="conference"
         data-title="automatic annotation of leishmania infections in fluorescence microscopy images"
         data-authors="joão c. neves, helena castro, hugo proença, miguel coimbra"
         data-venue="proceedings of the international conference on image analysis and recognition - iciar 2013"
         data-tags="fluorescence microscopy,leishmania,image analysis,automatic annotation"
         data-id="133">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-33.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Automatic annotation of Leishmania infections in fluorescence microscopy images</h3>
                <p class="text-gray-600 mb-2">João C. Neves, Helena Castro, Hugo Proença, Miguel Coimbra</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the International Conference on Image Analysis and Recognition - ICIAR 2013</span> (2013)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Fluorescence Microscopy</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Leishmania</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Analysis</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Automatic Annotation</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1007/978-3-642-39094-4_70" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/ICIAR13.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="133">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="133">
            <h2>Automatic annotation of Leishmania infections in fluorescence microscopy images</h2>
            <p class="authors">João C. Neves, Helena Castro, Hugo Proença, Miguel Coimbra</p>
            <p class="venue">Proceedings of the International Conference on Image Analysis and Recognition - ICIAR 2013 (2013)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Leishmania is a unicellular parasite that infects mammals and biologists are interested in determining the effect of drugs in Leishmania infections. This requires the manual annotation of the number of macrophages and parasites in images, in order to obtain the percentage of infection (IP), the average number of parasites per infected cell (NPI) and the infection index (IX). Considering that manual annotation is tedious, time-consuming and often erroneous, in this paper we propose an automatic method for automatic annotation of Leishmania infections us- ing fluorescent microscopy. Moreover, when compared to related works, the proposed method is able to get superior performance under most perspectives.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1007/978-3-642-39094-4_70" target="_blank">10.1007/978-3-642-39094-4_70</a></p>
            
            <div class="links">
                
                <a href="http://di.ubi.pt/~hugomcp/doc/ICIAR13.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2013" 
         data-type="journal"
         data-title="compensating for pose and illumination in unconstrained periocular biometrics"
         data-authors="chandrashekhar padole, hugo proença"
         data-venue="international journal of biometrics"
         data-tags="periocular biometrics,pose compensation,illumination compensation,unconstrained biometrics"
         data-id="134">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-70.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Compensating for Pose and Illumination in Unconstrained Periocular Biometrics</h3>
                <p class="text-gray-600 mb-2">Chandrashekhar Padole, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">International Journal of Biometrics</span> (2013)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Pose Compensation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Illumination Compensation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Unconstrained Biometrics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1504/IJBM.2013.055971" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Chandra_IJB.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="134">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="134">
            <h2>Compensating for Pose and Illumination in Unconstrained Periocular Biometrics</h2>
            <p class="authors">Chandrashekhar Padole, Hugo Proença</p>
            <p class="venue">International Journal of Biometrics (2013)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>In the context of less constrained biometrics recognition, the use of information from the vicinity of the eyes (periocular) is considered with high potential and motivated several recent proposals. In this paper, we focus on two factors that are known to degrade the performance of periocular recognition: varying illumination conditions and subjects pose. Hence, this paper has three major purposes: 1) describe the decreases in performance due to varying illumination and subjects poses; 2) propose two techniques to improve the robustness to these factors; 3) announce the availability of an annotated dataset of periocular data (UBIPosePr), where poses vary in regular intervals, turning it especially suitable to assess the effects of misalignments between camera and subjects in periocular recognition.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1504/IJBM.2013.055971" target="_blank">10.1504/IJBM.2013.055971</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Chandra_IJB.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2013" 
         data-type="conference"
         data-title="periocular biometrics: an emerging technology for unconstrained scenarios"
         data-authors="gil santos, hugo proença"
         data-venue="proceedings of the ieee symposium on computational intelligence in biometrics and identity management - cibim 2013"
         data-tags="periocular biometrics,unconstrained scenarios,biometric recognition"
         data-id="135">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-31.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Periocular Biometrics: An Emerging technology for Unconstrained Scenarios</h3>
                <p class="text-gray-600 mb-2">Gil Santos, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the IEEE Symposium on Computational Intelligence in Biometrics and Identity Management - CIBIM 2013</span> (2013)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Unconstrained Scenarios</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Recognition</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/CIBIM.2013.6607908" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/CIBIM13.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="135">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="135">
            <h2>Periocular Biometrics: An Emerging technology for Unconstrained Scenarios</h2>
            <p class="authors">Gil Santos, Hugo Proença</p>
            <p class="venue">Proceedings of the IEEE Symposium on Computational Intelligence in Biometrics and Identity Management - CIBIM 2013 (2013)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The periocular region has recently emerged as a promising trait for unconstrained biometric recognition, specially on cases where neither the iris and a full facial picture can be obtained. Previous studies concluded that the regions in the vicinity of the human eye - the periocular region- have surprisingly high discriminating ability between individuals, are relatively permanent and easily acquired at large distances. Hence, growing attention has been paid to periocular recognition methods, on the performance levels they are able to achieve, and on the correlation of the responses given by other. This work overviews the most relevant research works in the scope of periocular recognition: summarizes the developed methods, and enumerates the current issues, providing a comparative overview. For contextualization, a brief overview of the biometric field is also given.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/CIBIM.2013.6607908" target="_blank">10.1109/CIBIM.2013.6607908</a></p>
            
            <div class="links">
                
                <a href="http://di.ubi.pt/~hugomcp/doc/CIBIM13.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2013" 
         data-type="conference"
         data-title="facial expressions: discriminability of facial regions and relationship to biometrics recognition"
         data-authors="elisa barroso, gil santos, hugo proença"
         data-venue="proceedings of the ieee symposium on computational intelligence in biometrics and identity management - cibim 2013"
         data-tags="facial expressions,biometric recognition,facial regions,discriminability"
         data-id="136">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-32.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Facial Expressions: Discriminability of Facial Regions and Relationship to Biometrics Recognition</h3>
                <p class="text-gray-600 mb-2">Elisa Barroso, Gil Santos, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the IEEE Symposium on Computational Intelligence in Biometrics and Identity Management - CIBIM 2013</span> (2013)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Facial Expressions</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Facial Regions</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Discriminability</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/CIBIM.2013.6607918" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/CIBIM13_01.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="136">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="136">
            <h2>Facial Expressions: Discriminability of Facial Regions and Relationship to Biometrics Recognition</h2>
            <p class="authors">Elisa Barroso, Gil Santos, Hugo Proença</p>
            <p class="venue">Proceedings of the IEEE Symposium on Computational Intelligence in Biometrics and Identity Management - CIBIM 2013 (2013)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Facial expressions result from movements of muscular action units, in response to internal emotion states or perceptions, and it has been shown that they decrease the performance of face-based biometric recognition techniques. This paper focuses in the recognition of facial expressions and has the following purposes: 1) confirm the suitability of using dense image descriptors widely known in biometrics research (e.g., local binary patterns and histograms of oriented gradients) to recognize facial expressions; 2) compare the effectiveness attained when using different regions of the face to recognize expressions; 3) compare the effectiveness attained when the identity of subjects is known / unknown, before attempting to recognize their facial expressions.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/CIBIM.2013.6607918" target="_blank">10.1109/CIBIM.2013.6607918</a></p>
            
            <div class="links">
                
                <a href="http://di.ubi.pt/~hugomcp/doc/CIBIM13_01.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2013" 
         data-type="journal"
         data-title="periocular biometrics: constraining the egm algorithm to biologically plausible distortions"
         data-authors="hugo proença, juan c. moreno"
         data-venue="iet biometrics"
         data-tags="periocular biometrics,elastic graph matching,biologically plausible distortions"
         data-id="137">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-72.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Periocular Biometrics: Constraining the EGM Algorithm to Biologically Plausible Distortions</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Juan C. Moreno</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IET Biometrics</span> (2013)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Elastic Graph Matching</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biologically Plausible Distortions</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1049/iet-bmt.2013.0039" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/EGM_IETB.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="137">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="137">
            <h2>Periocular Biometrics: Constraining the EGM Algorithm to Biologically Plausible Distortions</h2>
            <p class="authors">Hugo Proença, Juan C. Moreno</p>
            <p class="venue">IET Biometrics (2013)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>In biometrics research, the periocular region has been regarded as an interesting trade-off between the face and the iris, particularly in unconstrained data acquisition setups. As in other biometric traits, the current challenge is the development of more robust recognition algorithms. Having investigated the suitability of the 'elastic graph matching' (EGM) algorithm to handle non- linear distortions in the periocular region because of facial expressions, the authors observed that vertices locations often not correspond to displacements in the biological tissue. Hence, they propose a 'globally coherent' variant of EGM (GC-EGM) that avoids sudden local angular movements of vertices while maintains the ability to faithfully model non-linear distortions. Two main adaptations were carried out: (i) a new term for measuring vertices similarity and (ii) a new term in the edges-cost function penalises changes in orientation between the model and test graphs. Experiments were carried out both in synthetic and real data and point for the advantages of the proposed algorithm. Also, the recognition performance when using the EGM and GC-EGM was compared, and statistically significant improvements in the error rates were observed when using the GC-EGM variant.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1049/iet-bmt.2013.0039" target="_blank">10.1049/iet-bmt.2013.0039</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/EGM_IETB.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2013" 
         data-type="book_chapter"
         data-title="iris recognition in the visible wavelength: issues and trends"
         data-authors="hugo proença"
         data-venue="handbook of iris recognition, mark j. burge and k. bowyer (eds.), springer verlag book series"
         data-tags="iris recognition,visible wavelength,unconstrained recognition"
         data-id="138">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-2.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Iris Recognition in the Visible Wavelength: Issues and Trends</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Handbook of Iris Recognition, Mark J. Burge and K. Bowyer (Eds.), Springer Verlag book series</span> (2013)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge book_chapter">Book chapter</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visible Wavelength</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Unconstrained Recognition</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/BC_Handbook.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="138">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="138">
            <h2>Iris Recognition in the Visible Wavelength: Issues and Trends</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">Handbook of Iris Recognition, Mark J. Burge and K. Bowyer (Eds.), Springer Verlag book series (2013)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The human iris supports contactless data acquisition and can be imaged covertly. Thus, at least theoretically, the subsequent biometric recognition procedure can be performed without subjects' knowledge. The feasibility of this type of recognition has received increasing attention and is of particular interest for forensic and security purposes, such as the pursuit of criminals and terrorists and the search for missing children. Among others, one active research area sought to use visible wavelength (VW) light imagery to acquire data at significantly larger distances than usual and on moving subjects, which is a difficult task because this real-world data is notoriously different from the one used in the near infra-red (NIR) setup. This chapter addresses the feasibility of performing reliable biometric recognition using VW data acquired under dynamic lighting conditions and unconstrained acquisition protocols: with subjects at large distances (between 4 and 8 meters) and on-the-move.</p>
            </div>
            
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/BC_Handbook.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2013" 
         data-type="conference"
         data-title="robust periocular recognition by fusing local to holistic sparse representations"
         data-authors="juan c. moreno, v. b. surya prasath, hugo proença"
         data-venue="proceedings of the 6th international conference on security of information and networks - sin 2013"
         data-tags="periocular recognition,sparse representations,data fusion"
         data-id="139">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-35.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Robust Periocular Recognition by Fusing Local to Holistic Sparse Representations</h3>
                <p class="text-gray-600 mb-2">Juan C. Moreno, V. B. Surya Prasath, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the 6th International Conference on Security of Information and Networks - SIN 2013</span> (2013)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Sparse Representations</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Data Fusion</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1145/2523514.2523540" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/SIN13.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="139">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="139">
            <h2>Robust Periocular Recognition by Fusing Local to Holistic Sparse Representations</h2>
            <p class="authors">Juan C. Moreno, V. B. Surya Prasath, Hugo Proença</p>
            <p class="venue">Proceedings of the 6th International Conference on Security of Information and Networks - SIN 2013 (2013)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Sparse representations have been advocated as a relevant advance in biometrics research. In this paper we propose a new algorithm for fusion at the data level of sparse representations, each one obtained from image patches. The main novelties are two-fold: 1) a dictionary fusion scheme is formalised, using the l1−minimization with the gradient projection method; 2) the proposed representation and classification method does not require the non-overlapping condition of image patches from where individual dictionaries are obtained. In the experiments, we focused in the recognition of periocular images and obtained independent dictionaries for the eye, eyebrow and skin regions, that were subsequently fused. Results obtained in the publicly available UBIRIS.v2 data set show consistent improvements in the recognition effectiveness when compared to state-of- the-art related representation and classification techniques.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1145/2523514.2523540" target="_blank">10.1145/2523514.2523540</a></p>
            
            <div class="links">
                
                <a href="http://di.ubi.pt/~hugomcp/doc/SIN13.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2012" 
         data-type="journal"
         data-title="editorial of the special issue on the recognition of visible wavelength iris images captured at-a-distance and on-the-move"
         data-authors="hugo proença, luís a. alexandre"
         data-venue="elsevier pattern recognition letters"
         data-tags="iris recognition,visible wavelength,nice challenge"
         data-id="140">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-68.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Editorial of the Special Issue On the Recognition of Visible Wavelength Iris Images Captured At-a-distance and On-the-move</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Pattern Recognition Letters</span> (2012)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visible Wavelength</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">NICE Challenge</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.patrec.2012.03.003" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Introd_PRL.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="140">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="140">
            <h2>Editorial of the Special Issue On the Recognition of Visible Wavelength Iris Images Captured At-a-distance and On-the-move</h2>
            <p class="authors">Hugo Proença, Luís A. Alexandre</p>
            <p class="venue">Elsevier Pattern Recognition Letters (2012)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>This special issue regards the recognition of degraded iris images acquired in visible wavelengths. During 2009 and 2010, the University of Beira Interior (Portugal) promoted two International evaluation initiatives about this subject, named Noisy Iris Challenge Evaluation (NICE) I and II. The first one focussed on the evaluation of iris segmentation strategies, considering that iris data acquired in visible wavelengths (VW) usually has much higher level of detail than traditionally used near infra-red data (NIR), but also has many more noise artefacts, including specular and diffuse reflections and shadows. Also, the spectral reflectance of the sclera is significantly higher in the VW than in the NIR and the spectral radiance of the iris with respect to the levels of its pigmentation varies much more significantly in the VW than in the NIR.The NICE:II contest complemented its predecessor in terms of the traditional pattern recognition stages, evaluating different signature encoding and matching strategies. In order to guarantee that unbiased performance measures were obtained, all the participants used the exact same segmented data, which were automatically obtained according to the highest performing method in the NICE:I. Again, participation in NICE:II was free of charge and opened to all research and academic institutions. Sixty-seven participants from thirty countries registered in the contest 1 and received a training set composed of 1000 images and the corresponding binary iris segmentation masks.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.patrec.2012.03.003" target="_blank">10.1016/j.patrec.2012.03.003</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Introd_PRL.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2012" 
         data-type="conference"
         data-title="multimodal ocular biometrics approach: a feasibility study"
         data-authors="oleg v. komogortsev, alex karpov, corey holland, hugo proença"
         data-venue="ieee fifth international conference on biometrics: theory, applications and systems (btas)"
         data-tags="ocular biometrics,multimodal biometrics,oculomotor plant characteristics"
         data-id="141">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-30.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Multimodal Ocular Biometrics Approach: A Feasibility Study</h3>
                <p class="text-gray-600 mb-2">Oleg V. Komogortsev, Alex Karpov, Corey Holland, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Fifth International Conference on Biometrics: Theory, Applications and Systems (BTAS)</span> (2012)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Ocular Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Multimodal Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Oculomotor Plant Characteristics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/BTAS.2012.6374579" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/BTAS12.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="141">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="141">
            <h2>Multimodal Ocular Biometrics Approach: A Feasibility Study</h2>
            <p class="authors">Oleg V. Komogortsev, Alex Karpov, Corey Holland, Hugo Proença</p>
            <p class="venue">IEEE Fifth International Conference on Biometrics: Theory, Applications and Systems (BTAS) (2012)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Growing efforts have been concentrated on the development of alternative biometric recognition strategies, the intended goal to increase the accuracy and counterfeit-resistance of existing systems without increased cost. In this paper, we propose and evaluate a novel biometric approach using three fundamentally different traits captured by the same camera sensor. Considered traits include: 1) the internal, non-visible, anatomical properties of the human eye, represented by Oculomotor Plant Characteristics (OPC); 2) the visual attention strategies employed by the brain, represented by Complex Eye Movement pat- terns (CEM); and, 3) the unique physical structure of the iris. Our experiments, performed using a low-cost web camera, indicate that the combined ocular traits improve the accuracy of the resulting system. As a result, the combined ocular traits have the potential to enhance the ac- curacy and counterfeit-resistance of existing and future biometric systems.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/BTAS.2012.6374579" target="_blank">10.1109/BTAS.2012.6374579</a></p>
            
            <div class="links">
                
                <a href="http://di.ubi.pt/~hugomcp/doc/BTAS12.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2012" 
         data-type="journal"
         data-title="fusing color and shape descriptors in the recognition of degraded iris images acquired at visible wavelength"
         data-authors="hugo proença, gil santos"
         data-venue="elsevier computer vision and image understanding"
         data-tags="iris recognition,visible wavelength,color features,shape features"
         data-id="142">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-66.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Fusing Color and Shape Descriptors in the Recognition of Degraded Iris Images Acquired at Visible Wavelength</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Gil Santos</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Computer Vision and Image Understanding</span> (2012)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visible Wavelength</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Color Features</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Shape Features</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.cviu.2011.10.008" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/VWIR_CVIU.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="142">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="142">
            <h2>Fusing Color and Shape Descriptors in the Recognition of Degraded Iris Images Acquired at Visible Wavelength</h2>
            <p class="authors">Hugo Proença, Gil Santos</p>
            <p class="venue">Elsevier Computer Vision and Image Understanding (2012)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Despite the substantial research into the development of covert iris recognition technologies, no machine to date has been able to reliably perform recognition of human beings in real-world data. This limitation is especially evident in the application of such technology to large-scale identification scenarios, which demand extremely low error rates to avoid frequent false alarms. Most previously published works have used intensity data and performed multi-scale analysis to achieve recognition, obtaining encouraging performance values that are nevertheless far from desirable. This paper presents two key innovations. (1) A recognition scheme is proposed based on techniques that are substantially different from those traditionally used, starting with the dynamic partition of the noise-free iris into disjoint regions from which MPEG-7 color and shape descriptors are extracted. (2) The minimal levels of linear correlation between the outputs produced by the proposed strategy and other state-of-the-art techniques suggest that the fusion of both recognition techniques significantly improve performance, which is regarded as a positive step towards the development of extremely ambitious types of biometric recognition.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.cviu.2011.10.008" target="_blank">10.1016/j.cviu.2011.10.008</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/VWIR_CVIU.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2012" 
         data-type="conference"
         data-title="periocular recognition: analysis of performance degradation factors"
         data-authors="chandrashekhar padole, hugo proença"
         data-venue="fifth iapr/ieee international conference on biometrics (icb)"
         data-tags="periocular recognition,biometrics,performance analysis"
         data-id="143">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-29.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Periocular Recognition: Analysis of Performance Degradation Factors</h3>
                <p class="text-gray-600 mb-2">Chandrashekhar Padole, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Fifth IAPR/IEEE International Conference on Biometrics (ICB)</span> (2012)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Periocular Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Performance Analysis</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/ICB.2012.6199790" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/ICB12.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="143">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="143">
            <h2>Periocular Recognition: Analysis of Performance Degradation Factors</h2>
            <p class="authors">Chandrashekhar Padole, Hugo Proença</p>
            <p class="venue">Fifth IAPR/IEEE International Conference on Biometrics (ICB) (2012)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Among the available biometric traits such as face, iris and fingerprint, there is an active research being carried out in the direction of unconstrained biometrics. Periocular recognition has proved its effectiveness and is regarded as complementary to iris recognition. The main objectives of this paper are three-fold: 1) to announce the availability of periocular dataset, which has a variability in terms of scale change (due to camera-subject distance), pose variation and non-uniform illumination; 2) to investigate the perfor- mance of periocular recognition methods with the presence of various degradation factors; 3) propose a new initialization strategy for the definition of the periocular region-of-interest (ROI), based on the geometric mean of eye corners. Our experiments confirm that performance can be consistently improved by this initialization method, when compared to the classical technique.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/ICB.2012.6199790" target="_blank">10.1109/ICB.2012.6199790</a></p>
            
            <div class="links">
                
                <a href="http://di.ubi.pt/~hugomcp/doc/ICB12.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2012" 
         data-type="journal"
         data-title="toward covert iris biometric recognition: experimental results from the nice contests"
         data-authors="hugo proença, luís a. alexandre"
         data-venue="ieee transactions on information forensics and security"
         data-tags="iris recognition,visible wavelength,nice contest"
         data-id="144">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-67.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Toward Covert Iris Biometric Recognition: Experimental Results From the NICE Contests</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Information Forensics and Security</span> (2012)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visible Wavelength</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">NICE Contest</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIFS.2011.2177659" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/NICE_TIFS.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="144">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="144">
            <h2>Toward Covert Iris Biometric Recognition: Experimental Results From the NICE Contests</h2>
            <p class="authors">Hugo Proença, Luís A. Alexandre</p>
            <p class="venue">IEEE Transactions on Information Forensics and Security (2012)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>This paper announces and discusses the experimental results from the Noisy Iris Challenge Evaluation (NICE), an iris biometric evaluation initiative that received worldwide partici- pation and whose main innovation is the use of heavily degraded data acquired in the visible wavelength and uncontrolled setups, with subjects moving and at widely varying distances. The NICE contest included two separate phases: 1) the NICE.I evaluated iris segmentation and noise detection techniques and 2) the NICE:II evaluated encoding and matching strategies for biometric signatures. Further, we give the performance values observed when fusing recognition methods at the score level, which was observed to outperform any isolated recognition strategy. These results provide an objective estimate of the potential of such recognition systems and should be regarded as reference values for further improvements of this technology, which—if successful—may significantly broaden the applicability of iris biometric systems to domains where the subjects cannot be expected to cooperate.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIFS.2011.2177659" target="_blank">10.1109/TIFS.2011.2177659</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/NICE_TIFS.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2012" 
         data-type="special_issue"
         data-title="special issue on the recognition of visible wavelength iris images captured at-a-distance and on-the-move"
         data-authors="hugo proença, luís a. alexandre"
         data-venue="elsevier pattern recognition letters"
         data-tags="iris recognition,visible wavelength,nice challenge"
         data-id="145">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-126.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Special Issue On the Recognition of Visible Wavelength Iris Images Captured At-a-distance and On-the-move</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Pattern Recognition Letters</span> (2012)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge special_issue">Special issue</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visible Wavelength</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">NICE Challenge</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    <a href="http://www.journals.elsevier.com/pattern-recognition-letters/" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="145">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="145">
            <h2>Special Issue On the Recognition of Visible Wavelength Iris Images Captured At-a-distance and On-the-move</h2>
            <p class="authors">Hugo Proença, Luís A. Alexandre</p>
            <p class="venue">Elsevier Pattern Recognition Letters (2012)</p>
            
            
            <div class="links">
                
                <a href="http://www.journals.elsevier.com/pattern-recognition-letters/" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2011" 
         data-type="conference"
         data-title="non-cooperative iris recognition: issues and trends"
         data-authors="hugo proença"
         data-venue="eusipco'11 - nineteenth european signal processing conference"
         data-tags="iris recognition,biometrics,non-cooperative recognition"
         data-id="146">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-27.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Non-Cooperative Iris Recognition: Issues and trends</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">EUSIPCO'11 - Nineteenth European Signal Processing Conference</span> (2011)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Non-Cooperative Recognition</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/EUSIPCO11.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="146">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="146">
            <h2>Non-Cooperative Iris Recognition: Issues and trends</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">EUSIPCO'11 - Nineteenth European Signal Processing Conference (2011)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>To date, no research effort has produced a machine able to covertly recognize human beings. Contrary to popular belief, such automata are confined to science fiction, although it's not hard to anticipate the potential impact that they would have in the security and safety of modern societies (forensics and surveillance). Among the research programs that pursuit such type of biometric recognition, previous initiatives sought to acquire data from moving subjects, at long distances and under uncontrolled lighting conditions. This real-world scenario brings many challenges to the Pattern Recognition process, essentially due to poor quality of the acquired data. Several programs now seek to increase the robustness to noise of each phase of the recognition process (detection, segmentation, normalization, encoding and matching). This paper addresses the feasibility of such extremely ambitious type of biometric recognition, discusses the major issues behind the development of this technology and points some directions for further improvements.</p>
            </div>
            
            
            <div class="links">
                
                <a href="http://di.ubi.pt/~hugomcp/doc/EUSIPCO11.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2011" 
         data-type="special_issue"
         data-title="special issue on unconstrained biometrics: advances and trends"
         data-authors="hugo proença, eliza yingzi du, jacob scharcanski"
         data-venue="springer signal, image and video processing"
         data-tags="unconstrained biometrics,signal processing,image processing"
         data-id="147">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-125.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Special Issue On Unconstrained Biometrics: Advances and Trends</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Eliza Yingzi Du, Jacob Scharcanski</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Springer Signal, Image and Video Processing</span> (2011)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge special_issue">Special issue</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Unconstrained Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Signal Processing</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Processing</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    <a href="http://www.springer.com/engineering/signals/journal/11760" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="147">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="147">
            <h2>Special Issue On Unconstrained Biometrics: Advances and Trends</h2>
            <p class="authors">Hugo Proença, Eliza Yingzi Du, Jacob Scharcanski</p>
            <p class="venue">Springer Signal, Image and Video Processing (2011)</p>
            
            
            <div class="links">
                
                <a href="http://www.springer.com/engineering/signals/journal/11760" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2011" 
         data-type="conference"
         data-title="a robust eye-corner detection method for real-world data"
         data-authors="gil santos, hugo proença"
         data-venue="ieee international joint conference on biometrics (ijcb)"
         data-tags="eye corner detection,computer vision,biometrics"
         data-id="148">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-28.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">A Robust Eye-Corner Detection Method for Real-World Data</h3>
                <p class="text-gray-600 mb-2">Gil Santos, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE International Joint Conference on Biometrics (IJCB)</span> (2011)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Eye Corner Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Computer Vision</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/IJCB.2011.6117596" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/IJCB11.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="148">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="148">
            <h2>A Robust Eye-Corner Detection Method for Real-World Data</h2>
            <p class="authors">Gil Santos, Hugo Proença</p>
            <p class="venue">IEEE International Joint Conference on Biometrics (IJCB) (2011)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Corner detection has been motivating several research works and is particularly important in different computer vision tasks, acting as basis for further image understanding stages. Particularly, the detection of eye-corners in facial images is relevant for domains such as biometric systems and assisted-driving systems. Having empirically evaluated the state-of-the-art of eye-corner detection proposals, we observed that they only achieve satisfactory results when dealing with good quality data. Hence, in this paper we describe an eye- corner detection method with particular focus on robustness, i.e., the suitability to deal with degraded data, toward the applicability in real-world conditions. Our experiments show that the proposed method outperforms others either in noise- free and degraded data (blurred, rotated and with significant variations in scale), which is regarded as the main achievement.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/IJCB.2011.6117596" target="_blank">10.1109/IJCB.2011.6117596</a></p>
            
            <div class="links">
                
                <a href="http://di.ubi.pt/~hugomcp/doc/IJCB11.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2011" 
         data-type="journal"
         data-title="introduction to the special issue on unconstrained biometrics: advances and trends"
         data-authors="hugo proença, eliza yingzi du, jacob scharcanski"
         data-venue="springer signal, image and video processing"
         data-tags="unconstrained biometrics,signal processing,image processing"
         data-id="149">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-65.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Introduction to the Special Issue on Unconstrained Biometrics: Advances and Trends</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Eliza Yingzi Du, Jacob Scharcanski</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Springer Signal, Image and Video Processing</span> (2011)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Unconstrained Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Signal Processing</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Processing</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1007/s11760-011-0243-7" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Introd_SIVP.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="149">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="149">
            <h2>Introduction to the Special Issue on Unconstrained Biometrics: Advances and Trends</h2>
            <p class="authors">Hugo Proença, Eliza Yingzi Du, Jacob Scharcanski</p>
            <p class="venue">Springer Signal, Image and Video Processing (2011)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>To date, no research effort has produced a machine able to autonomously and covertly perform reliable recognition of human beings. Perhaps, contrary to popular belief, such automata are confined to science fiction, although it is not hard to anticipate the potential impact that they would have in modern societies (e.g., forensics and surveillance). Some of the biological traits used to perform biometric recognition support contactless data acquisition and can be imaged covertly. Thus, at least theoretically, the subsequent biometric recognition procedure can be performed without subjects' knowledge and in uncontrolled scenarios. This real-world scenario brings many challenges to the Pattern Recognition process, essentially due to poor quality of the acquired data. The feasibility of this type of recognition has received increasing attention and is of particular interest in visual surveillance, computer forensics, threat assessment, and other security areas. Though a growing number of researchers are concerned about the development of biometric recognition systems that operate in unconstrained conditions, many problems remain to be solved: how to deal with varying illumination sources, variations in poses and distances or blurred and low-quality data resultant of such acquisition conditions. This special issue is particularly devoted to emerging strategies to perform biometric recognition under uncontrolled data acquisition conditions, ideally fully covert ones. Topics of interest include the following: less controlled/covert data acquisition frameworks; biometric data quality assessment; normalization of poor-quality biometric data; contactless biometric recognition; analysis of recognition robustness; multimodal and multispectral biometrics.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1007/s11760-011-0243-7" target="_blank">10.1007/s11760-011-0243-7</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Introd_SIVP.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2011" 
         data-type="journal"
         data-title="quality assessment of degraded iris images acquired in the visible wavelength"
         data-authors="hugo proença"
         data-venue="ieee transactions on information forensics and security"
         data-tags="iris recognition,biometrics,image quality,visible wavelength"
         data-id="150">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-64.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Quality Assessment of Degraded Iris Images Acquired in the Visible Wavelength</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Information Forensics and Security</span> (2011)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Quality</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visible Wavelength</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TIFS.2010.2086446" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/VWIQ_IEEETIFS.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="150">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="150">
            <h2>Quality Assessment of Degraded Iris Images Acquired in the Visible Wavelength</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">IEEE Transactions on Information Forensics and Security (2011)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Data quality assessment is a key issue, in order to broad the applicability of iris biometrics to unconstrained imaging conditions. Previous research efforts sought to use visible wavelength (VW) light imagery to acquire data at significantly larger distances than usual and on moving subjects, which makes this real world data notoriously different from the acquired in the near infra-red (NIR) setup. Having empirically observed that published strategies to assess iris image quality do not handle the specificity of such data, this paper proposes a method to assess the quality of VW iris samples captured in unconstrained conditions, according to the factors that are known to determine the quality of iris biometric data: focus, motion, angle, occlusions, area, pupillary dilation and levels of iris pigmentation. The key insight is to use the output of the segmentation phase in each assessment, which permits to handle severely degraded samples that are likely to result of such imaging setup. Also, our experiments point that the given method improves the effectiveness of VW iris recognition, by avoiding that poor quality samples are considered in the recognition process.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TIFS.2010.2086446" target="_blank">10.1109/TIFS.2010.2086446</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/VWIQ_IEEETIFS.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2011" 
         data-type="conference"
         data-title="ubear: a dataset of ear images captured on-the-move in uncontrolled conditions"
         data-authors="rui raposo, edmundo hoyle, adolfo peixinho, hugo proença"
         data-venue="ieee workshop on computational intelligence in biometrics and identity management - ssci 2011 cibim"
         data-tags="ear biometrics,dataset,unconstrained recognition"
         data-id="151">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-26.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">UBEAR: A Dataset of Ear Images Captured On-the-move in Uncontrolled Conditions</h3>
                <p class="text-gray-600 mb-2">Rui Raposo, Edmundo Hoyle, Adolfo Peixinho, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Workshop on Computational Intelligence in Biometrics and Identity Management - SSCI 2011 CIBIM</span> (2011)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Ear Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Dataset</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Unconstrained Recognition</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/CIBIM.2011.5949208" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/UBEAR_SSCI10.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="151">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="151">
            <h2>UBEAR: A Dataset of Ear Images Captured On-the-move in Uncontrolled Conditions</h2>
            <p class="authors">Rui Raposo, Edmundo Hoyle, Adolfo Peixinho, Hugo Proença</p>
            <p class="venue">IEEE Workshop on Computational Intelligence in Biometrics and Identity Management - SSCI 2011 CIBIM (2011)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>In order to broad the applicability of biometric systems, the data acquisition constraints required for reliable recognition are receiving increasing attention. For some of the traits (e.g., face and iris) significant research efforts were already made toward the development of systems able to operate in completely unconstrained conditions. For other traits (e.g., the ear) no similar efforts are known. The main purpose of this paper is to announce the availability of a new data set of ear images, which main distinguishing feature is that its images were acquired from on-the-move subjects, under varying lighting conditions and without demanding to subjects any particular care regarding ear occlusions and poses. The data set is freely available to the research community and should constitute a valuable tool in assessing the possibility of performing reliable ear biometric recognition in such d challenging conditions.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/CIBIM.2011.5949208" target="_blank">10.1109/CIBIM.2011.5949208</a></p>
            
            <div class="links">
                
                <a href="http://di.ubi.pt/~hugomcp/doc/UBEAR_SSCI10.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2010" 
         data-type="journal"
         data-title="the ubiris.v2: a database of visible wavelength iris images captured on-the-move and at-a-distance"
         data-authors="hugo proença, sílvio filipe, ricardo santos, joão oliveira, luís a. alexandre"
         data-venue="ieee transactions on pattern analysis and machine intelligence"
         data-tags="iris recognition,biometrics,image database,visible wavelength"
         data-id="152">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/pami.jpg" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">The UBIRIS.v2: A Database of Visible Wavelength Iris Images Captured On-The-Move and At-A-Distance</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Sílvio Filipe, Ricardo Santos, João Oliveira, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Pattern Analysis and Machine Intelligence</span> (2010)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Database</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visible Wavelength</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TPAMI.2009.66" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/UBIRISv2_IEEETPAMI.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="152">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="152">
            <h2>The UBIRIS.v2: A Database of Visible Wavelength Iris Images Captured On-The-Move and At-A-Distance</h2>
            <p class="authors">Hugo Proença, Sílvio Filipe, Ricardo Santos, João Oliveira, Luís A. Alexandre</p>
            <p class="venue">IEEE Transactions on Pattern Analysis and Machine Intelligence (2010)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>This paper announces the availability of the UBIRIS.v2 database, a multisession iris images database which singularly contains data captured in the visible wavelength, at-a-distance (between four and eight meters) and on-the-move. This database is freely available for research purposes and will be useful to evaluate the feasibility of visible wavelength iris recognition in the aforementioned capturing conditions that significantly increase the probability of noise factors.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TPAMI.2009.66" target="_blank">10.1109/TPAMI.2009.66</a></p>
            
            <div class="links">
                
                <a href="http://di.ubi.pt/~hugomcp/doc/UBIRISv2_IEEETPAMI.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2010" 
         data-type="journal"
         data-title="the ubiris.v2: a database of visible wavelength iris images captured on-the-move and at-a-distance"
         data-authors="hugo proença, sílvio filipe, ricardo santos, joão oliveira, luís a. alexandre"
         data-venue="ieee transactions on pattern analysis and machine intelligence"
         data-tags="iris recognition,dataset,visible wavelength"
         data-id="153">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-62.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">The UBIRIS.v2: A Database of Visible Wavelength Iris Images Captured On-The-Move and At-A-Distance</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Sílvio Filipe, Ricardo Santos, João Oliveira, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Pattern Analysis and Machine Intelligence</span> (2010)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Dataset</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visible Wavelength</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TPAMI.2009.66" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/UBIRISv2_IEEETPAMI.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="153">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="153">
            <h2>The UBIRIS.v2: A Database of Visible Wavelength Iris Images Captured On-The-Move and At-A-Distance</h2>
            <p class="authors">Hugo Proença, Sílvio Filipe, Ricardo Santos, João Oliveira, Luís A. Alexandre</p>
            <p class="venue">IEEE Transactions on Pattern Analysis and Machine Intelligence (2010)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The iris is regarded as one of the most useful traits for biometric recognition and the dissemination of nationwide iris-based recognition systems is imminent. However, currently deployed systems rely on heavy imaging constraints to capture near infrared images with enough quality. Also, all of the publicly available iris image databases contain data correspondent to such imaging constraints and therefore are exclusively suitable to evaluate methods thought to operate on these type of environments. The main purpose of this paper is to announce the availability of the UBIRIS.v2 database, a multisession iris images database which singularly contains data captured in the visible wavelength, at-a-distance (between four and eight meters) and on on-the-move. This database is freely available for researchers concerned about visible wavelength iris recognition and will be useful in accessing the feasibility and specifying the constraints of this type of biometric recognition.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TPAMI.2009.66" target="_blank">10.1109/TPAMI.2009.66</a></p>
            
            <div class="links">
                
                <a href="http://di.ubi.pt/~hugomcp/doc/UBIRISv2_IEEETPAMI.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2010" 
         data-type="journal"
         data-title="iris recognition: on the segmentation of degraded images acquired in the visible wavelength"
         data-authors="hugo proença"
         data-venue="ieee transactions on pattern analysis and machine intelligence"
         data-tags="iris recognition,image segmentation,visible wavelength"
         data-id="154">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-63.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Iris Recognition: On the Segmentation of Degraded Images Acquired in the Visible Wavelength</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Pattern Analysis and Machine Intelligence</span> (2010)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Segmentation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visible Wavelength</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TPAMI.2009.140" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/VWIS_IEEETPAMI.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="154">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="154">
            <h2>Iris Recognition: On the Segmentation of Degraded Images Acquired in the Visible Wavelength</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">IEEE Transactions on Pattern Analysis and Machine Intelligence (2010)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Iris recognition imaging constraints are receiving increasing attention. There are several proposals to develop systems that operate in the visible wavelength and in less constrained environments. These imaging conditions engender acquired noisy artefacts that lead to severely degraded images, making iris segmentation a major issue. Having observed that existing iris segmentation methods tend to fail in these challenging conditions, we present a segmentation method that can handle degraded images acquired in less constrained conditions. We offer the following contributions: 1) to consider the sclera the most easily distinguishable part of the eye in degraded images, 2) to propose a new type of feature that measures the proportion of sclera in each direction and is fundamental in segmenting the iris, and 3) to run the entire procedure in deterministically linear time in respect to the size of the image, making the procedure suitable for real-time applications.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TPAMI.2009.140" target="_blank">10.1109/TPAMI.2009.140</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/VWIS_IEEETPAMI.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2010" 
         data-type="conference"
         data-title="iris recognition: preliminary assessment about the discriminating capacity of visible wavelength data"
         data-authors="gil santos, marco bernardo, paulo fiadeiro, hugo proença"
         data-venue="sixth ieee international workshop on multimedia information processing and retrieval - mipr 2010"
         data-tags="iris recognition,visible wavelength,biometrics"
         data-id="155">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-25.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Iris Recognition: Preliminary Assessment about the Discriminating Capacity of Visible Wavelength Data</h3>
                <p class="text-gray-600 mb-2">Gil Santos, Marco Bernardo, Paulo Fiadeiro, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Sixth IEEE International Workshop on Multimedia Information Processing and Retrieval - MIPR 2010</span> (2010)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visible Wavelength</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/ISM.2010.56" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/MIPR2010.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="155">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="155">
            <h2>Iris Recognition: Preliminary Assessment about the Discriminating Capacity of Visible Wavelength Data</h2>
            <p class="authors">Gil Santos, Marco Bernardo, Paulo Fiadeiro, Hugo Proença</p>
            <p class="venue">Sixth IEEE International Workshop on Multimedia Information Processing and Retrieval - MIPR 2010 (2010)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The human iris supports contactless data acquisition and can be imaged covertly. These factors give raise to the possibility of performing biometric recognition procedure with- out subjects' knowledge and in uncontrolled data acquisition scenarios. The feasibility of this type of recognition has been receiving increasing attention, as is of particular interest in visual surveillance, computer forensics, threat assessment, and other security areas. In this paper we stress the role played by the spectrum of the visible light used in the acquisition process and assess the discriminating iris patterns that are likely to be acquired according to three factors: type of illuminant, it's luminance, and levels of iris pigmentation. Our goal is to perceive and quantify the conditions that appear to enable the biometric recognition process with enough confidence.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/ISM.2010.56" target="_blank">10.1109/ISM.2010.56</a></p>
            
            <div class="links">
                
                <a href="http://di.ubi.pt/~hugomcp/doc/MIPR2010.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2010" 
         data-type="conference"
         data-title="iris recognition: analysing the distribution of the iriscodes concordant bits"
         data-authors="gil santos, hugo proença"
         data-venue="ieee proceedings of the 3rd international congress on image and signal processing - cisp 2010"
         data-tags="iris recognition,biometrics,pattern matching"
         data-id="156">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-24.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Iris Recognition: Analysing the Distribution of the Iriscodes Concordant Bits</h3>
                <p class="text-gray-600 mb-2">Gil Santos, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Proceedings of the 3rd International Congress on Image and Signal Processing - CISP 2010</span> (2010)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Pattern Matching</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/CISP.2010.5647404" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/CISP2010.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="156">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="156">
            <h2>Iris Recognition: Analysing the Distribution of the Iriscodes Concordant Bits</h2>
            <p class="authors">Gil Santos, Hugo Proença</p>
            <p class="venue">IEEE Proceedings of the 3rd International Congress on Image and Signal Processing - CISP 2010 (2010)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The growth in practical applications for iris bio- metrics has been accompanied by relevant developments in the underlying algorithms and techniques. Efforts are being made to minimize the tradeoff between the recognition error rates and data quality, acquired in the visible wavelength, in less controlled environments, over simplified acquisition protocols and varying lighting conditions. This paper presents an approach that can be regarded as an extension to the widely known Daugman's method. Its basis is the analysis of the distribution of the concordant bits when matching iriscodes on both the spatial and frequency domains. Our experiments show that this method is able to improve the recognition performance over images captured in less constrained acquisition setups and protocols. Such conclusion was drawn upon trials conducted for multiple datasets.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/CISP.2010.5647404" target="_blank">10.1109/CISP.2010.5647404</a></p>
            
            <div class="links">
                
                <a href="http://di.ubi.pt/~hugomcp/doc/CISP2010.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2010" 
         data-type="book_chapter"
         data-title="caries detection in panoramic dental x-ray images"
         data-authors="joão oliveira, hugo proença"
         data-venue="computational vision and medical image processing - recent trends, springer verlag book series: computational methods in applied sciences"
         data-tags="dental x-ray,caries detection,medical imaging"
         data-id="157">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-1.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Caries Detection in Panoramic Dental X-ray Images</h3>
                <p class="text-gray-600 mb-2">João Oliveira, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Computational Vision and Medical Image Processing - Recent Trends, Springer Verlag book series: Computational Methods in Applied Sciences</span> (2010)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge book_chapter">Book chapter</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Dental X-Ray</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Caries Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Medical Imaging</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1007/978-94-007-0011-6_10" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/BC_Caries.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="157">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="157">
            <h2>Caries Detection in Panoramic Dental X-ray Images</h2>
            <p class="authors">João Oliveira, Hugo Proença</p>
            <p class="venue">Computational Vision and Medical Image Processing - Recent Trends, Springer Verlag book series: Computational Methods in Applied Sciences (2010)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Dental Caries, also known as dental decay or tooth decay, is defined as a disease of the hard tissues of the teeth caused by the action of microorganisms found in plaque on fermentable carbohydrates (principally sugars). Therefore, the detection of dental caries in a preliminary stage is an important task. This chapter has two major purposes, firstly to announce the availability of a new data set of panoramic dental X-ray images. This data set contains 1392 images with varying types of noise, usually inherent to this kind of images. Secondly, we present a complete case study for the detection of dental caries in panoramic dental X-ray images.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1007/978-94-007-0011-6_10" target="_blank">10.1007/978-94-007-0011-6_10</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/BC_Caries.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2010" 
         data-type="journal"
         data-title="the ubiris.v2: a database of visible wavelength iris images captured on-the-move and at-a-distance"
         data-authors="hugo proença, sílvio filipe, ricardo santos, joão oliveira, luís a. alexandre"
         data-venue="ieee transactions on pattern analysis and machine intelligence"
         data-tags="iris recognition,database,visible wavelength,biometrics"
         data-id="158">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-62.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">The UBIRIS.v2: A Database of Visible Wavelength Iris Images Captured On-The-Move and At-A-Distance</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Sílvio Filipe, Ricardo Santos, João Oliveira, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Pattern Analysis and Machine Intelligence</span> (2010)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Database</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visible Wavelength</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TPAMI.2009.66" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://di.ubi.pt/~hugomcp/doc/UBIRISv2_IEEETPAMI.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="158">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="158">
            <h2>The UBIRIS.v2: A Database of Visible Wavelength Iris Images Captured On-The-Move and At-A-Distance</h2>
            <p class="authors">Hugo Proença, Sílvio Filipe, Ricardo Santos, João Oliveira, Luís A. Alexandre</p>
            <p class="venue">IEEE Transactions on Pattern Analysis and Machine Intelligence (2010)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The iris is regarded as one of the most useful traits for biometric recognition and the dissemination of nationwide iris-based recognition systems is imminent. However, currently deployed systems rely on heavy imaging constraints to capture near infrared images with enough quality. Also, all of the publicly available iris image databases contain data correspondent to such imaging constraints and therefore are exclusively suitable to evaluate methods thought to operate on these type of environments. The main purpose of this paper is to announce the availability of the UBIRIS.v2 database, a multisession iris images database which singularly contains data captured in the visible wavelength, at-a-distance (between four and eight meters) and on on-the-move. This database is freely available for researchers concerned about visible wavelength iris recognition and will be useful in accessing the feasibility and specifying the constraints of this type of biometric recognition.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TPAMI.2009.66" target="_blank">10.1109/TPAMI.2009.66</a></p>
            
            <div class="links">
                
                <a href="http://di.ubi.pt/~hugomcp/doc/UBIRISv2_IEEETPAMI.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2010" 
         data-type="journal"
         data-title="iris recognition: on the segmentation of degraded images acquired in the visible wavelength"
         data-authors="hugo proença"
         data-venue="ieee transactions on pattern analysis and machine intelligence"
         data-tags="iris recognition,image segmentation,visible wavelength"
         data-id="159">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-63.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Iris Recognition: On the Segmentation of Degraded Images Acquired in the Visible Wavelength</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Pattern Analysis and Machine Intelligence</span> (2010)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Segmentation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visible Wavelength</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TPAMI.2009.140" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/VWIS_IEEETPAMI.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="159">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="159">
            <h2>Iris Recognition: On the Segmentation of Degraded Images Acquired in the Visible Wavelength</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">IEEE Transactions on Pattern Analysis and Machine Intelligence (2010)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Iris recognition imaging constraints are receiving increasing attention. There are several proposals to develop systems that operate in the visible wavelength and in less constrained environments. These imaging conditions engender acquired noisy artefacts that lead to severely degraded images, making iris segmentation a major issue. Having observed that existing iris segmentation methods tend to fail in these challenging conditions, we present a segmentation method that can handle degraded images acquired in less constrained conditions. We offer the following contributions: 1) to consider the sclera the most easily distinguishable part of the eye in degraded images, 2) to propose a new type of feature that measures the proportion of sclera in each direction and is fundamental in segmenting the iris, and 3) to run the entire procedure in deterministically linear time in respect to the size of the image, making the procedure suitable for real-time applications.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TPAMI.2009.140" target="_blank">10.1109/TPAMI.2009.140</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/VWIS_IEEETPAMI.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2010" 
         data-type="journal"
         data-title="analysis of the error rates regarding the accuracy of the segmentation stage"
         data-authors="hugo proença, luís a. alexandre"
         data-venue="elsevier image and vision computing"
         data-tags="iris recognition,image segmentation,error analysis"
         data-id="160">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-59.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Analysis of the Error Rates Regarding the Accuracy of the Segmentation Stage</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Image and Vision Computing</span> (2010)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Segmentation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Error Analysis</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.imavis.2009.03.003" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/IVC_SegmentInnaccuracies.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="160">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="160">
            <h2>Analysis of the Error Rates Regarding the Accuracy of the Segmentation Stage</h2>
            <p class="authors">Hugo Proença, Luís A. Alexandre</p>
            <p class="venue">Elsevier Image and Vision Computing (2010)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Iris recognition has been widely used in several scenarios with very satisfactory results. As it is one of the earliest stages, the image segmentation is in the basis of the process and plays a crucial role in the success of the recognition task. In this paper we analyze the relationship between the accuracy of the iris segmentation process and the error rates of three typical iris recognition methods. We selected 5000 images of the UBIRIS, CASIA and ICE databases that the used segmentation algorithm can accurately segment and artificially simulated four types of segmentation inaccuracies. The obtained results allowed us to con- clude about a strong relationship between translational segmentation inaccuracies – that lead to errors in phase – and the recognition error rates.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.imavis.2009.03.003" target="_blank">10.1016/j.imavis.2009.03.003</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/IVC_SegmentInnaccuracies.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2010" 
         data-type="journal"
         data-title="an iris recognition approach through structural pattern analysis methods"
         data-authors="hugo proença"
         data-venue="wiley expert systems, special issue on computer recognition systems"
         data-tags="iris recognition,structural pattern analysis,biometrics"
         data-id="161">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-60.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">An Iris Recognition Approach Through Structural Pattern Analysis Methods</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Wiley Expert Systems, Special issue on Computer Recognition Systems</span> (2010)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Structural Pattern Analysis</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1111/j.1468-0394.2009.00534.x" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ES_Structural.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="161">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="161">
            <h2>An Iris Recognition Approach Through Structural Pattern Analysis Methods</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">Wiley Expert Systems, Special issue on Computer Recognition Systems (2010)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Continuous efforts have been made in searching for robust and effective iris cod- ing methods, since Daugman's pioneering work on iris recognition was published. Proposed algorithms follow the statistical pattern recognition paradigm and encode the iris texture in- formation through phase, zero-crossing or texture-analysis based methods. In this paper we propose an iris recognition algorithm that follows the structural (syntactic) pattern recognition paradigm, which can be advantageous essentially for the purposes of description and of the human-perception of the system's functioning. Our experiments, that were performed on two widely used iris image databases (CASIA.v3 and ICE), show that the proposed iris structure provides enough discriminating information to enable accurate biometric recognition, while maintains the advantages intrinsic to structural pattern recognition systems.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1111/j.1468-0394.2009.00534.x" target="_blank">10.1111/j.1468-0394.2009.00534.x</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ES_Structural.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2010" 
         data-type="journal"
         data-title="introduction to the special issue on the segmentation of visible wavelength iris images captured at-a-distance and on-the-move"
         data-authors="hugo proença, luís a. alexandre"
         data-venue="elsevier image and vision computing"
         data-tags="iris recognition,image segmentation,visible wavelength,nice challenge"
         data-id="162">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-61.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Introduction to the Special Issue on the Segmentation of Visible Wavelength Iris Images Captured At-a-distance and On-the-move</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Image and Vision Computing</span> (2010)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Segmentation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visible Wavelength</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">NICE Challenge</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1016/j.imavis.2009.09.004" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/Introd_NICE1.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="162">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="162">
            <h2>Introduction to the Special Issue on the Segmentation of Visible Wavelength Iris Images Captured At-a-distance and On-the-move</h2>
            <p class="authors">Hugo Proença, Luís A. Alexandre</p>
            <p class="venue">Elsevier Image and Vision Computing (2010)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Deployed iris recognition systems are mainly based on Daugman's pioneering approach, and have proven their effectiveness in relatively constrained scenarios: operating in the near infra-red spectrum (NIR, 700-900 nm), at close acquisition distances and with stop-and-stare interfaces. However, the human iris supports contactless data acquisition, and it can — at least theoretically — be imaged covertly. The feasibility of covert iris recognition receives increasing attention and is of particular interest for forensic and security purposes. In this scope, one possibility is the use of visible wavelength light (VW) to perform image acquisition, although the use of this type of light can severely degrade the quality of the captured data. This is mainly due to the optical properties of the two molecules that constitute the pigment of the human iris: brown-black Eumelanin (over 90%) and yellow-reddish Pheomelanin. Eumelanin has most of its radiative fluorescence under VW, which enables the capturing of a much higher level of detail, but also of many more noisy artefacts, including specular and diffuse reflections and shadows. Also, the spectral reflectance of the sclera is significantly higher in the VW than in the NIR and the spectral radiance of the iris in respect to the levels of its pigmentation varies much more significantly in the VW than in the NIR. Furthermore, traditional template- and boundary-based iris segmentation approaches will probably fail, due to difficulties in detecting edges or in fitting rigid shapes. All these reasons justify the need of specialized segmentation strategies and were the major motivations behind the NICE.I contest (http://nice1.di.ubi.pt) that gave birth to this issue of the Image and Vision Computing Journal.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1016/j.imavis.2009.09.004" target="_blank">10.1016/j.imavis.2009.09.004</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/Introd_NICE1.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2010" 
         data-type="special_issue"
         data-title="on the segmentation of visible wavelength iris images acquired at-a-distance and on-the-move"
         data-authors="hugo proença, luís a. alexandre"
         data-venue="elsevier image and vision computing"
         data-tags="iris recognition,image segmentation,visible wavelength,non-cooperative recognition"
         data-id="163">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-127.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">On the Segmentation of Visible Wavelength Iris Images Acquired At-a-Distance and On-the-Move</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Elsevier Image and Vision Computing</span> (2010)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge special_issue">Special issue</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Segmentation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visible Wavelength</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Non-Cooperative Recognition</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    <a href="http://www.sciencedirect.com/science/publication?issn=02628856&volume=28&issue=2" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="163">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="163">
            <h2>On the Segmentation of Visible Wavelength Iris Images Acquired At-a-Distance and On-the-Move</h2>
            <p class="authors">Hugo Proença, Luís A. Alexandre</p>
            <p class="venue">Elsevier Image and Vision Computing (2010)</p>
            
            
            <div class="links">
                
                <a href="http://www.sciencedirect.com/science/publication?issn=02628856&volume=28&issue=2" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2009" 
         data-type="conference"
         data-title="on the role of interpolation in the normalization of non-ideal visible wavelength iris images"
         data-authors="gil santos, hugo proença"
         data-venue="proceedings of the 2009 international conference on computational intelligence and security (cis)"
         data-tags="iris recognition,biometrics,image normalization,interpolation methods"
         data-id="164">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-22.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">On the Role of Interpolation in the Normalization of Non-Ideal Visible Wavelength Iris Images</h3>
                <p class="text-gray-600 mb-2">Gil Santos, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the 2009 International Conference on Computational Intelligence and Security (CIS)</span> (2009)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Normalization</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Interpolation Methods</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/CIS.2009.113" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/cis09.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="164">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="164">
            <h2>On the Role of Interpolation in the Normalization of Non-Ideal Visible Wavelength Iris Images</h2>
            <p class="authors">Gil Santos, Hugo Proença</p>
            <p class="venue">Proceedings of the 2009 International Conference on Computational Intelligence and Security (CIS) (2009)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The growth in practical applications for iris bio- metrics has been accompanied by relevant developments in the underlying algorithms and techniques. Along with the research focused on near-infrared (NIR) cooperatively captured images, efforts are being made to minimize the trade-off between the quality of the captured data and the recognition accuracy on less constrained environments, where images are obtained at the visible wavelength, at increased distances, over simplified protocols and adverse lightning. This paper addresses the effect of the interpolation method, used in the iris normalization stage, in the overall recognition error rates. This effect is stressed for systems operating under less constrained image acquisition setups and protocols, due to higher variations in the amounts of captured data. Our experiments led us to conclude that the utility of the image interpolating methods is directly corresponding to the levels of noise that images contain.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/CIS.2009.113" target="_blank">10.1109/CIS.2009.113</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/cis09.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2009" 
         data-type="conference"
         data-title="biometric recognition: when is evidence fusion advantageous?"
         data-authors="hugo proença"
         data-venue="springer lecture notes in computer science – isvc 2009: 5th international symposium on visual computing"
         data-tags="biometrics,evidence fusion,multimodal recognition,performance analysis"
         data-id="165">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-23.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Biometric Recognition: When Is Evidence Fusion Advantageous?</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Springer Lecture Notes in Computer Science – ISVC 2009: 5th International Symposium on Visual Computing</span> (2009)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Evidence Fusion</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Multimodal Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Performance Analysis</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1007/978-3-642-10520-3_66" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ISVC09.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="165">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="165">
            <h2>Biometric Recognition: When Is Evidence Fusion Advantageous?</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">Springer Lecture Notes in Computer Science – ISVC 2009: 5th International Symposium on Visual Computing (2009)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Having assessed the performance gains due to evidence fusion, previous works reported contradictory conclusions. For some, a consistent improvement is achieved, while others state that the fusion of a stronger and a weaker biometric expert tends to produce worst results than if the best expert was used individually. The main contribution of this paper is to assess when improvements in performance are actually achieved, regarding the individual performance of each expert. Starting from readily satisfied assumptions about the score distributions generated by a biometric system, we predict the performance of each of the individual experts and of the fused system. Then, we conclude about the performance gains in fusing evidence from multiple sources. Also, we parameterize an empirically obtained relationship between the individual performance of the fused experts that contributes to decide whether evidence fusion techniques are advantageous or not.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1007/978-3-642-10520-3_66" target="_blank">10.1007/978-3-642-10520-3_66</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ISVC09.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2009" 
         data-type="conference"
         data-title="dental x-ray: a data set of panoramic dental radiographs for stomatologic image processing purposes"
         data-authors="joão oliveira, hugo proença"
         data-venue="taylor & francis proceedings of the ii eccomas thematic conference on computational vision and medical image processing (vipimage)"
         data-tags="medical imaging,dental radiographs,image dataset,image processing"
         data-id="166">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-21.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Dental X-Ray: A Data Set of Panoramic Dental Radiographs for Stomatologic Image Processing Purposes</h3>
                <p class="text-gray-600 mb-2">João Oliveira, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Taylor & Francis Proceedings of the II ECCOMAS Thematic Conference on Computational Vision and Medical Image Processing (VIPIMAGE)</span> (2009)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Medical Imaging</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Dental Radiographs</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Dataset</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Processing</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ECCOMAS09.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="166">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="166">
            <h2>Dental X-Ray: A Data Set of Panoramic Dental Radiographs for Stomatologic Image Processing Purposes</h2>
            <p class="authors">João Oliveira, Hugo Proença</p>
            <p class="venue">Taylor & Francis Proceedings of the II ECCOMAS Thematic Conference on Computational Vision and Medical Image Processing (VIPIMAGE) (2009)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>This paper has two major purposes: at firstly, to announce the availability of a new data set of panoramic dental X-ray images. This data set contains 1392 images with varying types of noise, usually inherent to this kind of images. Furthermore, the number of teeth per image and their dental morphology were not constant. Secondly, we propose a method to approximate the panoramic images in bitewing images, which are the most common type of images used in the human identification and in the tooth segmentation for the diagnosis of dental diseases.</p>
            </div>
            
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ECCOMAS09.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2009" 
         data-type="conference"
         data-title="on the feasibility of the visible wavelength, at-a-distance and on-the-move iris recognition"
         data-authors="hugo proença"
         data-venue="proceedings of the ieee symposium series on computational intelligence in biometrics: theory, algorithms, and applications (cibim ssci)"
         data-tags="iris recognition,biometrics,visible wavelength,non-cooperative recognition"
         data-id="167">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-20.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">On the Feasibility of the Visible Wavelength, At-A-Distance and On-The-Move Iris Recognition</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Proceedings of the IEEE Symposium Series on Computational Intelligence in Biometrics: Theory, Algorithms, and Applications (CIBIM SSCI)</span> (2009)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visible Wavelength</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Non-cooperative Recognition</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/CIB.2009.4925680" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/CIBIM09.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="167">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="167">
            <h2>On the Feasibility of the Visible Wavelength, At-A-Distance and On-The-Move Iris Recognition</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">Proceedings of the IEEE Symposium Series on Computational Intelligence in Biometrics: Theory, Algorithms, and Applications (CIBIM SSCI) (2009)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The dramatic growth in practical applications for iris biometrics has been accompanied by relevant developments in the underlying algorithms and techniques. Among others, one active research area concerns about the development of iris recognition systems less constrained to users, either increasing the imaging distances, simplifying the acquisition protocols or the required lighting conditions. In this paper we address the possibility of perform reliable recognition using visible wavelength images captured under high heterogeneous lighting conditions, with subjects at-a-distance (between 4 and 8 meters) and on-the-move. The feasibility of this extremely ambitious type of recognition is analyzed, its major obstacles and challenges discussed and some directions for forthcoming work pointed.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/CIB.2009.4925680" target="_blank">10.1109/CIB.2009.4925680</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/CIBIM09.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2009" 
         data-type="conference"
         data-title="dental x-ray: a data set of panoramic dental radiographs for stomatologic image processing purposes"
         data-authors="joão oliveira, hugo proença"
         data-venue="taylor & francis proceedings of the ii eccomas thematic conference on computational vision and medical image processing - vipimage'09"
         data-tags="dental x-ray,dataset,medical imaging"
         data-id="168">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-21.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Dental X-Ray: A Data Set of Panoramic Dental Radiographs for Stomatologic Image Processing Purposes</h3>
                <p class="text-gray-600 mb-2">João Oliveira, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Taylor & Francis Proceedings of the II ECCOMAS Thematic Conference on Computational Vision and Medical Image Processing - VIPIMAGE'09</span> (2009)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Dental X-Ray</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Dataset</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Medical Imaging</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ECCOMAS09.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="168">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="168">
            <h2>Dental X-Ray: A Data Set of Panoramic Dental Radiographs for Stomatologic Image Processing Purposes</h2>
            <p class="authors">João Oliveira, Hugo Proença</p>
            <p class="venue">Taylor & Francis Proceedings of the II ECCOMAS Thematic Conference on Computational Vision and Medical Image Processing - VIPIMAGE'09 (2009)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>This paper has two major purposes: at firstly, to announce the availability of a new data set of panoramic dental X-ray images. This data set contains 1392 images with varying types of noise, usually inherent to this kind of images. Furthermore, the number of teeth per image and their dental morphology were not constant. Secondly, we propose a method to approximate the panoramic images in bitewing images, which are the most common type of images used in the human identification and in the tooth segmentation for the diagnosis of dental diseases.</p>
            </div>
            
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ECCOMAS09.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2009" 
         data-type="conference"
         data-title="biometric recognition: when is evidence fusion advantageous?"
         data-authors="hugo proença"
         data-venue="springer lecture notes in computer science – isvc 2009: 5th international symposium on visual computing"
         data-tags="biometric recognition,evidence fusion,performance analysis"
         data-id="169">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-23.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Biometric Recognition: When Is Evidence Fusion Advantageous?</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Springer Lecture Notes in Computer Science – ISVC 2009: 5th International Symposium on Visual Computing</span> (2009)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometric Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Evidence Fusion</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Performance Analysis</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1007/978-3-642-10520-3_66" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ISVC09.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="169">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="169">
            <h2>Biometric Recognition: When Is Evidence Fusion Advantageous?</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">Springer Lecture Notes in Computer Science – ISVC 2009: 5th International Symposium on Visual Computing (2009)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Having assessed the performance gains due to evidence fusion, previous works reported contradictory conclusions. For some, a consistent improvement is achieved, while others state that the fusion of a stronger and a weaker biometric expert tends to produce worst results than if the best expert was used individually. The main contribution of this paper is to assess when improvements in performance are actually achieved, regarding the individual performance of each expert. Starting from readily satisfied assumptions about the score distributions generated by a biometric system, we predict the performance of each of the individual experts and of the fused system. Then, we conclude about the performance gains in fusing evidence from multiple sources. Also, we parameterize an empirically obtained relationship between the individual performance of the fused experts that contributes to decide whether evidence fusion techniques are advantageous or not.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1007/978-3-642-10520-3_66" target="_blank">10.1007/978-3-642-10520-3_66</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ISVC09.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2009" 
         data-type="conference"
         data-title="on the role of interpolation in the normalization of non-ideal visible wavelength iris images"
         data-authors="gil santos, hugo proença"
         data-venue="international conference on computational intelligence and security - cis'09"
         data-tags="iris recognition,image interpolation,visible wavelength"
         data-id="170">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-22.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">On the Role of Interpolation in the Normalization of Non-Ideal Visible Wavelength Iris Images</h3>
                <p class="text-gray-600 mb-2">Gil Santos, Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">International Conference on Computational Intelligence and Security - CIS'09</span> (2009)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Interpolation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visible Wavelength</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/CIS.2009.113" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/cis09.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="170">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="170">
            <h2>On the Role of Interpolation in the Normalization of Non-Ideal Visible Wavelength Iris Images</h2>
            <p class="authors">Gil Santos, Hugo Proença</p>
            <p class="venue">International Conference on Computational Intelligence and Security - CIS'09 (2009)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The growth in practical applications for iris bio- metrics has been accompanied by relevant developments in the underlying algorithms and techniques. Along with the research focused on near-infrared (NIR) cooperatively captured images, efforts are being made to minimize the trade-off between the quality of the captured data and the recognition accuracy on less constrained environments, where images are obtained at the visible wavelength, at increased distances, over simplified protocols and adverse lightning. This paper addresses the effect of the interpolation method, used in the iris normalization stage, in the overall recognition error rates. This effect is stressed for systems operating under less constrained image acquisition setups and protocols, due to higher variations in the amounts of captured data. Our experiments led us to conclude that the utility of the image interpolating methods is directly corresponding to the levels of noise that images contain.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/CIS.2009.113" target="_blank">10.1109/CIS.2009.113</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/cis09.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2009" 
         data-type="conference"
         data-title="on the feasibility of the visible wavelength, at-a-distance and on-the-move iris recognition"
         data-authors="hugo proença"
         data-venue="ieee symposium series on computational intelligence in biometrics: theory, algorithms, and applications - cibim ssci 2009"
         data-tags="iris recognition,visible wavelength,unconstrained recognition"
         data-id="171">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-20.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">On the Feasibility of the Visible Wavelength, At-A-Distance and On-The-Move Iris Recognition</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Symposium Series on Computational Intelligence in Biometrics: Theory, Algorithms, and Applications - CIBIM SSCI 2009</span> (2009)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visible Wavelength</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Unconstrained Recognition</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/CIB.2009.4925680" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/CIBIM09.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="171">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="171">
            <h2>On the Feasibility of the Visible Wavelength, At-A-Distance and On-The-Move Iris Recognition</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">IEEE Symposium Series on Computational Intelligence in Biometrics: Theory, Algorithms, and Applications - CIBIM SSCI 2009 (2009)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The dramatic growth in practical applications for iris biometrics has been accompanied by relevant developments in the underlying algorithms and techniques. Among others, one active research area concerns about the development of iris recognition systems less constrained to users, either increasing the imaging distances, simplifying the acquisition protocols or the required lighting conditions. In this paper we address the possibility of perform reliable recognition using visible wavelength images captured under high heterogeneous lighting conditions, with subjects at-a-distance (between 4 and 8 meters) and on-the-move. The feasibility of this extremely ambitious type of recognition is analyzed, its major obstacles and challenges discussed and some directions for forthcoming work pointed.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/CIB.2009.4925680" target="_blank">10.1109/CIB.2009.4925680</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/CIBIM09.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2008" 
         data-type="conference"
         data-title="iris recognition: a method to segment visible wavelength iris images acquired on-the-move and at-a-distance"
         data-authors="hugo proença"
         data-venue="springer lecture notes in computer science – isvc 2008: 4th international symposium on visual computing"
         data-tags="iris recognition,image segmentation,visible wavelength"
         data-id="172">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-19.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Iris Recognition: A Method To Segment Visible Wavelength Iris Images Acquired On-The-Move and At-A-Distance</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Springer Lecture Notes in Computer Science – ISVC 2008: 4th International Symposium on Visual Computing</span> (2008)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Segmentation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visible Wavelength</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1007/978-3-540-89639-5_70" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ISVC08.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="172">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="172">
            <h2>Iris Recognition: A Method To Segment Visible Wavelength Iris Images Acquired On-The-Move and At-A-Distance</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">Springer Lecture Notes in Computer Science – ISVC 2008: 4th International Symposium on Visual Computing (2008)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The dramatic growth in practical applications for iris biometrics has been accompanied by many important developments in the underlying algorithms and techniques. Among others, one of the most active research areas concerns about the development of iris recognition systems less constrained to users, either increasing the image acquisition distances or the required lighting conditions. The main point of this paper is to give a process suitable for the automatic segmentation of iris images captured at the visible wavelength, on-the-move and within a large range of image acquisition distances (between 4 and 8 meters). Our experiments were performed on images of the UBIRIS.v2 database and show the robustness of the proposed method to handle the types of non-ideal images resultant of the aforementioned less constrained image acquisition conditions.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1007/978-3-540-89639-5_70" target="_blank">10.1007/978-3-540-89639-5_70</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ISVC08.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2008" 
         data-type="conference"
         data-title="combining rectangular and triangular image regions to perform real-time face detection"
         data-authors="hugo proença, sílvio filipe"
         data-venue="ieee proceedings of the international conference on signal processing - icsp'08"
         data-tags="face detection,computer vision,real-time processing"
         data-id="173">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-18.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Combining Rectangular and Triangular Image Regions to Perform Real-Time Face Detection</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Sílvio Filipe</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Proceedings of the International Conference on Signal Processing - ICSP'08</span> (2008)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Face Detection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Computer Vision</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Real-Time Processing</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/ICOSP.2008.4697274" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ICSP08.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="173">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="173">
            <h2>Combining Rectangular and Triangular Image Regions to Perform Real-Time Face Detection</h2>
            <p class="authors">Hugo Proença, Sílvio Filipe</p>
            <p class="venue">IEEE Proceedings of the International Conference on Signal Processing - ICSP'08 (2008)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Nowadays, face detection techniques assume growing relevance in a wide range of applications (e.g., bio- metrics and automatic surveillance)and constitute a pre- requisite of many image processing stages. Among a large number of published approaches, one of the most relevant is the method proposed by Viola and Jones [18] to perform real-time face detection through a cascade schema of weak classifiers that act together to com- pose a strong and robust classifier. This method was the basis of our work and motivated the key contributions given in this paper. At first, based on the computer graphics concept of "triangle mesh" we propose the notion of "triangular integral feature" to describe and model face properties. Also, we show results of our face detection experiments that point to an increase of the detection accuracy when the triangular features are mixed with the rectangular in the candidate feature set, which is considered an achievement. Also, it should be stressed that this optimization is obtained without any relevant in- crease in the computational requirements, either spacial or temporal, of the detection method.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/ICOSP.2008.4697274" target="_blank">10.1109/ICOSP.2008.4697274</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ICSP08.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2008" 
         data-type="conference"
         data-title="iris recognition: a method to segment visible wavelength iris images acquired on-the-move and at-a-distance"
         data-authors="hugo proença"
         data-venue="springer lecture notes in computer science – isvc 2008: 4th international symposium on visual computing"
         data-tags="iris recognition,biometrics,image segmentation,visible wavelength"
         data-id="174">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-19.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Iris Recognition: A Method To Segment Visible Wavelength Iris Images Acquired On-The-Move and At-A-Distance</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Springer Lecture Notes in Computer Science – ISVC 2008: 4th International Symposium on Visual Computing</span> (2008)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Segmentation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Visible Wavelength</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1007/978-3-540-89639-5_70" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ISVC08.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="174">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="174">
            <h2>Iris Recognition: A Method To Segment Visible Wavelength Iris Images Acquired On-The-Move and At-A-Distance</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">Springer Lecture Notes in Computer Science – ISVC 2008: 4th International Symposium on Visual Computing (2008)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The dramatic growth in practical applications for iris biometrics has been accompanied by many important developments in the underlying algorithms and techniques. Among others, one of the most active research areas concerns about the development of iris recognition systems less constrained to users, either increasing the image acquisition distances or the required lighting conditions. The main point of this paper is to give a process suitable for the automatic segmentation of iris images captured at the visible wavelength, on-the-move and within a large range of image acquisition distances (between 4 and 8 meters). Our experiments were performed on images of the UBIRIS.v2 database and show the robustness of the proposed method to handle the types of non-ideal images resultant of the aforementioned less constrained image acquisition conditions.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1007/978-3-540-89639-5_70" target="_blank">10.1007/978-3-540-89639-5_70</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ISVC08.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2007" 
         data-type="conference"
         data-title="the nice.i: noisy iris challenge evaluation – part i"
         data-authors="hugo proença, luís a. alexandre"
         data-venue="ieee first international conference on biometrics: theory, applications and systems – btas 2007"
         data-tags="iris recognition,image segmentation,nice challenge"
         data-id="175">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-14.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">The NICE.I: Noisy Iris Challenge Evaluation – Part I</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE First International Conference on Biometrics: Theory, Applications and Systems – BTAS 2007</span> (2007)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Segmentation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">NICE Challenge</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/BTAS.2007.4401910" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/BTAS07.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="175">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="175">
            <h2>The NICE.I: Noisy Iris Challenge Evaluation – Part I</h2>
            <p class="authors">Hugo Proença, Luís A. Alexandre</p>
            <p class="venue">IEEE First International Conference on Biometrics: Theory, Applications and Systems – BTAS 2007 (2007)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>This paper gives an overview of the NICE.I : Noisy Iris Challenge Evaluation - Part I contest. This contest differs from others in two fundamental points. First, instead of the complete iris recognition process, it exclusively evaluates the iris segmentation and noise detection stages, allowing the independent evaluation of one of the main recognition error sources. Second, it operates on highly noisy images that were captured to simulate less constrained imaging environments and constitute the second version of the UBIRIS database (UBIRIS.v2). Further details can be seen at the contest web site (http://nice1.di.ubi.pt).</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/BTAS.2007.4401910" target="_blank">10.1109/BTAS.2007.4401910</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/BTAS07.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2007" 
         data-type="conference"
         data-title="the nice.i: noisy iris challenge evaluation – part i"
         data-authors="hugo proença, luís a. alexandre"
         data-venue="ieee proceedings of the ieee first international conference on biometrics: theory, applications and systems (btas)"
         data-tags="iris recognition,biometrics,challenge evaluation,noisy images"
         data-id="176">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-14.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">The NICE.I: Noisy Iris Challenge Evaluation – Part I</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Proceedings of the IEEE First International Conference on Biometrics: Theory, Applications and Systems (BTAS)</span> (2007)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Challenge Evaluation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Noisy Images</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/BTAS.2007.4401910" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/BTAS07.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="176">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="176">
            <h2>The NICE.I: Noisy Iris Challenge Evaluation – Part I</h2>
            <p class="authors">Hugo Proença, Luís A. Alexandre</p>
            <p class="venue">IEEE Proceedings of the IEEE First International Conference on Biometrics: Theory, Applications and Systems (BTAS) (2007)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>This paper gives an overview of the NICE.I : Noisy Iris Challenge Evaluation - Part I contest. This contest differs from others in two fundamental points. First, instead of the complete iris recognition process, it exclusively evaluates the iris segmentation and noise detection stages, allowing the independent evaluation of one of the main recognition error sources. Second, it operates on highly noisy images that were captured to simulate less constrained imaging environments and constitute the second version of the UBIRIS database (UBIRIS.v2). Further details can be seen at the contest web site (http://nice1.di.ubi.pt).</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/BTAS.2007.4401910" target="_blank">10.1109/BTAS.2007.4401910</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/BTAS07.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2007" 
         data-type="conference"
         data-title="iris recognition: a method to increase the robustness to noisy imaging environments through the selection of the higher discriminating features"
         data-authors="hugo proença, luís a. alexandre"
         data-venue="ieee proceedings of the international conference on computational intelligence and multimedia applications - iccima'07"
         data-tags="iris recognition,feature selection,noisy environments"
         data-id="177">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-17.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Iris Recognition: A Method to Increase the Robustness to Noisy Imaging Environments Through the Selection of the Higher Discriminating Features</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Proceedings of the International Conference on Computational Intelligence and Multimedia Applications - ICCIMA'07</span> (2007)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Feature Selection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Noisy Environments</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/ICCIMA.2007.119" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/iccima15.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="177">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="177">
            <h2>Iris Recognition: A Method to Increase the Robustness to Noisy Imaging Environments Through the Selection of the Higher Discriminating Features</h2>
            <p class="authors">Hugo Proença, Luís A. Alexandre</p>
            <p class="venue">IEEE Proceedings of the International Conference on Computational Intelligence and Multimedia Applications - ICCIMA'07 (2007)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Continuous efforts have been made in searching for robust and effective iris coding methods, since Daugman's pioneering work on iris recognition was published. However, due to lack of robustness, the error rates of iris recognition systems significantly increase when images contain large portions of noise (reflections and iris obstructions), resultant from less constrained imaging conditions. Current iris encoding and matching proposals do not take into account the specific lighting conditions of the imaging environment, decreasing their adaptability to such dynamics conditions. In this paper we propose a method that, through a learning stage, takes into account the typical noisy regions propitiated by the imaging environment to select the higher discriminating features. Our experiments were performed on two well known iris image databases (CASIA and UBIRIS) and show a significant decrease of the error rates in the recognition of iris images corrupted by noise.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/ICCIMA.2007.119" target="_blank">10.1109/ICCIMA.2007.119</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/iccima15.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2007" 
         data-type="conference"
         data-title="a structural pattern analysis approach to iris recognition"
         data-authors="hugo proença"
         data-venue="springer lecture notes in computer science, advances in soft computing – cores 2007: 5th international conference on computer recognition systems"
         data-tags="iris recognition,structural pattern analysis,pattern recognition"
         data-id="178">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-15.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">A Structural Pattern Analysis Approach to Iris Recognition</h3>
                <p class="text-gray-600 mb-2">Hugo Proença</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Springer Lecture Notes in Computer Science, Advances in Soft Computing – CORES 2007: 5th International Conference on Computer Recognition Systems</span> (2007)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Structural Pattern Analysis</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Pattern Recognition</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1007/978-3-540-75175-5_90" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/CORES07.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="178">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="178">
            <h2>A Structural Pattern Analysis Approach to Iris Recognition</h2>
            <p class="authors">Hugo Proença</p>
            <p class="venue">Springer Lecture Notes in Computer Science, Advances in Soft Computing – CORES 2007: 5th International Conference on Computer Recognition Systems (2007)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Continuous efforts have been made in searching for robust and effective iris coding methods, since Daugman's pioneering work on iris recognition was published. Proposed algorithms follow the statistical pattern recognition paradigm and encode the iris texture in- formation through phase, zero-crossing or texture-analysis based methods. In this paper we propose an iris recognition algorithm that follows the structural (syntactic) pattern recognition paradigm, which can be advantageous essentially for the purposes of description and of the human-perception of the system's functioning. Our experiments, that were performed on two widely used iris image databases (CASIA.v3 and ICE), show that the proposed iris structure provides enough discriminating information to enable accurate biometric recognition, while maintains the advantages intrinsic to structural pattern recognition systems.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1007/978-3-540-75175-5_90" target="_blank">10.1007/978-3-540-75175-5_90</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/CORES07.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2007" 
         data-type="journal"
         data-title="toward non-cooperative iris recognition: a classification approach using multiple signatures"
         data-authors="hugo proença, luís a. alexandre"
         data-venue="ieee transactions on pattern analysis and machine intelligence"
         data-tags="iris recognition,non-cooperative,classification,multiple signatures"
         data-id="179">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-58.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Toward Non-Cooperative Iris Recognition: A Classification Approach Using Multiple Signatures</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Pattern Analysis and Machine Intelligence</span> (2007)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Non-Cooperative</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Classification</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Multiple Signatures</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TPAMI.2007.1016" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ProencaAlexandreMultipleSignaturesPAMI2007.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="179">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="179">
            <h2>Toward Non-Cooperative Iris Recognition: A Classification Approach Using Multiple Signatures</h2>
            <p class="authors">Hugo Proença, Luís A. Alexandre</p>
            <p class="venue">IEEE Transactions on Pattern Analysis and Machine Intelligence (2007)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>This paper focus on noncooperative iris recognition, i.e., the capture of iris images at large distances, under less controlled lighting conditions, and without active participation of the subjects. This increases the probability of capturing very heterogeneous images (regarding focus, contrast, or brightness) and with several noise factors (iris obstructions and reflections). Current iris recognition systems are unable to deal with noisy data and substantially increase their error rates, especially the false rejections, in these conditions. We propose an iris classification method that divides the segmented and normalized iris image into six regions, makes an independent feature extraction and comparison for each region, and combines each of the dissimilarity values through a classification rule. Experiments show a substantial decrease, higher than 40 percent, of the false rejection rates in the recognition of noisy iris images.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TPAMI.2007.1016" target="_blank">10.1109/TPAMI.2007.1016</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ProencaAlexandreMultipleSignaturesPAMI2007.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2007" 
         data-type="journal"
         data-title="toward non-cooperative iris recognition: a classification approach using multiple signatures"
         data-authors="hugo proença, luís a. alexandre"
         data-venue="ieee transactions on pattern analysis and machine intelligence"
         data-tags="iris recognition,biometrics,non-cooperative recognition,multiple signatures"
         data-id="180">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-58.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Toward Non-Cooperative Iris Recognition: A Classification Approach Using Multiple Signatures</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Pattern Analysis and Machine Intelligence</span> (2007)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Non-Cooperative Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Multiple Signatures</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TPAMI.2007.1016" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ProencaAlexandreMultipleSignaturesPAMI2007.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="180">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="180">
            <h2>Toward Non-Cooperative Iris Recognition: A Classification Approach Using Multiple Signatures</h2>
            <p class="authors">Hugo Proença, Luís A. Alexandre</p>
            <p class="venue">IEEE Transactions on Pattern Analysis and Machine Intelligence (2007)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>This paper focus on noncooperative iris recognition, i.e., the capture of iris images at large distances, under less controlled lighting conditions, and without active participation of the subjects. This increases the probability of capturing very heterogeneous images (regarding focus, contrast, or brightness) and with several noise factors (iris obstructions and reflections). Current iris recognition systems are unable to deal with noisy data and substantially increase their error rates, especially the false rejections, in these conditions. We propose an iris classification method that divides the segmented and normalized iris image into six regions, makes an independent feature extraction and comparison for each region, and combines each of the dissimilarity values through a classification rule. Experiments show a substantial decrease, higher than 40 percent, of the false rejection rates in the recognition of noisy iris images.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TPAMI.2007.1016" target="_blank">10.1109/TPAMI.2007.1016</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ProencaAlexandreMultipleSignaturesPAMI2007.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2007" 
         data-type="journal"
         data-title="toward non-cooperative iris recognition: a classification approach using multiple signatures"
         data-authors="hugo proença, luís a. alexandre"
         data-venue="ieee transactions on pattern analysis and machine intelligence"
         data-tags="iris recognition,non-cooperative recognition,multiple signatures"
         data-id="181">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-58.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Toward Non-Cooperative Iris Recognition: A Classification Approach Using Multiple Signatures</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Transactions on Pattern Analysis and Machine Intelligence</span> (2007)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Non-Cooperative Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Multiple Signatures</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/TPAMI.2007.1016" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ProencaAlexandreMultipleSignaturesPAMI2007.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="181">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="181">
            <h2>Toward Non-Cooperative Iris Recognition: A Classification Approach Using Multiple Signatures</h2>
            <p class="authors">Hugo Proença, Luís A. Alexandre</p>
            <p class="venue">IEEE Transactions on Pattern Analysis and Machine Intelligence (2007)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>This paper focus on noncooperative iris recognition, i.e., the capture of iris images at large distances, under less controlled lighting conditions, and without active participation of the subjects. This increases the probability of capturing very heterogeneous images (regarding focus, contrast, or brightness) and with several noise factors (iris obstructions and reflections). Current iris recognition systems are unable to deal with noisy data and substantially increase their error rates, especially the false rejections, in these conditions. We propose an iris classification method that divides the segmented and normalized iris image into six regions, makes an independent feature extraction and comparison for each region, and combines each of the dissimilarity values through a classification rule. Experiments show a substantial decrease, higher than 40 percent, of the false rejection rates in the recognition of noisy iris images.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/TPAMI.2007.1016" target="_blank">10.1109/TPAMI.2007.1016</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ProencaAlexandreMultipleSignaturesPAMI2007.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2007" 
         data-type="conference"
         data-title="iris recognition: an entropy-based coding strategy robust to noisy imaging environments"
         data-authors="hugo proença, luís a. alexandre"
         data-venue="springer lecture notes in computer science – isvc 2007: 3rd international symposium on visual computing"
         data-tags="iris recognition,entropy-based coding,noisy environments"
         data-id="182">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-16.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Iris Recognition: An Entropy-Based Coding Strategy Robust to Noisy Imaging Environments</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Springer Lecture Notes in Computer Science – ISVC 2007: 3rd International Symposium on Visual Computing</span> (2007)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Entropy-Based Coding</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Noisy Environments</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1007/978-3-540-76858-6_60" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ISVC07.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="182">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="182">
            <h2>Iris Recognition: An Entropy-Based Coding Strategy Robust to Noisy Imaging Environments</h2>
            <p class="authors">Hugo Proença, Luís A. Alexandre</p>
            <p class="venue">Springer Lecture Notes in Computer Science – ISVC 2007: 3rd International Symposium on Visual Computing (2007)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>The iris is currently accepted as one of the most accurate traits for biometric purposes. However, for the sake of accuracy, iris recognition systems rely on good quality images and significantly deteriorate their results when images contain large noisy regions, either due to iris obstructions (eyelids or eyelashes) or reflections (specular or lighting). In this paper we propose an entropy-based iris coding strategy that constructs an unidimensional signal from overlapped angular patches of normalized iris images. Further, in the comparison between biometric signatures we exclusively take into account signatures' segments of varying dimension. The hope is to avoid the comparison between components corrupted by noise and achieve accurate recognition, even on highly noisy images. Our experiments were performed in three widely used iris image databases (third version of CASIA, ICE and UBIRIS) and led us to observe that our proposal significantly decreases the error rates in the recognition of noisy iris images.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1007/978-3-540-76858-6_60" target="_blank">10.1007/978-3-540-76858-6_60</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ISVC07.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2006" 
         data-type="conference"
         data-title="a method for the identification of noisy regions in normalized iris images"
         data-authors="hugo proença, luís a. alexandre"
         data-venue="ieee proceedings of the 18th international conference on pattern recognition (icpr)"
         data-tags="iris recognition,biometrics,image processing,noise detection"
         data-id="183">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-11.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">A Method for the Identification of Noisy Regions in Normalized Iris Images</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Proceedings of the 18th International Conference on Pattern Recognition (ICPR)</span> (2006)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Processing</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Noise Detection</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/ICPR.2006.100" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ICPR06.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="183">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="183">
            <h2>A Method for the Identification of Noisy Regions in Normalized Iris Images</h2>
            <p class="authors">Hugo Proença, Luís A. Alexandre</p>
            <p class="venue">IEEE Proceedings of the 18th International Conference on Pattern Recognition (ICPR) (2006)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>In this paper we propose a new method for the identification of noisy regions in normalized iris images. Starting from a normalized and dimensionless iris image in the polar coordinate system, our goal consists in the classification of every pixel as 'noise' or 'not noise'. This classification could be helpful in the posterior feature extraction or feature comparison stages regarding the construction of biometric iris signatures more robust to noise.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/ICPR.2006.100" target="_blank">10.1109/ICPR.2006.100</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ICPR06.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2006" 
         data-type="conference"
         data-title="iris recognition: an analysis of the aliasing problem in the iris normalization stage"
         data-authors="hugo proença, luís a. alexandre"
         data-venue="ieee proceedings of the 2006 international conference on computational intelligence and security (cis)"
         data-tags="iris recognition,biometrics,image processing,aliasing"
         data-id="184">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-12.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Iris Recognition: An Analysis of the Aliasing Problem in the Iris Normalization Stage</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Proceedings of the 2006 International Conference on Computational Intelligence and Security (CIS)</span> (2006)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Processing</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Aliasing</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/ICCIAS.2006.295366" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/CIS06.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="184">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="184">
            <h2>Iris Recognition: An Analysis of the Aliasing Problem in the Iris Normalization Stage</h2>
            <p class="authors">Hugo Proença, Luís A. Alexandre</p>
            <p class="venue">IEEE Proceedings of the 2006 International Conference on Computational Intelligence and Security (CIS) (2006)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Iris recognition has been increasingly used with very satisfactory results. Presently, the challenge consists in unconstrain the image capturing conditions and enable its application to domains where the subjects' cooperation is not expectable (e.g. criminal/terrorist seek, missing children). In this type of use, due to variations in the image capturing distance and in the lighting conditions that determine the size of the subjects' pupil, the area correspondent to the iris in the captured images will be highly varying too. In order to compensate this variation, common iris recognition proposals translate the segmented iris image to a double dimensionless pseudo-polar coordinate system, in a process known as the normalization stage, which can be regarded as a sampling of the original data with the inherent possibility of aliasing. In this paper we analyze the relationship between the size of the captured iris image and the overall recognition's accuracy. Further, we identify the threshold for the sampling rate of the iris normalization process above which the error rates significantly increase.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/ICCIAS.2006.295366" target="_blank">10.1109/ICCIAS.2006.295366</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/CIS06.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2006" 
         data-type="journal"
         data-title="iris segmentation methodology for non-cooperative recognition"
         data-authors="hugo proença, luís a. alexandre"
         data-venue="iee proceedings of image, vision, image & signal processing"
         data-tags="iris recognition,biometrics,image segmentation,non-cooperative recognition"
         data-id="185">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-57.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Iris Segmentation Methodology for Non-Cooperative Recognition</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEE Proceedings of Image, Vision, Image & Signal Processing</span> (2006)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge journal">Journal</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Segmentation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Non-Cooperative Recognition</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1049/ip-vis:20050213" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ProencaAlexandreIrisSegmentationIEE.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="185">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="185">
            <h2>Iris Segmentation Methodology for Non-Cooperative Recognition</h2>
            <p class="authors">Hugo Proença, Luís A. Alexandre</p>
            <p class="venue">IEE Proceedings of Image, Vision, Image & Signal Processing (2006)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>An overview of the iris image segmentation methodologies for biometric purposes is presented. The main focus is on the analysis of the ability of segmentation algorithms to process images with heterogeneous characteristics, simulating the dynamics of a non-cooperative environment. The accuracy of the four selected methodologies on the UBIRIS database is tested and, having concluded about their weak robustness when dealing with non-optimal images regarding focus, reflections, brightness or eyelid obstruction, the authors introduce a new and more robust iris image segmentation methodology. This new methodology could contribute to the aim of non-cooperative biometric iris recognition, where the ability to process this type of image is required.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1049/ip-vis:20050213" target="_blank">10.1049/ip-vis:20050213</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ProencaAlexandreIrisSegmentationIEE.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2006" 
         data-type="conference"
         data-title="iris recognition: measuring feature's quality for the feature selection in unconstrained image capture environments"
         data-authors="hugo proença, luís a. alexandre"
         data-venue="ieee proceedings of the 2006 international conference on computational intelligence for homeland security and personal safety (cihsps)"
         data-tags="iris recognition,biometrics,feature selection,image quality"
         data-id="186">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="images/publication/paper-13.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">Iris Recognition: Measuring Feature's Quality for the Feature Selection in Unconstrained Image Capture Environments</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Proceedings of the 2006 International Conference on Computational Intelligence for Homeland Security and Personal Safety (CIHSPS)</span> (2006)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Feature Selection</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Quality</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/CIHSPS.2006.313298" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/CIHSPS06.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="186">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="186">
            <h2>Iris Recognition: Measuring Feature's Quality for the Feature Selection in Unconstrained Image Capture Environments</h2>
            <p class="authors">Hugo Proença, Luís A. Alexandre</p>
            <p class="venue">IEEE Proceedings of the 2006 International Conference on Computational Intelligence for Homeland Security and Personal Safety (CIHSPS) (2006)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Iris recognition is being increasingly used for several purposes. However, current iris recognition systems are unable to deal with noisy data and substantially increase their error rates, specially the false rejections in these conditions. Several proposals have been made to access image quality and to identify noisy regions in iris images. Based in these facts, in this paper we propose a method applicable in the feature extraction and comparison stages that measures the quality of each feature of the biometric signature and take account of this information to obtain the dissimilarity between iris signatures. Experiments led us to conclude that this method significantly decreases the error rates in the recognition of noisy iris images, resultant from the capturing in less constrained environments.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/CIHSPS.2006.313298" target="_blank">10.1109/CIHSPS.2006.313298</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/CIHSPS06.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2006" 
         data-type="conference"
         data-title="a method for the identification of inaccuracies in the pupil segmentation"
         data-authors="hugo proença, luís a. alexandre"
         data-venue="ieee proceedings of the first international conference on availability, reliability and security (ares)"
         data-tags="iris recognition,biometrics,image segmentation,pupil detection"
         data-id="187">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-10.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">A Method for the Identification of Inaccuracies in the Pupil Segmentation</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">IEEE Proceedings of the First International Conference on Availability, Reliability and Security (AReS)</span> (2006)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Segmentation</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Pupil Detection</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1109/ARES.2006.9" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ARES2006.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="187">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="187">
            <h2>A Method for the Identification of Inaccuracies in the Pupil Segmentation</h2>
            <p class="authors">Hugo Proença, Luís A. Alexandre</p>
            <p class="venue">IEEE Proceedings of the First International Conference on Availability, Reliability and Security (AReS) (2006)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>In this paper we analyze the relationship between the accuracy of the segmentation algorithm and the error rates of typical iris recognition systems. We selected 1000 images from the UBIRIS database that the segmentation algorithm can accurately segment and artificially introduced segmentation inaccuracies. We repeated the recognition tests and concluded about the strong relationship between the errors in the pupil segmentation and the overall false reject rate. Based on this fact, we propose a method to identify these inaccuracies.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1109/ARES.2006.9" target="_blank">10.1109/ARES.2006.9</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ARES2006.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2005" 
         data-type="conference"
         data-title="ubiris: a noisy iris image database"
         data-authors="hugo proença, luís a. alexandre"
         data-venue="springer lecture notes in computer science – iciap 2005: 13th international conference on image analysis and processing"
         data-tags="iris recognition,biometrics,image database,noisy images"
         data-id="188">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/paper-9.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">UBIRIS: a noisy iris image database</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Luís A. Alexandre</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">Springer Lecture Notes in Computer Science – ICIAP 2005: 13th International Conference on Image Analysis and Processing</span> (2005)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge conference">Conference</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Iris Recognition</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Biometrics</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Image Database</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Noisy Images</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1007/11553595_119" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="http://www.di.ubi.pt/~hugomcp/doc/ubiris_iciap.pdf" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="188">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="188">
            <h2>UBIRIS: a noisy iris image database</h2>
            <p class="authors">Hugo Proença, Luís A. Alexandre</p>
            <p class="venue">Springer Lecture Notes in Computer Science – ICIAP 2005: 13th International Conference on Image Analysis and Processing (2005)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>This paper presents a new iris database that contains images with noise. This is in contrast with the existing databases, that are noise free. UBIRIS is a tool for the development of robust iris recognition algorithms for biometric proposes. We present a detailed description of the many characteristics of UBIRIS and a comparison of several image segmentation approaches used in the current iris segmentation methods where it is evident their small tolerance to noisy images.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1007/11553595_119" target="_blank">10.1007/11553595_119</a></p>
            
            <div class="links">
                
                <a href="http://www.di.ubi.pt/~hugomcp/doc/ubiris_iciap.pdf" target="_blank">PDF</a>
                
                
                
                
                
                
            </div>
        </div>
    </div>
    
    <div class="publication-item bg-white rounded-xl shadow-md p-6 hover:shadow-lg transition-all duration-300 border border-gray-100 group" 
         data-year="2004" 
         data-type="book_chapter"
         data-title="marcs: multi-agent railway control system"
         data-authors="hugo proença, eugénio oliveira"
         data-venue="marcs: multi-agent railway control system. springer lecture notes in computer science, advances in artificial intelligence."
         data-tags="multi-agent systems,railway control,machine learning"
         data-id="189">
        <div class="flex items-start">
            
            <div class="flex-shrink-0 mr-6">
                <div class="relative overflow-hidden rounded-lg h-32 w-48 bg-gray-50 shadow-sm">
                    <img src="/is-ubi/assets/publications/MARCS.png" alt="Publication thumbnail" class="h-32 w-48 object-cover rounded-lg transform transition-transform duration-500 group-hover:scale-110">
                </div>
            </div>
            
            <div class="flex-grow">
                <h3 class="text-xl font-semibold mb-2 text-gray-900 group-hover:text-blue-600 transition-colors">MARCS: Multi-Agent Railway Control System</h3>
                <p class="text-gray-600 mb-2">Hugo Proença, Eugénio Oliveira</p>
                <p class="text-sm text-gray-500 mb-4">
                    <span class="font-medium">MARCS: Multi-Agent Railway Control System. Springer Lecture Notes in Computer Science, Advances in Artificial Intelligence.</span> (2004)
                    
                    <span class="ml-2 px-2 py-1 rounded-full text-xs publication-type-badge book_chapter">Book chapter</span>
                    
                </p>
                
                <div class="flex flex-wrap gap-1 mb-4">
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Multi-Agent Systems</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Railway Control</span>
                    
                    <span class="px-2 py-1 bg-gray-100 text-gray-700 text-xs rounded-full hover:bg-gray-200 transition-colors">Machine Learning</span>
                    
                </div>
                
                <div class="flex flex-wrap gap-4">
                    
                    <a href="https://doi.org/10.1007/978-3-540-30498-2_2" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" />
                        </svg>
                        DOI
                    </a>
                    
                    
                    <a href="/is-ubi/assets/papers/MARCS.png" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" />
                        </svg>
                        PDF
                    </a>
                    
                    
                    <a href="https://github.com/socia-lab/periocular-attention" class="inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" target="_blank">
                        <svg class="h-5 w-5 mr-1" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true">
                            <path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z" clip-rule="evenodd" />
                        </svg>
                        Code
                    </a>
                    
                    
                    
                    <button class="publication-details-btn inline-flex items-center text-blue-600 hover:text-blue-800 transition-colors" data-id="189">
                        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 16h-1v-4h-1m1-4h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                        </svg>
                        Details
                    </button>
                </div>
            </div>
        </div>
        
        <!-- Hidden content for modal -->
        <div class="hidden publication-details" data-id="189">
            <h2>MARCS: Multi-Agent Railway Control System</h2>
            <p class="authors">Hugo Proença, Eugénio Oliveira</p>
            <p class="venue">MARCS: Multi-Agent Railway Control System. Springer Lecture Notes in Computer Science, Advances in Artificial Intelligence. (2004)</p>
            
            <div class="abstract">
                <h3>Abstract</h3>
                <p>Previous research works have demonstrated that traffic control models based on the comparison between an historical archive of information and current traffic conditions tend to produce better results, usually by improving the system's proactivity behavior. Based on this assumption, we present in this paper MARCS - Multi-Agent Railway Control System, a multi-agent system for communications based trains traffic control. For this purpose we have developed a system infrastructure based on an architecture composed of two independent layers: 'Control' and 'Learning'. 'Control' layer is responsible for traffic supervision, regulation, security and fluidity, including three distinct agent types: 'Supervisor', 'Train' and 'Station'. The 'Learning' layer, using situations accumulated by the 'Control' layer, will infer rules that can improve traffic control processes, minimizing waiting time and stop orders sent for each train.</p>
            </div>
            
            
            <p class="doi">DOI: <a href="https://doi.org/10.1007/978-3-540-30498-2_2" target="_blank">10.1007/978-3-540-30498-2_2</a></p>
            
            <div class="links">
                
                <a href="/is-ubi/assets/papers/MARCS.png" target="_blank">PDF</a>
                
                
                <a href="https://github.com/socia-lab/periocular-attention" target="_blank">Code</a>
                
                
                
                
                
            </div>
        </div>
    </div>
    
</div>

<!-- Empty State -->
<div id="no-publications-message" class="hidden text-center py-12">
    <svg xmlns="http://www.w3.org/2000/svg" class="h-16 w-16 mx-auto text-gray-400 mb-4" fill="none" viewBox="0 0 24 24" stroke="currentColor">
        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9.172 16.172a4 4 0 015.656 0M9 10h.01M15 10h.01M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
    </svg>
    <h3 class="text-xl font-semibold text-gray-700 mb-2">No publications found</h3>
    <p class="text-gray-500">Try adjusting your search or filter criteria</p>
</div>

<!-- Publication Details Modal -->
<div id="publication-modal" class="fixed inset-0 bg-black bg-opacity-50 z-50 flex items-center justify-center hidden">
    <div class="bg-white rounded-xl shadow-2xl max-w-4xl w-full max-h-[90vh] overflow-y-auto mx-4">
        <div class="p-6">
            <div class="flex justify-between items-start mb-4">
                <h2 id="modal-title" class="text-2xl font-bold text-gray-900 pr-8"></h2>
                <button id="close-modal" class="text-gray-500 hover:text-gray-700">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M6 18L18 6M6 6l12 12" />
                    </svg>
                </button>
            </div>
            <p id="modal-authors" class="text-gray-600 mb-2"></p>
            <p id="modal-venue" class="text-sm text-gray-500 mb-6"></p>
            
            <div id="modal-thumbnail" class="mb-6 hidden">
                <img src="" alt="Publication thumbnail" class="rounded-lg shadow-md max-w-full max-h-72 mx-auto">
            </div>
            
            <div id="modal-abstract" class="mb-6">
                <h3 class="text-lg font-semibold mb-2">Abstract</h3>
                <p class="text-gray-600"></p>
            </div>
            
            
            
            <div class="grid grid-cols-1 md:grid-cols-2 gap-6 mb-6">
                <div id="modal-citation" class="bg-gray-50 p-4 rounded-lg">
                    <h3 class="text-lg font-semibold mb-2">Citation</h3>
                    <div class="text-sm text-gray-600 font-mono overflow-x-auto"></div>
                </div>
                
                <div id="modal-links">
                    <h3 class="text-lg font-semibold mb-2">Links</h3>
                    <div class="flex flex-wrap gap-3"></div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
document.addEventListener('DOMContentLoaded', function() {
    const publicationsList = document.getElementById('publications-list');
    const yearFilter = document.getElementById('year-filter');
    const typeFilter = document.getElementById('type-filter');
    const tagFilter = document.getElementById('tag-filter');
    const sortBy = document.getElementById('sort-by');
    const searchInput = document.getElementById('publication-search');
    const publicationsCount = document.getElementById('publications-count');
    const publications = Array.from(document.querySelectorAll('.publication-item'));
    const noPublicationsMessage = document.getElementById('no-publications-message');
    
    // Modal elements
    const modal = document.getElementById('publication-modal');
    const closeModal = document.getElementById('close-modal');
    const modalTitle = document.getElementById('modal-title');
    const modalAuthors = document.getElementById('modal-authors');
    const modalVenue = document.getElementById('modal-venue');
    const modalAbstract = document.getElementById('modal-abstract');
    const modalCitation = document.getElementById('modal-citation').querySelector('div');
    const modalLinks = document.getElementById('modal-links').querySelector('div');
    
    // Detail buttons
    const detailButtons = document.querySelectorAll('.publication-details-btn');
    const detailsContent = document.querySelectorAll('.publication-details');
    
    // Open modal with publication details
    detailButtons.forEach(btn => {
        btn.addEventListener('click', function() {
            const pubId = this.getAttribute('data-id');
            const details = document.querySelector(`.publication-details[data-id="${pubId}"]`);
            
            if (details) {
                modalTitle.textContent = details.querySelector('h2').textContent;
                modalAuthors.textContent = details.querySelector('.authors').textContent;
                modalVenue.textContent = details.querySelector('.venue').textContent;
                
                // Thumbnail
                const modalThumbnail = document.getElementById('modal-thumbnail');
                const pubItem = document.querySelector(`.publication-item[data-id="${pubId}"]`);
                const thumbnailImg = pubItem.querySelector('.flex-shrink-0 img');
                
                if (thumbnailImg) {
                    modalThumbnail.classList.remove('hidden');
                    modalThumbnail.querySelector('img').src = thumbnailImg.src;
                } else {
                    modalThumbnail.classList.add('hidden');
                }
                
                // Abstract
                const abstract = details.querySelector('.abstract');
                if (abstract) {
                    modalAbstract.classList.remove('hidden');
                    modalAbstract.querySelector('p').textContent = abstract.querySelector('p').textContent;
                } else {
                    modalAbstract.classList.add('hidden');
                }
                
                // Tags
                const modalTags = document.getElementById('modal-tags');
                const pubItemTags = pubItem.querySelectorAll('.flex.flex-wrap.gap-1 span');
                
                if (pubItemTags.length > 0 && modalTags) {
                    modalTags.classList.remove('hidden');
                    const tagsContainer = modalTags.querySelector('.flex.flex-wrap.gap-2');
                    tagsContainer.innerHTML = '';
                    
                    pubItemTags.forEach(tag => {
                        const newTag = document.createElement('span');
                        newTag.className = 'px-2 py-1 bg-gray-100 text-gray-700 text-sm rounded-full';
                        newTag.textContent = tag.textContent;
                        tagsContainer.appendChild(newTag);
                    });
                } else if (modalTags) {
                    modalTags.classList.add('hidden');
                }
                
                // Citation (BibTeX)
                const bibtex = details.querySelector('.bibtex');
                if (bibtex) {
                    modalCitation.textContent = bibtex.textContent;
                } else {
                    modalCitation.textContent = `${modalAuthors.textContent}. "${modalTitle.textContent}". ${modalVenue.textContent}`;
                }
                
                // Links
                modalLinks.innerHTML = '';
                const links = details.querySelectorAll('.links a');
                links.forEach(link => {
                    const newLink = document.createElement('a');
                    newLink.href = link.href;
                    newLink.target = '_blank';
                    newLink.classList.add('inline-flex', 'items-center', 'px-3', 'py-2', 'bg-blue-600', 'text-white', 'rounded-lg', 'hover:bg-blue-700', 'transition-colors');
                    
                    let icon = '';
                    if (link.textContent.includes('PDF')) {
                        icon = '<svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M7 21h10a2 2 0 002-2V9.414a1 1 0 00-.293-.707l-5.414-5.414A1 1 0 0012.586 3H7a2 2 0 00-2 2v14a2 2 0 002 2z" /></svg>';
                    } else if (link.textContent.includes('Code')) {
                        icon = '<svg class="h-5 w-5 mr-1" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z" clip-rule="evenodd" /></svg>';
                    } else if (link.textContent.includes('Video')) {
                        icon = '<svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 10l4.553-2.276A1 1 0 0121 8.618v6.764a1 1 0 01-1.447.894L15 14M5 18h8a2 2 0 002-2V8a2 2 0 00-2-2H5a2 2 0 00-2 2v8a2 2 0 002 2z" /></svg>';
                    } else {
                        icon = '<svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-1" fill="none" viewBox="0 0 24 24" stroke="currentColor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6H6a2 2 0 00-2 2v10a2 2 0 002 2h10a2 2 0 002-2v-4M14 4h6m0 0v6m0-6L10 14" /></svg>';
                    }
                    
                    newLink.innerHTML = `${icon} ${link.textContent}`;
                    modalLinks.appendChild(newLink);
                });
                
                // Show modal
                modal.classList.remove('hidden');
                document.body.classList.add('overflow-hidden');
            }
        });
    });
    
    // Close modal
    closeModal.addEventListener('click', function() {
        modal.classList.add('hidden');
        document.body.classList.remove('overflow-hidden');
    });
    
    // Close modal when clicking outside
    modal.addEventListener('click', function(e) {
        if (e.target === modal) {
            modal.classList.add('hidden');
            document.body.classList.remove('overflow-hidden');
        }
    });
    
    // Filter and search functionality
    function filterAndSortPublications() {
        const selectedYear = yearFilter.value;
        const selectedType = typeFilter.value;
        const selectedTag = tagFilter.value;
        const searchTerm = searchInput.value.toLowerCase();
        
        let visibleCount = 0;
        
        publications.forEach(pub => {
            const year = pub.dataset.year;
            const type = pub.dataset.type;
            const title = pub.dataset.title;
            const authors = pub.dataset.authors;
            const venue = pub.dataset.venue;
            const tags = pub.dataset.tags ? pub.dataset.tags.split(',') : [];
            
            // Check if publication matches current filters and search term
            const matchesYear = selectedYear === 'all' || year === selectedYear;
            const matchesType = selectedType === 'all' || type === selectedType.toLowerCase();
            const matchesTag = selectedTag === 'all' || tags.includes(selectedTag.toLowerCase());
            const matchesSearch = !searchTerm || 
                                title.includes(searchTerm) || 
                                authors.includes(searchTerm) || 
                                venue.includes(searchTerm);
            
            if (matchesYear && matchesType && matchesTag && matchesSearch) {
                pub.classList.remove('hidden');
                pub.classList.add('animate-fade-in');
                visibleCount++;
            } else {
                pub.classList.add('hidden');
                pub.classList.remove('animate-fade-in');
            }
        });
        
        // Update results count
        publicationsCount.textContent = visibleCount;
        
        // Show or hide empty state message
        if (visibleCount === 0) {
            publicationsList.classList.add('hidden');
            noPublicationsMessage.classList.remove('hidden');
        } else {
            publicationsList.classList.remove('hidden');
            noPublicationsMessage.classList.add('hidden');
        }
        
        // Sort visible publications
        const visiblePubs = publications.filter(pub => pub.style.display !== 'none');
        
        visiblePubs.sort((a, b) => {
            switch(sortBy.value) {
                case 'year':
                    return parseInt(b.dataset.year) - parseInt(a.dataset.year);
                case 'title':
                    return a.dataset.title.localeCompare(b.dataset.title);
                case 'venue':
                    return a.dataset.venue.localeCompare(b.dataset.venue);
                default:
                    return 0;
            }
        });

        // Clear and re-append sorted publications
        publicationsList.innerHTML = '';
        visiblePubs.forEach(pub => publicationsList.appendChild(pub));
    }

    // Add event listeners
    yearFilter.addEventListener('change', filterAndSortPublications);
    typeFilter.addEventListener('change', filterAndSortPublications);
    tagFilter.addEventListener('change', filterAndSortPublications);
    sortBy.addEventListener('change', filterAndSortPublications);
    searchInput.addEventListener('input', filterAndSortPublications);

    // Initial filter and sort
    filterAndSortPublications();
});
</script>

<style>
/* Animation for fade-in */
@keyframes fadeIn {
    from {
        opacity: 0;
        transform: translateY(10px);
    }
    to {
        opacity: 1;
        transform: translateY(0);
    }
}

.animate-fade-in {
    animation: fadeIn 0.5s ease-out forwards;
}

/* Publication type badges */
.publication-type-badge {
    color: white;
    font-weight: 500;
}

.conference {
    background-color: #22c55e; /* green-500 */
}

.journal {
    background-color: #ef4444; /* red-500 */
}

.book {
    background-color: #171717; /* neutral-900 */
}

.book_chapter {
    background-color: #3b82f6; /* blue-500 */
}

.special_issue {
    background-color: #6b7280; /* gray-500 */
}
</style>

</div>

    </main>

    <!-- Footer -->
    <footer class="bg-gradient-to-br from-gray-900 to-gray-800 text-white py-12 mt-16">
  <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
    <div class="grid grid-cols-1 md:grid-cols-3 gap-10">
      <!-- About Column -->
      <div>
        <h3 class="text-xl font-semibold mb-4 relative inline-block">
          Intelligent Systems
          <span class="absolute bottom-0 left-0 w-full h-0.5 bg-blue-500 transform scale-x-0 transition-transform duration-300 group-hover:scale-x-100"></span>
        </h3>
        <p class="text-gray-300 mb-4">The Intelligent Systems Laboratory at the University of Beira Interior focuses on computer vision, pattern recognition, and biometrics research.</p>
        <div class="flex space-x-4 mt-6">
          <a href="https://github.com/socia-lab" aria-label="GitHub" class="text-gray-400 hover:text-white transition-colors transform hover:scale-110 duration-300">
            <svg class="h-6 w-6" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true">
              <path fill-rule="evenodd" d="M12 2C6.477 2 2 6.484 2 12.017c0 4.425 2.865 8.18 6.839 9.504.5.092.682-.217.682-.483 0-.237-.008-.868-.013-1.703-2.782.605-3.369-1.343-3.369-1.343-.454-1.158-1.11-1.466-1.11-1.466-.908-.62.069-.608.069-.608 1.003.07 1.531 1.032 1.531 1.032.892 1.53 2.341 1.088 2.91.832.092-.647.35-1.088.636-1.338-2.22-.253-4.555-1.113-4.555-4.951 0-1.093.39-1.988 1.029-2.688-.103-.253-.446-1.272.098-2.65 0 0 .84-.27 2.75 1.026A9.564 9.564 0 0112 6.844c.85.004 1.705.115 2.504.337 1.909-1.296 2.747-1.027 2.747-1.027.546 1.379.202 2.398.1 2.651.64.7 1.028 1.595 1.028 2.688 0 3.848-2.339 4.695-4.566 4.943.359.309.678.92.678 1.855 0 1.338-.012 2.419-.012 2.747 0 .268.18.58.688.482A10.019 10.019 0 0022 12.017C22 6.484 17.522 2 12 2z" clip-rule="evenodd" />
            </svg>
          </a>
          <a href="https://twitter.com/socia_lab" aria-label="Twitter" class="text-gray-400 hover:text-white transition-colors transform hover:scale-110 duration-300">
            <svg class="h-6 w-6" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true">
              <path d="M8.29 20.251c7.547 0 11.675-6.253 11.675-11.675 0-.178 0-.355-.012-.53A8.348 8.348 0 0022 5.92a8.19 8.19 0 01-2.357.646 4.118 4.118 0 001.804-2.27 8.224 8.224 0 01-2.605.996 4.107 4.107 0 00-6.993 3.743 11.65 11.65 0 01-8.457-4.287 4.106 4.106 0 001.27 5.477A4.072 4.072 0 012.8 9.713v.052a4.105 4.105 0 003.292 4.022 4.095 4.095 0 01-1.853.07 4.108 4.108 0 003.834 2.85A8.233 8.233 0 012 18.407a11.616 11.616 0 006.29 1.84" />
            </svg>
          </a>
          <a href="mailto:is-ubi@di.ubi.pt" aria-label="Email" class="text-gray-400 hover:text-white transition-colors transform hover:scale-110 duration-300">
            <svg xmlns="http://www.w3.org/2000/svg" class="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 8l7.89 5.26a2 2 0 002.22 0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z" />
            </svg>
          </a>
          <a href="https://www.linkedin.com/company/socia-lab" aria-label="LinkedIn" class="text-gray-400 hover:text-white transition-colors transform hover:scale-110 duration-300">
            <svg class="h-6 w-6" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true">
              <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h-.003z"/>
            </svg>
          </a>
          <a href="https://www.youtube.com/channel/UCsocialab" aria-label="YouTube" class="text-gray-400 hover:text-white transition-colors transform hover:scale-110 duration-300">
            <svg class="h-6 w-6" fill="currentColor" viewBox="0 0 24 24" aria-hidden="true">
              <path d="M23.498 6.186a3.016 3.016 0 0 0-2.122-2.136C19.505 3.545 12 3.545 12 3.545s-7.505 0-9.377.505A3.017 3.017 0 0 0 .502 6.186C0 8.07 0 12 0 12s0 3.93.502 5.814a3.016 3.016 0 0 0 2.122 2.136c1.871.505 9.376.505 9.376.505s7.505 0 9.377-.505a3.015 3.015 0 0 0 2.122-2.136C24 15.93 24 12 24 12s0-3.93-.502-5.814zM9.545 15.568V8.432L15.818 12l-6.273 3.568z"/>
            </svg>
          </a>
        </div>
      </div>
      
      <!-- Quick Links -->
      <div>
        <h3 class="text-xl font-semibold mb-4 relative inline-block">Quick Links</h3>
        <ul class="space-y-3">
          
          
          
          <li>
            <a href="/is-ubi/news/" class="text-gray-300 hover:text-white transition-colors flex items-center group">
              <span class="w-1.5 h-1.5 bg-blue-500 rounded-full mr-2 transform scale-0 group-hover:scale-100 transition-transform duration-300"></span>
              News
            </a>
          </li>
          
          
          <li>
            <a href="/is-ubi/team/" class="text-gray-300 hover:text-white transition-colors flex items-center group">
              <span class="w-1.5 h-1.5 bg-blue-500 rounded-full mr-2 transform scale-0 group-hover:scale-100 transition-transform duration-300"></span>
              Team
            </a>
          </li>
          
          
          <li>
            <a href="/is-ubi/publications/" class="text-gray-300 hover:text-white transition-colors flex items-center group">
              <span class="w-1.5 h-1.5 bg-blue-500 rounded-full mr-2 transform scale-0 group-hover:scale-100 transition-transform duration-300"></span>
              Publications
            </a>
          </li>
          
          
          <li>
            <a href="/is-ubi/projects/" class="text-gray-300 hover:text-white transition-colors flex items-center group">
              <span class="w-1.5 h-1.5 bg-blue-500 rounded-full mr-2 transform scale-0 group-hover:scale-100 transition-transform duration-300"></span>
              Projects
            </a>
          </li>
          
          
          <li>
            <a href="/is-ubi/funding/" class="text-gray-300 hover:text-white transition-colors flex items-center group">
              <span class="w-1.5 h-1.5 bg-blue-500 rounded-full mr-2 transform scale-0 group-hover:scale-100 transition-transform duration-300"></span>
              Funding
            </a>
          </li>
          
          
          <li>
            <a href="/is-ubi/alumni/" class="text-gray-300 hover:text-white transition-colors flex items-center group">
              <span class="w-1.5 h-1.5 bg-blue-500 rounded-full mr-2 transform scale-0 group-hover:scale-100 transition-transform duration-300"></span>
              Alumni
            </a>
          </li>
          
        </ul>
      </div>
      
      <!-- Contact -->
      <div>
        <h3 class="text-xl font-semibold mb-4 relative inline-block">Contact</h3>
        <address class="not-italic text-gray-300 space-y-2">
          <p class="flex items-center">
            <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-2 text-blue-400" fill="none" viewBox="0 0 24 24" stroke="currentColor">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M19 21V5a2 2 0 00-2-2H7a2 2 0 00-2 2v16m14 0h2m-2 0h-5m-9 0H3m2 0h5M9 7h1m-1 4h1m4-4h1m-1 4h1m-5 10v-5a1 1 0 011-1h2a1 1 0 011 1v5m-4 0h4" />
            </svg>
            University of Beira Interior
          </p>
          <p class="flex items-center">
            <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-2 text-blue-400" fill="none" viewBox="0 0 24 24" stroke="currentColor">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12h6m-6 4h6m2 5H7a2 2 0 01-2-2V5a2 2 0 012-2h5.586a1 1 0 01.707.293l5.414 5.414a1 1 0 01.293.707V19a2 2 0 01-2 2z" />
            </svg>
            Department of Computer Science
          </p>
          <p class="flex items-center">
            <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-2 text-blue-400" fill="none" viewBox="0 0 24 24" stroke="currentColor">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M17.657 16.657L13.414 20.9a1.998 1.998 0 01-2.827 0l-4.244-4.243a8 8 0 1111.314 0z" />
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M15 11a3 3 0 11-6 0 3 3 0 016 0z" />
            </svg>
            Rua Marquês d'Ávila e Bolama
          </p>
          <p class="flex items-center ml-7">6201-001 Covilhã, Portugal</p>
          <p class="mt-4 flex items-center">
            <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-2 text-blue-400" fill="none" viewBox="0 0 24 24" stroke="currentColor">
              <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M3 8l7.89 5.26a2 2 0 002.22 0L21 8M5 19h14a2 2 0 002-2V7a2 2 0 00-2-2H5a2 2 0 00-2 2v10a2 2 0 002 2z" />
            </svg>
            <a href="mailto:is-ubi@di.ubi.pt" class="text-blue-400 hover:text-blue-300 transition-colors">is-ubi@di.ubi.pt</a>
          </p>
        </address>
      </div>
    </div>
    
    <div class="border-t border-gray-800 mt-10 pt-6 text-center text-gray-400 text-sm">
      <p>© 2025 Intelligent Systems. All rights reserved.</p>
    </div>
  </div>
</footer>

</body>
</html>
